2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.client.capability.check does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.false.positive.probability does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.broker.address.default does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.orc.time.counters does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve-fraction.min does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.ms.footer.cache.ppd.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.event.message.factory does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.metrics.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.hs2.user.access does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.storage.storageDirectory does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.liveness.connection.timeout.ms does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.dynamic.semijoin.reduction.threshold does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.connect.retry.limit does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.xmx.headroom does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.dynamic.semijoin.reduction does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.direct does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.stats does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.client.consistent.splits does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.session.lifetime does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.timedout.txn.reaper.start does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.ttl does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.management.acl does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.delegation.token.lifetime does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.guidKey does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.ats.hook.queue.capacity does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.large.query does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.bigtable.minsize.semijoin.reduction does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.alloc.min does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.user does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.alloc.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.wait.queue.comparator.class.name does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.port does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.cache.use.soft.references does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve.fraction.max does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.listener.thread-count does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.container.max.java.heap.fraction does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.stats.column.autogather does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.am.liveness.heartbeat.interval.ms does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.decoding.metrics.percentiles.intervals does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.groupby.position.alias does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.txn.store.impl does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.groupby.shuffle does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.object.cache.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.parallel.ops.in.session does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.groupby.limit.extrastep does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.use.ssl does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.local does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.file.location does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.retry.delay.seconds does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.fileformat does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.num.file.cleaner.threads does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.test.fail.compaction does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.use.blobstore.as.scratchdir does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.class does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.mmap.path does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.download.permanent.fns does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.max.historic.queries does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.execution.reducesink.new.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.max.num.delta does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.attempted does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.port does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.initiator.failed.compacts.threshold does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.reporter does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.max.pending.writes does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.execution.mode does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.enable.grace.join.in.llap does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.memory.mode does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.threadpool.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.select.threshold does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.scratchdir.lock does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.use.spnego does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.file.frequency does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.hs2.coordinator.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.timeout.seconds does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.filter.stats.reduction does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.exec.orc.base.delta.ratio does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.fastpath does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.clear.dangling.scratchdir does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.test.fail.heartbeater does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.file.cleanup.delay.seconds does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.management.rpc.port does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.mapjoin.hybridgrace.bloomfilter does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.tree does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.stats.ndv.tuner does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.query.length does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.failed does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.close.session.on.disconnect does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.ppd.windowing does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.initial.metadata.count.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.host does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.ms.footer.cache.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.point.lookup.min does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.file.metadata.threads does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.service.refresh.interval.sec does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.max.output.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.driver.parallel.compilation does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.remote.token.requires.signing does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.bucket.pruning does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.cache.allow.synthetic.fileid does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.hash.table.inflation.factor does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.hbase.ttl does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.vectorized does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.writeset.reaper.interval does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.vector.serde.deserialize does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.order.columnalignment does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.send.buffer.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.exec.schema.evolution does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.elements.values.clause does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.llap.concurrent.queries does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.allow.uber does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.partition.size.max does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.auth does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.include.fileid does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.communicator.num.threads does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.orderby.position.alias does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.connection.sleep.between.retries.ms does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.max.partitions does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.hadoop2.component does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.yarn.shuffle.port does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.elements.in.clause does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.passiveWaitTimeMs does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.load.dynamic.partitions.thread does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.segments.granularity does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.http.response.header.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.conf.internal.variable.list does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose.reductionpercentage does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.retry.limit does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.serialize.in.tasks does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.query.timeout.seconds does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.hadoop2.frequency does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.directory.batch.ms does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.reader.wait does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.reenable.max.timeout.ms does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.max.open.txns does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.auto.convert.sortmerge.join.reduce.side does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.zookeeper.publish.configs does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.auto.convert.join.hashtable.max.entries does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.sessions.init.threads does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.authorization.storage.check.externaltable.drop does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.execution.mode does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.cbo.cnf.maxnodes does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.adaptor.usage.mode does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.rewriting does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.groupMembershipKey does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.catalog.cache.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.cbo.show.warnings does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.fshandler.threads does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.max.bloom.filter.entries does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.metadata.fraction does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.serde does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.scheduler.wait.queue.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.cache.entries does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.txn.operational.properties does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.memory.ttl does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.rpc.port does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.nonvector.wrapper.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.cache.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.vectorized.input.format does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.cte.materialize.threshold does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.clean.until does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.semijoin.conversion does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.port does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.dynamic.partition.pruning does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.metrics.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.rootdir does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.limit.partition.request does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.async.log.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.logger does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.allow.udf.load.on.demand does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.cli.tez.session.async does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.bloom.filter.factor does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.am-reporter.max.threads does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.file.size.for.mapjoin does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.bucketing does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.bucket.pruning.compat does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.spnego.principal does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.preemption.metrics.intervals does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.shuffle.dir.watcher.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.arena.count does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.use.SSL does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.connection.timeout.ms does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.transpose.aggr.join does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.maxTries does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.dynamic.partition.pruning.max.data.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.metadata.base does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.invalidator.frequency does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.use.lrfu does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.mmap does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.coordinator.address.default does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.max.fetch.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.conf.hidden.list does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.io.sarg.cache.max.weight.mb does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.clear.dangling.scratchdir.interval does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.sleep.time does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.row.serde.deserialize does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.compile.lock.timeout does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.timedout.txn.reaper.interval does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.max.variance does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.lrfu.lambda does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.metadata.db.type does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.stream.timeout does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.transactional.events.mem does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.default.fetch.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.retain does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.merge.cardinality.check does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.groupClassKey does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.point.lookup does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.allow.permanent.fns does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.web.ssl does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.txn.manager.dump.lock.state.on.acquire.timeout does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.succeeded does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.use.fileid.path does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.slice.row.count does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.mapjoin.optimized.hashtable.probe.percent does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.select.distribute does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.use.fqdn does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.reenable.min.timeout.ms does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.validate.acls does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.support.special.characters.tablename does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.mv.files.thread does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.skip.compile.udf.check does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.vector.serde.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.interval does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.sleep.interval.between.start.attempts does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.yarn.container.mb does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.http.read.timeout does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.optimizations.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.orc.gap.cache does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.dynamic.partition.hashjoin does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.exec.copyfile.maxnumfiles does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.formats does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.http.numConnection does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.scheduler.enable.preemption does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.num.executors does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.full does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.connection.class does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.sessions.custom.queue.allowed does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.slice.lrr does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.password does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.writer.wait does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.http.request.header.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.max.threads does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose.reductiontuples does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.test.rollbacktxn does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.num.schedulable.tasks.per.node does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.acl does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.memory.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.type.safety does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.async.exec.async.compile does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.max.input.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.enable.memory.manager does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.msck.repair.batch.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.supported.schemes does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.allow.synthetic.fileid does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.stats.filter.in.factor does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.op.stats does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.exec.input.listing.max.threads does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.session.lifetime.jitter does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.web.port does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.cartesian.product does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.rpc.num.handlers does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.vcpus.per.instance does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.count.open.txns.interval does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.min.bloom.filter.entries does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.partition.columns.separate does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.cache.stripe.details.mem.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.txn.heartbeat.threadpool.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.locality.delay does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.cmrootdir does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.disable.backoff.factor does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.liveness.connection.sleep.between.retries.ms does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.exec.inplace.progress does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.working.directory does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.memory.per.instance.mb does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.msck.path.validation does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve.fraction does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.merge.nway.joins does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.reaper.interval does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.txn.strict.locking.mode does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.vector.serde.async.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.input.generate.consistent.splits does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.in.place.progress does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.memory.rownum.max does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.xsrf.filter.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.alloc.max does not exist
2020-05-31 02:30:27 ERROR KeyProviderCache:87 - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
2020-05-31 02:46:57 WARN  TaskSetManager:66 - Stage 9 contains a task of very large size (7429 KB). The maximum recommended task size is 100 KB.
2020-05-31 02:47:09 WARN  TaskSetManager:66 - Stage 14 contains a task of very large size (3112 KB). The maximum recommended task size is 100 KB.
2020-05-31 02:47:22 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 02:47:22 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 02:47:22 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 02:47:22 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 02:47:22 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 02:47:22 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 02:47:22 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 469.508450623002 msec.
2020-05-31 03:23:10 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
2020-05-31 03:23:10 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
2020-05-31 03:24:06 WARN  TaskSetManager:66 - Stage 58 contains a task of very large size (5777 KB). The maximum recommended task size is 100 KB.
2020-05-31 03:24:18 WARN  TaskSetManager:66 - Stage 63 contains a task of very large size (11182 KB). The maximum recommended task size is 100 KB.
==================================================movie_recall.movie_vector_1969==================================================
48470
+--------+-------+--------------------+
|movie_id|cate_id|         movieVector|
+--------+-------+--------------------+
|   95526|   1969|[0.23771860052645...|
|   95841|   1969|[0.17754080359252...|
|   96165|   1969|[0.17630079222572...|
|   96204|   1969|[0.18295143597674...|
|   96359|   1969|[0.18900632141778...|
|  105657|   1969|[0.39322548967709...|
|  105784|   1969|[0.23620111830271...|
|  107108|   1969|[0.16201304576183...|
|  109192|   1969|[0.44579496613561...|
|  110942|   1969|[0.26769197148234...|
|  112603|   1969|[0.01049936843707...|
|  117180|   1969|[0.09482653237247...|
|  117264|   1969|[0.13695887875695...|
|  128784|   1969|[0.20858274922611...|
|  131797|   1969|[0.31830866695862...|
|  133142|   1969|[0.24694094740720...|
|  133164|   1969|[0.68205416575771...|
|  133382|   1969|[0.38621537197859...|
|  133904|   1969|[0.19140644934334...|
|  134532|   1969|[0.25569060864241...|
+--------+-------+--------------------+
only showing top 20 rows

2020-05-31 03:25:56 WARN  TaskSetManager:66 - Stage 78 contains a task of very large size (7397 KB). The maximum recommended task size is 100 KB.
2020-05-31 03:26:02 WARN  TaskSetManager:66 - Stage 79 contains a task of very large size (7397 KB). The maximum recommended task size is 100 KB.
==================================================factor.movie_hot_sort==================================================
341441
+-------+--------------------+---+------+--------+
|    aid|               title|cid|weight|sort_num|
+-------+--------------------+---+------+--------+
|1220433|ROBLIX...|  0|    98|       1|
|1162502|  |  0|    34|       2|
|1231801|txf...|  0|    26|       3|
|1161506|02...|  0|    16|       4|
|1218177|...|  0|    14|       5|
|1224441|...|  0|    11|       6|
|1161617|...|  0|    10|       7|
|1164208|...|  0|     8|       8|
|1305026|        |  0|     8|       9|
|1156459|...|  0|     7|      10|
|1163896|...|  0|     6|      11|
|1232968|              |  0|     6|      12|
|1149860|817...|  0|     6|      13|
|1222562|...|  0|     5|      14|
|1132343|...|  0|     5|      15|
|1151678|...|  0|     5|      16|
|1303750|               |  0|     4|      17|
|1150528|...|  0|     4|      18|
|1161634|...|  0|     4|      19|
|1166197|...|  0|     4|      20|
+-------+--------------------+---+------+--------+
only showing top 20 rows

18210
2020-05-31 03:27:03 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:03 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:03 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:03 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:03 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:03 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:03 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 25.096518254027366 msec.
==================================================factor.movie_hot_factor==================================================
341441
+-------+--------------------+---+--------+----------+--------+
|    aid|               title|cid|play_num|    factor|sort_num|
+-------+--------------------+---+--------+----------+--------+
|1220433|ROBLIX...|  0|      98|0.09255501|       1|
|1162502|  |  0|      34|0.09252481|       2|
|1231801|txf...|  0|      26|0.09252104|       3|
|1161506|02...|  0|      16|0.09251633|       4|
|1218177|...|  0|      14|0.09251539|       5|
|1224441|...|  0|      11|0.09251398|       6|
|1161617|...|  0|      10|0.09251351|       7|
|1164208|...|  0|       8|0.09251257|       8|
|1305026|        |  0|       8|0.09251257|       9|
|1156459|...|  0|       7| 0.0925121|      10|
|1163896|...|  0|       6|0.09251163|      11|
|1232968|              |  0|       6|0.09251163|      12|
|1149860|817...|  0|       6|0.09251163|      13|
|1222562|...|  0|       5|0.09251116|      14|
|1132343|...|  0|       5|0.09251116|      15|
|1151678|...|  0|       5|0.09251116|      16|
|1303750|               |  0|       4|0.09251069|      17|
|1150528|...|  0|       4|0.09251069|      18|
|1161634|...|  0|       4|0.09251069|      19|
|1166197|...|  0|       4|0.09251069|      20|
+-------+--------------------+---+--------+----------+--------+
only showing top 20 rows

7.314787520059517
2020-05-31 03:27:31 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:31 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:31 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:31 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:31 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:31 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:31 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 686.4449325040218 msec.
==================================================factor.movie_score_factor==================================================
341441
+-------+--------+---+-----+------+--------+
|    aid|   title|cid|score|factor|sort_num|
+-------+--------+---+-----+------+--------+
|1271614||  0| 10.0|   1.0|       1|
|1271615||  0| 10.0|   1.0|       2|
|1271616||  0| 10.0|   1.0|       3|
|1271617||  0| 10.0|   1.0|       4|
|1271618||  0| 10.0|   1.0|       5|
|1271619||  0| 10.0|   1.0|       6|
|1271620||  0| 10.0|   1.0|       7|
|1271621| |  0| 10.0|   1.0|       8|
|1271622||  0| 10.0|   1.0|       9|
|1305067|  |  0|  9.5|0.7115|      10|
|1302743|    |  0|  9.3|0.6533|      11|
|1305098|   |  0|  9.2|0.6298|      12|
|1305107|     |  0|  9.2|0.6298|      13|
|1331679|  |  0|  9.2|0.6298|      14|
|1302865|    |  0|  9.1|0.6091|      15|
|1302810|     |  0|  9.0|0.5906|      16|
|1302888|    |  0|  9.0|0.5906|      17|
|1303629|  ALLOUT|  0|  9.0|0.5906|      18|
|1304987|   |  0|  9.0|0.5906|      19|
|1304988|    |  0|  9.0|0.5906|      20|
+-------+--------+---+-----+------+--------+
only showing top 20 rows

2020-05-31 03:28:00 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:00 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:00 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:00 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:00 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:00 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:00 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 2016.5263020264213 msec.
==================================================factor.movie_year_factor==================================================
341441
+--------+----+------+
|movie_id|year|factor|
+--------+----+------+
|       8|2020|   1.0|
|      12|2015|0.7071|
|      57|2014|0.6598|
|     105|2016|0.7579|
|     157|2014|0.6598|
|     228|2014|0.6598|
|     517|2012|0.5743|
|     548|2009|0.4665|
|     571|2012|0.5743|
|     790|1997|0.2031|
|     818|2012|0.5743|
|     940|2015|0.7071|
|    1221|2015|0.7071|
|    1244|2014|0.6598|
|    1257|2014|0.6598|
|    1297|2017|0.8123|
|    1361|2015|0.7071|
|    1377|2017|0.8123|
|    1379|2016|0.7579|
|    1391|1999|0.2333|
+--------+----+------+
only showing top 20 rows

2020-05-31 03:28:24 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:24 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:24 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:24 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:24 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:24 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:24 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 2418.5998715120495 msec.
==================================================factor.movie_time==================================================
266411
+--------+---------+
|movie_id|movie_len|
+--------+---------+
|    2214|        0|
|    8484|       68|
|   95841|       90|
|   96165|       87|
|   96204|        0|
|   96359|      125|
|   98089|       45|
|   98426|        0|
|  101738|        1|
|  103809|       83|
|  103854|       19|
|  104529|       86|
|  104919|        0|
|  105657|        0|
|  105784|      153|
|  109192|        0|
|  112603|       86|
|  117180|      134|
|  117264|        0|
|  119319|       54|
+--------+---------+
only showing top 20 rows


update_movie_recall 72.83600540161133 
2020-05-31 03:42:50 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:42:50 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:42:50 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:42:50 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:42:50 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:42:50 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:42:50 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 1104.358359198705 msec.
==================================================pre_user.user_click_35==================================================
675909
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|  datetime|
+-----------------+--------+-------+----------+
|4C:91:7A:C3:7D:26|  371302|   1970|1589990400|
|4C:91:7A:39:98:51|  938160|   1970|1589990400|
|4C:91:7A:71:5B:66| 1298428|   1970|1589990400|
|4C:91:7A:6E:AB:07| 1325674|   1969|1589990400|
|4C:91:7A:8E:B1:61|  932677|   1969|1589990400|
|4C:91:7A:C9:39:9A|  761429|   1969|1589990400|
|4C:91:7A:4E:27:9C| 1324430|   1970|1589990400|
|4C:91:7A:AA:6F:7A| 1139145|   1970|1589990400|
|4C:91:7A:25:9F:53| 1324818|   1971|1589990400|
|4C:91:7A:53:09:D2| 1267581|   1971|1589990400|
|4C:91:7A:53:09:D2| 1289934|   1971|1589990400|
|4C:91:7A:53:09:D2| 1332657|   1971|1589990400|
|4C:91:7A:73:90:F9|  105358|   1972|1589990400|
|4C:91:7A:A8:66:3C|  939589|   1969|1589990400|
|4C:91:7A:7D:38:9B| 1331284|   1973|1589990400|
|4C:91:7A:7D:38:9B| 1331333|   1973|1589990400|
|4C:91:7A:7D:38:9B|  884662|   1972|1589990400|
|4C:91:7A:37:89:4B|  172145|   1969|1589990400|
|4C:91:7A:37:89:4B|  590212|   1969|1589990400|
|4C:91:7A:1D:4B:E8| 1260511|   1972|1589990400|
+-----------------+--------+-------+----------+
only showing top 20 rows

==================================================pre_user.merge_click==================================================
15048513
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|  datetime|
+-----------------+--------+-------+----------+
|4C:91:7A:BB:C5:6E|  511000|      0|1559318400|
|4C:91:7A:BB:C5:6E| 1077842|      0|1559318400|
|4C:91:7A:77:6D:3B|  736033|   1969|1559318400|
|4C:91:7A:77:6D:3B|  754332|   1973|1559318400|
|4C:91:7A:77:6D:3B| 1077842|   1970|1559318400|
|80:0B:52:06:1F:95|  319578|   1969|1559318400|
|80:0B:52:10:AE:DF|  765403|   1969|1559318400|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400|
|80:0B:52:10:AE:DF|  997259|   1969|1559318400|
|80:0B:52:10:AE:DF| 1084636|   1969|1559318400|
|80:0B:52:10:AE:DF|  973986|   1969|1559318400|
|80:0B:52:10:AE:DF| 1088005|   1970|1559318400|
|80:0B:52:10:AE:DF| 1088005|   1970|1559318400|
|4C:91:7A:2A:61:0C|  997259|   1969|1559318400|
|4C:91:7A:1F:6C:D2|  997259|   1969|1559318400|
|80:0B:52:10:AE:4D| 1100215|   1970|1559318400|
|4C:91:7A:24:B0:27|      49|   2271|1559318400|
|4C:91:7A:24:B0:27|      26|   2271|1559318400|
+-----------------+--------+-------+----------+
only showing top 20 rows

2020-05-31 03:45:27 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:45:27 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:45:27 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:45:27 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:45:27 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:45:27 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:45:27 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 2164.982515704551 msec.
==================================================pre_user.user_top_35==================================================
8191
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|  datetime|
+-----------------+--------+-------+----------+
|4C:91:7A:FC:E5:0F|  981351|   1973|1589990400|
|4C:91:7A:C3:7B:E2| 1081950|   1970|1589990400|
|4C:91:7A:76:A5:B3| 1329495|   1970|1589990400|
|4C:91:7A:76:A5:B3| 1286974|   1970|1589990400|
|4C:91:7A:E9:6F:D3|  970100|   1969|1589990400|
|4C:91:7A:FC:E5:0F|  938012|   1973|1589990400|
|4C:91:7A:C3:7B:E2| 1284424|   1970|1589990400|
|4C:91:7A:C3:7B:E2|   95646|   1969|1589990400|
|4C:91:7A:C3:7B:E2| 1241542|   1969|1589990400|
|4C:91:7A:C3:7B:E2| 1306481|   1970|1589990400|
|4C:91:7A:C3:7B:E2| 1186911|   1970|1589990400|
|4C:91:7A:C3:7B:E2| 1081928|   1970|1589990400|
|4C:91:7A:C3:7B:E2| 1323403|   1970|1589990400|
|4C:91:7A:7A:2D:C6|  245532|   1973|1589990400|
|4C:91:7A:2C:09:D1| 1324818|   1971|1589990400|
|4C:91:7A:C3:7B:E2| 1189693|   1969|1589990400|
|4C:91:7A:3F:18:E0| 1221984|   1969|1589990400|
|4C:91:7A:D1:C5:7B|  320789|   1969|1589990400|
|4C:91:7A:69:C0:F5| 1267454|   1971|1589990400|
|4C:91:7A:9F:16:BC|  928968|   1970|1589990400|
+-----------------+--------+-------+----------+
only showing top 20 rows

==================================================pre_user.merge_top==================================================
197445
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|  datetime|
+-----------------+--------+-------+----------+
|4C:91:7A:0B:27:D9|  997259|   1969|1559318400|
|4C:91:7A:36:9C:23|      19|   2271|1559318400|
|4C:91:7A:36:9C:23| 1089783|      0|1559318400|
|4C:91:7A:24:B0:27| 1103390|      0|1559318400|
|4C:91:7A:24:B0:27| 1103390|      0|1559318400|
|4C:91:7A:24:B0:27| 1103390|      0|1559318400|
|4C:91:7A:24:B0:27| 1103390|      0|1559318400|
|4C:91:7A:56:63:55| 1082910|   1973|1559404800|
|4C:91:7A:56:63:55|  574779|   1973|1559404800|
|4C:91:7A:F2:44:34|  997259|   1969|1559491200|
|4C:91:7A:F2:44:34|  509420|   1969|1559491200|
|4C:91:7A:F2:44:34|  188413|   1969|1559491200|
|4C:91:7A:36:9C:23| 1091820|      0|1559491200|
|4C:91:7A:51:6E:D9|  987081|   1969|1559491200|
|4C:91:7A:51:6E:D9|  987081|   1969|1559491200|
|4C:91:7A:F2:44:34|  983559|   1969|1559491200|
|4C:91:7A:29:7D:88| 1085389|      0|1559577600|
|4C:91:7A:7E:4A:ED| 1104223|   1970|1559664000|
|4C:91:7A:7E:4A:ED|  966570|   1970|1559664000|
|4C:91:7A:7E:4A:ED| 1088358|   1969|1559664000|
+-----------------+--------+-------+----------+
only showing top 20 rows

2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.client.capability.check does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.false.positive.probability does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.broker.address.default does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.orc.time.counters does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve-fraction.min does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.ms.footer.cache.ppd.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.event.message.factory does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.metrics.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.hs2.user.access does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.storage.storageDirectory does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.liveness.connection.timeout.ms does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.dynamic.semijoin.reduction.threshold does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.connect.retry.limit does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.xmx.headroom does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.dynamic.semijoin.reduction does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.direct does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.stats does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.client.consistent.splits does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.session.lifetime does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.timedout.txn.reaper.start does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.ttl does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.management.acl does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.delegation.token.lifetime does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.guidKey does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.ats.hook.queue.capacity does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.large.query does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.bigtable.minsize.semijoin.reduction does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.alloc.min does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.user does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.alloc.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.wait.queue.comparator.class.name does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.port does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.cache.use.soft.references does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve.fraction.max does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.listener.thread-count does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.container.max.java.heap.fraction does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.stats.column.autogather does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.am.liveness.heartbeat.interval.ms does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.decoding.metrics.percentiles.intervals does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.groupby.position.alias does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.txn.store.impl does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.groupby.shuffle does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.object.cache.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.parallel.ops.in.session does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.groupby.limit.extrastep does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.use.ssl does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.local does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.file.location does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.retry.delay.seconds does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.fileformat does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.num.file.cleaner.threads does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.test.fail.compaction does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.use.blobstore.as.scratchdir does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.class does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.mmap.path does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.download.permanent.fns does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.max.historic.queries does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.execution.reducesink.new.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.max.num.delta does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.attempted does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.port does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.initiator.failed.compacts.threshold does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.reporter does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.max.pending.writes does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.execution.mode does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.enable.grace.join.in.llap does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.memory.mode does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.threadpool.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.select.threshold does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.scratchdir.lock does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.use.spnego does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.file.frequency does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.hs2.coordinator.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.timeout.seconds does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.filter.stats.reduction does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.exec.orc.base.delta.ratio does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.fastpath does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.clear.dangling.scratchdir does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.test.fail.heartbeater does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.file.cleanup.delay.seconds does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.management.rpc.port does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.mapjoin.hybridgrace.bloomfilter does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.tree does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.stats.ndv.tuner does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.query.length does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.failed does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.close.session.on.disconnect does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.ppd.windowing does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.initial.metadata.count.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.host does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.ms.footer.cache.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.point.lookup.min does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.file.metadata.threads does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.service.refresh.interval.sec does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.max.output.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.driver.parallel.compilation does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.remote.token.requires.signing does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.bucket.pruning does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.cache.allow.synthetic.fileid does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.hash.table.inflation.factor does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.hbase.ttl does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.vectorized does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.writeset.reaper.interval does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.vector.serde.deserialize does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.order.columnalignment does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.send.buffer.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.exec.schema.evolution does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.elements.values.clause does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.llap.concurrent.queries does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.allow.uber does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.partition.size.max does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.auth does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.include.fileid does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.communicator.num.threads does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.orderby.position.alias does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.connection.sleep.between.retries.ms does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.max.partitions does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.hadoop2.component does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.yarn.shuffle.port does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.elements.in.clause does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.passiveWaitTimeMs does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.load.dynamic.partitions.thread does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.segments.granularity does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.http.response.header.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.conf.internal.variable.list does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose.reductionpercentage does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.retry.limit does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.serialize.in.tasks does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.query.timeout.seconds does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.hadoop2.frequency does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.directory.batch.ms does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.reader.wait does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.reenable.max.timeout.ms does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.max.open.txns does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.auto.convert.sortmerge.join.reduce.side does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.zookeeper.publish.configs does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.auto.convert.join.hashtable.max.entries does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.sessions.init.threads does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.authorization.storage.check.externaltable.drop does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.execution.mode does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.cbo.cnf.maxnodes does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.adaptor.usage.mode does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.rewriting does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.groupMembershipKey does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.catalog.cache.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.cbo.show.warnings does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.fshandler.threads does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.max.bloom.filter.entries does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.metadata.fraction does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.serde does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.scheduler.wait.queue.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.cache.entries does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.txn.operational.properties does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.memory.ttl does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.rpc.port does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.nonvector.wrapper.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.cache.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.vectorized.input.format does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.cte.materialize.threshold does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.clean.until does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.semijoin.conversion does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.port does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.dynamic.partition.pruning does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.metrics.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.rootdir does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.limit.partition.request does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.async.log.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.logger does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.allow.udf.load.on.demand does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.cli.tez.session.async does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.bloom.filter.factor does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.am-reporter.max.threads does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.file.size.for.mapjoin does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.bucketing does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.bucket.pruning.compat does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.spnego.principal does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.preemption.metrics.intervals does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.shuffle.dir.watcher.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.arena.count does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.use.SSL does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.connection.timeout.ms does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.transpose.aggr.join does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.maxTries does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.dynamic.partition.pruning.max.data.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.metadata.base does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.invalidator.frequency does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.use.lrfu does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.mmap does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.coordinator.address.default does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.max.fetch.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.conf.hidden.list does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.io.sarg.cache.max.weight.mb does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.clear.dangling.scratchdir.interval does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.sleep.time does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.row.serde.deserialize does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.compile.lock.timeout does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.timedout.txn.reaper.interval does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.max.variance does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.lrfu.lambda does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.metadata.db.type does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.stream.timeout does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.transactional.events.mem does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.default.fetch.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.retain does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.merge.cardinality.check does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.groupClassKey does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.point.lookup does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.allow.permanent.fns does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.web.ssl does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.txn.manager.dump.lock.state.on.acquire.timeout does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.succeeded does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.use.fileid.path does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.slice.row.count does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.mapjoin.optimized.hashtable.probe.percent does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.select.distribute does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.use.fqdn does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.reenable.min.timeout.ms does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.validate.acls does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.support.special.characters.tablename does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.mv.files.thread does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.skip.compile.udf.check does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.vector.serde.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.interval does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.sleep.interval.between.start.attempts does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.yarn.container.mb does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.http.read.timeout does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.optimizations.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.orc.gap.cache does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.dynamic.partition.hashjoin does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.exec.copyfile.maxnumfiles does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.formats does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.http.numConnection does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.scheduler.enable.preemption does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.num.executors does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.full does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.connection.class does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.sessions.custom.queue.allowed does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.slice.lrr does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.password does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.writer.wait does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.http.request.header.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.max.threads does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose.reductiontuples does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.test.rollbacktxn does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.num.schedulable.tasks.per.node does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.acl does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.memory.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.type.safety does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.async.exec.async.compile does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.max.input.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.enable.memory.manager does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.msck.repair.batch.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.supported.schemes does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.allow.synthetic.fileid does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.stats.filter.in.factor does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.op.stats does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.exec.input.listing.max.threads does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.session.lifetime.jitter does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.web.port does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.cartesian.product does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.rpc.num.handlers does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.vcpus.per.instance does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.count.open.txns.interval does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.min.bloom.filter.entries does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.partition.columns.separate does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.cache.stripe.details.mem.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.txn.heartbeat.threadpool.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.locality.delay does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.cmrootdir does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.disable.backoff.factor does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.liveness.connection.sleep.between.retries.ms does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.exec.inplace.progress does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.working.directory does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.memory.per.instance.mb does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.msck.path.validation does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve.fraction does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.merge.nway.joins does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.reaper.interval does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.txn.strict.locking.mode does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.vector.serde.async.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.input.generate.consistent.splits does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.in.place.progress does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.memory.rownum.max does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.xsrf.filter.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.alloc.max does not exist
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.3.4
      /_/

Using Python version 3.7.7 (default, Mar 26 2020 15:48:22)
SparkSession available as 'spark'.
2020-05-31 03:46:00 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:46:00 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:46:00 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:46:00 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:46:00 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:46:00 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:46:00 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 2633.8572796639946 msec.
==================================================pre_user.user_play_35==================================================
2834016
+-----------------+--------+-------+---------+---------+----------+
|          user_id|movie_id|cate_id|play_time|movie_num|  datetime|
+-----------------+--------+-------+---------+---------+----------+
|4C:91:7A:C3:7D:26|  371302|   1970|  2305386|       35|1589990400|
|4C:91:7A:C3:7D:26|  371302|   1970|  4773890|       35|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  4338638|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  5068065|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  5797567|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  6526846|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  7256091|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  7985334|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  8714614|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  9443856|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973| 10173401|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973| 10902644|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973| 11631973|        1|1589990400|
|4C:91:7A:39:98:51| 1233848|   1972|  1216727|      720|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973| 14364906|       17|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973|   344060|       18|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973|   473599|       18|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973|  1063331|       18|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973|  1713674|       18|1589990400|
|4C:91:7A:39:98:51|  938160|   1970|     7883|        1|1589990400|
+-----------------+--------+-------+---------+---------+----------+
only showing top 20 rows

==================================================pre_user.merge_play==================================================
34639987
+-----------------+--------+-------+---------+---------+----------+
|          user_id|movie_id|cate_id|play_time|movie_num|  datetime|
+-----------------+--------+-------+---------+---------+----------+
|00:0C:18:EF:EE:E7|  226509|   1969|        2|      -10|1565712000|
|00:0C:18:EF:EE:E7|  226509|   1969|        2|      -10|1565712000|
|00:0C:18:EF:EE:E7|  143088|   1969|        6|      -10|1565712000|
|00:0C:18:EF:EE:E7|  992955|   1969|        6|      -10|1565712000|
|00:0C:18:EF:EE:E7| 1132226|   1970|       16|       13|1565712000|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       11|       31|1565798400|
|4C:91:7A:34:CA:3A|  987187|   1969|       53|      -10|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       28|       36|1565798400|
|4C:91:7A:29:7D:88|  927671|   1969|        6|      -10|1565884800|
|4C:91:7A:29:7D:88|  984018|   1969|        3|      -10|1565884800|
|4C:91:7A:29:7D:88|  984018|   1969|        3|      -10|1565884800|
|4C:91:7A:E3:F0:B6| 1156978|   1969|        2|      -10|1565884800|
|4C:91:7A:29:7D:88| 1138814|   1970|       38|        1|1565884800|
|4C:91:7A:35:60:C1|  978191|   1970|       20|        1|1565884800|
|4C:91:7A:35:60:C1|  978191|   1970|       20|        1|1565884800|
+-----------------+--------+-------+---------+---------+----------+
only showing top 20 rows

==================================================pre_user.merge_action==================================================
49885945
+-----------------+--------+-------+----------+-----+-----+---------+---------+
|          user_id|movie_id|cate_id|  datetime|click|  top|play_time|movie_num|
+-----------------+--------+-------+----------+-----+-----+---------+---------+
|4C:91:7A:BB:C5:6E|  511000|      0|1559318400| true|false|      -10|      -10|
|4C:91:7A:BB:C5:6E| 1077842|      0|1559318400| true|false|      -10|      -10|
|4C:91:7A:77:6D:3B|  736033|   1969|1559318400| true|false|      -10|      -10|
|4C:91:7A:77:6D:3B|  754332|   1973|1559318400| true|false|      -10|      -10|
|4C:91:7A:77:6D:3B| 1077842|   1970|1559318400| true|false|      -10|      -10|
|80:0B:52:06:1F:95|  319578|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF|  765403|   1969|1559318400| true|false|      -10|      -10|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400| true|false|      -10|      -10|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400| true|false|      -10|      -10|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF|  997259|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF| 1084636|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF|  973986|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF| 1088005|   1970|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF| 1088005|   1970|1559318400| true|false|      -10|      -10|
|4C:91:7A:2A:61:0C|  997259|   1969|1559318400| true|false|      -10|      -10|
|4C:91:7A:1F:6C:D2|  997259|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:4D| 1100215|   1970|1559318400| true|false|      -10|      -10|
|4C:91:7A:24:B0:27|      49|   2271|1559318400| true|false|      -10|      -10|
|4C:91:7A:24:B0:27|      26|   2271|1559318400| true|false|      -10|      -10|
+-----------------+--------+-------+----------+-----+-----+---------+---------+
only showing top 20 rows

==================================================factor.user_history_click==================================================
4739571
+--------+-----------------+-------+----------+--------------------+----+---------+
|movie_id|          user_id|cate_id|  datetime|               title|year|sort_time|
+--------+-----------------+-------+----------+--------------------+----+---------+
| 1286974|00:11:22:33:44:55|   1970|1588867200|                |2020|        1|
| 1311945|00:11:22:33:44:55|   1970|1588694400|                  |2020|        2|
|   98176|00:11:22:33:44:55|   1970|1587916800|          |2012|        3|
| 1286089|00:11:22:33:44:55|   1970|1585670400|             |2020|        4|
|  994596|00:24:68:D5:A7:72|   1971|1561737600|            3|2019|        1|
| 1228118|00:66:CE:0C:31:37|      0|1579104000|1-1.Dubbo+zookeee...|2019|        1|
| 1266797|00:66:CE:0C:31:37|      0|1579017600|               5|2020|        2|
|  105540|04:95:73:35:B5:7D|      0|1579104000|          |1993|        1|
|  142595|04:95:73:35:B5:7D|      0|1579104000|                |2015|        2|
| 1266684|04:95:73:35:B5:7D|      0|1579104000|               |2020|        3|
|  972618|04:95:73:35:B5:7D|      0|1579104000|            |2018|        4|
| 1109895|04:95:73:F8:D5:2E|   1970|1561651200|               |2019|        1|
|  997259|08:A5:C8:59:E2:C2|   1969|1561651200|               4|2019|        1|
| 1085389|0C:C6:55:AD:04:74|   1971|1561651200|             5|2019|        1|
|  971906|0C:C6:55:BF:BB:45|   1973|1561737600|              X|2018|        1|
| 1085389|0E:18:E2:37:97:82|   1971|1561564800|             5|2019|        1|
| 1089711|18:82:19:19:17:39|   2272|1561564800|      |2019|        1|
|  897593|18:82:19:1B:05:39|   1973|1561737600|                |2017|        1|
|  327574|18:82:19:1B:05:39|   1973|1561737600|          |2013|        2|
|  927941|18:82:19:1B:05:39|   1973|1561651200|               |2018|        3|
+--------+-----------------+-------+----------+--------------------+----+---------+
only showing top 20 rows

==================================================factor.user_history_top==================================================
125337
+--------+-----------------+-------+----------+-----------------+----+---------+
|movie_id|          user_id|cate_id|  datetime|            title|year|sort_time|
+--------+-----------------+-------+----------+-----------------+----+---------+
| 1087476|4C:91:7A:00:3B:82|      0|1563465600|             |2019|        1|
| 1112139|4C:91:7A:00:C7:D6|   1969|1572278400|             |2019|        1|
|  988774|4C:91:7A:01:1C:8D|   1969|1590768000|         |2018|        1|
|  804690|4C:91:7A:01:82:68|   1972|1590508800|    1|2016|        1|
|  987081|4C:91:7A:02:31:C7|   1969|1577548800|          |2018|        1|
| 1102963|4C:91:7A:02:31:C7|   1969|1575734400|             |2019|        2|
|  938839|4C:91:7A:02:31:C7|   1969|1575734400|           2|2019|        3|
| 1033987|4C:91:7A:03:4B:10|   1971|1583942400|           3|2019|        1|
| 1200865|4C:91:7A:03:4B:10|   1971|1583683200|      |2020|        2|
| 1129956|4C:91:7A:03:4B:10|   1971|1582819200|     3()|   0|        3|
| 1144115|4C:91:7A:03:4B:10|   1971|1582819200|        4|2016|        4|
| 1221969|4C:91:7A:03:4B:10|   1971|1582819200|              |2019|        5|
|  108278|4C:91:7A:03:4B:10|   1971|1580486400|         2|2015|        6|
| 1192084|4C:91:7A:04:31:F2|   1970|1575216000|           |2019|        1|
|  188773|4C:91:7A:04:DC:87|   1970|1582819200|            3|2009|        1|
|  917354|4C:91:7A:05:E3:2A|   1972|1590768000|       3|2017|        1|
|  991179|4C:91:7A:05:E3:2A|   1972|1587139200|StarTwinkle|2019|        2|
|  492089|4C:91:7A:05:E3:2A|   1972|1585238400|      Suite|2011|        3|
|  270722|4C:91:7A:05:E3:2A|   1972|1585238400|  Crystal3|2016|        4|
|  153105|4C:91:7A:05:E3:2A|   1972|1585238400|     Crystal|2014|        5|
+--------+-----------------+-------+----------+-----------------+----+---------+
only showing top 20 rows

==================================================factor.user_history_play==================================================
1709391
+--------+-----------------+-------+----------+---------------+----+---------+
|movie_id|          user_id|cate_id|  datetime|          title|year|sort_time|
+--------+-----------------+-------+----------+---------------+----+---------+
| 1286974|00:11:22:33:44:55|   1970|1588694400|           |2020|        1|
| 1311945|00:11:22:33:44:55|   1970|1588694400|             |2020|        2|
|   98176|00:11:22:33:44:55|   1970|1587830400|     |2012|        3|
|  113798|4C:91:7A:00:74:25|   1969|1581350400|           |2007|        1|
| 1122414|4C:91:7A:00:74:25|   1969|1581350400|            |2019|        2|
| 1235405|4C:91:7A:00:74:25|   1969|1581264000|         |2019|        3|
| 1192086|4C:91:7A:00:74:25|   1969|1580659200|           |2019|        4|
|  717396|4C:91:7A:00:74:25|   1969|1580659200|           |2014|        5|
|   95844|4C:91:7A:00:74:25|   1969|1580659200|           |2014|        6|
| 1081744|4C:91:7A:00:74:25|   1969|1580659200|           |2010|        7|
| 1236354|4C:91:7A:00:74:25|   1969|1580400000|           |2019|        8|
|  971886|4C:91:7A:00:74:25|   1969|1580054400|      1|2001|        9|
| 1130996|4C:91:7A:00:74:25|   1969|1580054400|           |2019|       10|
|  103287|4C:91:7A:00:AF:28|   1972|1588089600|           |1993|        1|
|  990193|4C:91:7A:00:D8:3F|   1970|1580313600|             |2019|        1|
| 1214970|4C:91:7A:01:51:D2|   1973|1589644800|        7|2019|        1|
| 1082077|4C:91:7A:01:51:D2|   1973|1589558400|2|2019|        2|
| 1300676|4C:91:7A:01:51:D2|   1973|1588435200|     3|2020|        3|
|  244404|4C:91:7A:01:51:D2|   1973|1587657600|         |2015|        4|
|  216817|4C:91:7A:01:51:D2|   1973|1586448000|      |2016|        5|
+--------+-----------------+-------+----------+---------------+----+---------+
only showing top 20 rows


update_user_history 41.57600565751394 
==================================================update_user.merge_action==================================================
5838420
+-----------------+--------+-------+----------+-----+-----+---------+---------+
|          user_id|movie_id|cate_id|  datetime|click|  top|play_time|movie_num|
+-----------------+--------+-------+----------+-----+-----+---------+---------+
|4C:91:7A:F3:EA:5F| 1302343|   1971|1590336000| true|false|      -10|      -10|
|4C:91:7A:16:C5:F1| 1286974|   1970|1590336000| true|false|      -10|      -10|
|4C:91:7A:E6:AF:9B|  203962|   1973|1590336000| true|false|      -10|      -10|
|4C:91:7A:16:C5:F1|  789289|   1970|1590336000| true|false|      -10|      -10|
|4C:91:7A:16:C5:F1| 1324430|   1970|1590336000| true|false|      -10|      -10|
|4C:91:7A:C9:B5:50| 1324818|   1971|1590336000| true|false|      -10|      -10|
|4C:91:7A:CE:2D:49|  191686|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:CE:2D:49|  929092|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:AB:10:23|  765286|   1973|1590336000| true|false|      -10|      -10|
|4C:91:7A:54:BB:6C|  717166|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:54:BB:6C|  732487|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:6C:65:D9|   96360|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:6C:65:D9|  789289|   1970|1590336000| true|false|      -10|      -10|
|4C:91:7A:38:5C:72| 1328252|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:6C:65:D9|  789289|   1970|1590336000| true|false|      -10|      -10|
|4C:91:7A:6C:65:D9|  789289|   1970|1590336000| true|false|      -10|      -10|
|4C:91:7A:24:86:71|  108164|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:24:86:71|  747492|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:24:86:71|  113983|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:A2:27:06|  154385|      0|1590336000| true|false|      -10|      -10|
+-----------------+--------+-------+----------+-----+-----+---------+---------+
only showing top 20 rows

2020-05-31 04:29:20 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 04:29:20 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 04:29:20 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 04:29:20 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 04:29:20 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 04:29:20 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 04:29:20 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 2626.0906198378857 msec.
==================================================update_user.action_weight==================================================
215175
+-----------------+--------+------------------+-------+
|          user_id|movie_id|            weight|cate_id|
+-----------------+--------+------------------+-------+
|4C:91:7A:FB:7F:EC|  142940|10.183799999999998|   1969|
|4C:91:7A:C7:F9:DE|  575643| 67.15499999999996|   1970|
|4C:91:7A:29:42:F8| 1083262|12.220199999999997|   1969|
|4C:91:7A:08:7B:48|  997501|4.0733999999999995|   1973|
|4C:91:7A:70:47:CE| 1337100| 8.147400000000001|   1971|
|4C:91:7A:3C:6E:8F|  991362|4.0733999999999995|   1973|
|4C:91:7A:E6:A1:01|  244404|13.802799999999992|   1973|
|4C:91:7A:E0:76:AE|  949244| 51.93660000000001|   1973|
|4C:91:7A:BF:C4:72|  153232|21.323000000000004|   1972|
|4C:91:7A:C2:A5:41|  935968|4.0733999999999995|   1969|
|4C:91:7A:76:A6:3E| 1129516|4.0733999999999995|   1971|
|4C:91:7A:80:1B:6E| 1311945|           79.1526|   1970|
|4C:91:7A:D4:E6:56|  204212| 9.827300000000003|   1973|
|4C:91:7A:1C:B7:42|  108978|4.0733999999999995|   1969|
|4C:91:7A:66:3C:05| 1229066|18.212199999999985|   1973|
|4C:91:7A:06:76:67|  376781|12.428399999999996|   1970|
|4C:91:7A:4E:87:0F|  203286|           46.8456|   1973|
|4C:91:7A:44:E7:8A|  878564|30.551399999999994|   1972|
|4C:91:7A:24:67:D8| 1181218|4.0733999999999995|   1970|
|4C:91:7A:82:68:37|  935895| 45.23819999999999|   1973|
+-----------------+--------+------------------+-------+
only showing top 20 rows

0.0016
2435.8332
==================================================update_user.action_weight_normal==================================================
215175
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|    weight|
+-----------------+--------+-------+----------+
|4C:91:7A:FB:7F:EC|  142940|   1969|0.00418017|
|4C:91:7A:C7:F9:DE|  575643|   1970|0.02756898|
|4C:91:7A:29:42:F8| 1083262|   1969|0.00501619|
|4C:91:7A:08:7B:48|  997501|   1973|0.00167163|
|4C:91:7A:70:47:CE| 1337100|   1971|0.00334416|
|4C:91:7A:3C:6E:8F|  991362|   1973|0.00167163|
|4C:91:7A:E6:A1:01|  244404|   1973|0.00566591|
|4C:91:7A:E0:76:AE|  949244|   1973|0.02132126|
|4C:91:7A:BF:C4:72|  153232|   1972|0.00875323|
|4C:91:7A:C2:A5:41|  935968|   1969|0.00167163|
|4C:91:7A:76:A6:3E| 1129516|   1971|0.00167163|
|4C:91:7A:80:1B:6E| 1311945|   1970|0.03249445|
|4C:91:7A:D4:E6:56|  204212|   1973|0.00403382|
|4C:91:7A:1C:B7:42|  108978|   1969|0.00167163|
|4C:91:7A:66:3C:05| 1229066|   1973|0.00747613|
|4C:91:7A:06:76:67|  376781|   1970|0.00510167|
|4C:91:7A:4E:87:0F|  203286|   1973|0.01923121|
|4C:91:7A:44:E7:8A|  878564|   1972|0.01254184|
|4C:91:7A:24:67:D8| 1181218|   1970|0.00167163|
|4C:91:7A:82:68:37|  935895|   1973|0.01857132|
+-----------------+--------+-------+----------+
only showing top 20 rows


update_user_profile 6.3613568862279255 
==================================================update_user.user_similar_recall_1969==================================================
4491399
+-----------------+--------+------+----+---------+---------+-----+-----+--------+---------+----------+
|          user_id|movie_id| title|year|movie_id2|   title2|year2|score|play_num|   weight|total_sort|
+-----------------+--------+------+----+---------+---------+-----+-----+--------+---------+----------+
|4C:91:7A:12:DF:95|  135239||1959|  1112139|     | 2019|  8.7|      41|2.1777E-4|         1|
|4C:91:7A:12:DF:95|  135239||1959|   929382|    | 2018|  9.1|      18|2.1269E-4|         2|
|4C:91:7A:12:DF:95|  135239||1959|  1130996|     | 2019|  8.8|      83|2.1233E-4|         3|
|4C:91:7A:12:DF:95|  135239||1959|   846996|      | 2017|  8.3|       9|  1.77E-4|         4|
|4C:91:7A:12:DF:95|  135239||1959|   226812|     | 2016|  8.7|      11|1.7348E-4|         5|
|4C:91:7A:12:DF:95|  135239||1959|   715777|  | 2012|  9.1|       3|1.5238E-4|         6|
|4C:91:7A:12:DF:95|  135239||1959|   968610|     | 2017|  7.3|       0|1.2465E-4|         7|
|4C:91:7A:12:DF:95|  135239||1959|  1062210|    | 2010|  9.1|      19|1.2133E-4|         8|
|4C:91:7A:12:DF:95|  135108|  |1960|   972249|| 2018|  0.0|       2|1.1859E-4|         9|
|4C:91:7A:12:DF:95|  135239||1959|  1106506|| 2012|  0.0|       0|1.1261E-4|        10|
|4C:91:7A:12:DF:95|  135108|  |1960|   507701|       | 2016|  7.4|       0|1.0293E-4|        11|
|4C:91:7A:12:DF:95|  135108|  |1960|   192117|    | 2014|  8.3|       0| 1.024E-4|        12|
|4C:91:7A:12:DF:95|  135108|  |1960|   109773|    | 2011|  8.9|      55|1.0175E-4|        13|
|4C:91:7A:12:DF:95|  135239||1959|   109285|      11| 2012|  8.2|       0| 9.947E-5|        14|
|4C:91:7A:12:DF:95|  135108|  |1960|  1131852|       | 2015|  0.0|       0| 9.816E-5|        15|
|4C:91:7A:12:DF:95|  135108|  |1960|   192157|    | 2014|  7.4|       0| 8.953E-5|        16|
|4C:91:7A:12:DF:95|  135239||1959|   967638|     | 2009|  7.1|       0| 8.875E-5|        17|
|4C:91:7A:12:DF:95|  135108|  |1960|  1117057|      | 2013|  0.0|       0| 8.495E-5|        18|
|4C:91:7A:12:DF:95|  135108|  |1960|   193356|    | 2012|  7.3|       0| 8.381E-5|        19|
|4C:91:7A:12:DF:95|  135108|  |1960|   798152|      | 2012|  7.1|       0| 7.642E-5|        20|
+-----------------+--------+------+----+---------+---------+-----+-----+--------+---------+----------+
only showing top 20 rows

==================================================update_user.user_similar_filter_same_recall_1969==================================================
4169817
+-----------------+--------+---------+----+-----+--------+--------------------+--------------------+
|          user_id|movie_id|    title|year|score|play_num|              weight|          base_movie|
+-----------------+--------+---------+----+-----+--------+--------------------+--------------------+
|00:22:33:44:55:66|  129967|     |1999|  7.1|       0|           1.4943E-4|[[1121176, , 2...|
|00:22:33:44:55:66|  968449|      |2018|  7.4|       0|             5.35E-6|[[1341341, ,...|
|00:22:33:44:55:66| 1322357|     |2003|  6.9|       0|             7.63E-6|[[1339141, , 20...|
|00:30:1B:BA:02:DB|    6954|       |1996|  8.2|       0|            7.607E-5|[[965305, , 20...|
|00:30:1B:BA:02:DB|  114773|     |2006|  6.8|       0|            6.469E-5|[[196680, ,...|
|00:30:1B:BA:02:DB|  128747|     |1960|  9.0|       0|            2.056E-5|[[1121176, , 2...|
|00:30:1B:BA:02:DB|  133740|     |1982|  7.8|       0|1.579999999999999...|[[132723, , 19...|
|00:30:1B:BA:02:DB|  156511|       |2013|  8.2|       0|            7.405E-5|[[151623, , ...|
|00:30:1B:BA:02:DB|  211034|     |2011|  7.2|       0|           1.4888E-4|[[1269531, , ...|
|00:30:1B:BA:02:DB|  397549|     |2014|  0.0|       0|           5.7346E-4|[[1121176, , 2...|
|00:30:1B:BA:02:DB|  509382|     |2013|  8.6|       0|            2.195E-5|[[984161, , 1988]]|
|00:30:1B:BA:02:DB|  762707|     |1960|  7.2|       1|             3.61E-6|[[901794, , ...|
|00:30:1B:BA:02:DB|  901801|   |2015|  6.8|      12|             7.62E-6|[[129710, ,...|
|00:30:1B:BA:02:DB|  902140|     |2015|  7.1|       0|           1.4804E-4|[[1339141, , 20...|
|00:30:1B:BA:02:DB|  927242|   |1989|  7.6|       1|           1.4554E-4|[[1338119, , ...|
|00:30:1B:BA:02:DB| 1074939|      |2011|  7.5|       0|           3.3888E-4|[[778689, , 2...|
|00:30:1B:BA:02:DB| 1080214||2008|  0.0|       0|            4.616E-5|[[151623, , ...|
|00:30:1B:BA:02:DB| 1119983|    |1970|  7.5|       0|3.420000000000000...|[[993066, ...|
|00:30:1B:BA:02:DB| 1124350|  ()|2017|  4.0|       0|             6.89E-6|[[879132, ...|
|00:30:1B:BA:02:DB| 1337461|     |1989|  7.2|       0|              8.3E-6|[[1326854, ,...|
+-----------------+--------+---------+----+-----+--------+--------------------+--------------------+
only showing top 20 rows

==================================================update_user.user_similar_filter_history_recall_1969==================================================
4139726
+-----------------+--------+---------+----+-----+--------+---------+--------------------+--------+---------+
|          user_id|movie_id|    title|year|score|play_num|   weight|          base_movie|sort_num|timestamp|
+-----------------+--------+---------+----+-----+--------+---------+--------------------+--------+---------+
|4C:91:7A:12:DF:95| 1112139|     |2019|  8.7|      41|2.1777E-4|[[135239, ,...|       1| 20200531|
|4C:91:7A:12:DF:95|  929382|    |2018|  9.1|      18|2.1269E-4|[[135239, ,...|       2| 20200531|
|4C:91:7A:12:DF:95| 1130996|     |2019|  8.8|      83|2.1233E-4|[[135239, ,...|       3| 20200531|
|4C:91:7A:12:DF:95|  846996|      |2017|  8.3|       9|  1.77E-4|[[135239, ,...|       4| 20200531|
|4C:91:7A:12:DF:95|  226812|     |2016|  8.7|      11|1.7348E-4|[[135239, ,...|       5| 20200531|
|4C:91:7A:12:DF:95|  715777|  |2012|  9.1|       3|1.5238E-4|[[135239, ,...|       6| 20200531|
|4C:91:7A:12:DF:95|  968610|     |2017|  7.3|       0|1.2465E-4|[[135239, ,...|       7| 20200531|
|4C:91:7A:12:DF:95| 1062210|    |2010|  9.1|      19|1.2133E-4|[[135239, ,...|       8| 20200531|
|4C:91:7A:12:DF:95|  972249||2018|  0.0|       2|1.1859E-4|[[135108, , 1...|       9| 20200531|
|4C:91:7A:12:DF:95| 1106506||2012|  0.0|       0|1.1261E-4|[[135239, ,...|      10| 20200531|
|4C:91:7A:12:DF:95|  507701|       |2016|  7.4|       0|1.0293E-4|[[135108, , 1...|      11| 20200531|
|4C:91:7A:12:DF:95|  192117|    |2014|  8.3|       0| 1.024E-4|[[135108, , 1...|      12| 20200531|
|4C:91:7A:12:DF:95|  109773|    |2011|  8.9|      55|1.0175E-4|[[135108, , 1...|      13| 20200531|
|4C:91:7A:12:DF:95|  109285|      11|2012|  8.2|       0| 9.947E-5|[[135239, ,...|      14| 20200531|
|4C:91:7A:12:DF:95| 1131852|       |2015|  0.0|       0| 9.816E-5|[[135108, , 1...|      15| 20200531|
|4C:91:7A:12:DF:95|  192157|    |2014|  7.4|       0| 8.953E-5|[[135108, , 1...|      16| 20200531|
|4C:91:7A:12:DF:95|  967638|     |2009|  7.1|       0| 8.875E-5|[[135239, ,...|      17| 20200531|
|4C:91:7A:12:DF:95| 1117057|      |2013|  0.0|       0| 8.495E-5|[[135108, , 1...|      18| 20200531|
|4C:91:7A:12:DF:95|  193356|    |2012|  7.3|       0| 8.381E-5|[[135108, , 1...|      19| 20200531|
|4C:91:7A:12:DF:95|  798152|      |2012|  7.1|       0| 7.642E-5|[[135108, , 1...|      20| 20200531|
+-----------------+--------+---------+----+-----+--------+---------+--------------------+--------+---------+
only showing top 20 rows

==================================================update_user.user_similar_recall_1969_100==================================================
1369302
+-----------------+--------+---------+----+-----+--------+---------+--------------------+--------+---------+
|          user_id|movie_id|    title|year|score|play_num|   weight|          base_movie|sort_num|timestamp|
+-----------------+--------+---------+----+-----+--------+---------+--------------------+--------+---------+
|4C:91:7A:12:DF:95| 1112139|     |2019|  8.7|      41|2.1777E-4|[[135239, ,...|       1| 20200531|
|4C:91:7A:12:DF:95|  929382|    |2018|  9.1|      18|2.1269E-4|[[135239, ,...|       2| 20200531|
|4C:91:7A:12:DF:95| 1130996|     |2019|  8.8|      83|2.1233E-4|[[135239, ,...|       3| 20200531|
|4C:91:7A:12:DF:95|  846996|      |2017|  8.3|       9|  1.77E-4|[[135239, ,...|       4| 20200531|
|4C:91:7A:12:DF:95|  226812|     |2016|  8.7|      11|1.7348E-4|[[135239, ,...|       5| 20200531|
|4C:91:7A:12:DF:95|  715777|  |2012|  9.1|       3|1.5238E-4|[[135239, ,...|       6| 20200531|
|4C:91:7A:12:DF:95|  968610|     |2017|  7.3|       0|1.2465E-4|[[135239, ,...|       7| 20200531|
|4C:91:7A:12:DF:95| 1062210|    |2010|  9.1|      19|1.2133E-4|[[135239, ,...|       8| 20200531|
|4C:91:7A:12:DF:95|  972249||2018|  0.0|       2|1.1859E-4|[[135108, , 1...|       9| 20200531|
|4C:91:7A:12:DF:95| 1106506||2012|  0.0|       0|1.1261E-4|[[135239, ,...|      10| 20200531|
|4C:91:7A:12:DF:95|  507701|       |2016|  7.4|       0|1.0293E-4|[[135108, , 1...|      11| 20200531|
|4C:91:7A:12:DF:95|  192117|    |2014|  8.3|       0| 1.024E-4|[[135108, , 1...|      12| 20200531|
|4C:91:7A:12:DF:95|  109773|    |2011|  8.9|      55|1.0175E-4|[[135108, , 1...|      13| 20200531|
|4C:91:7A:12:DF:95|  109285|      11|2012|  8.2|       0| 9.947E-5|[[135239, ,...|      14| 20200531|
|4C:91:7A:12:DF:95| 1131852|       |2015|  0.0|       0| 9.816E-5|[[135108, , 1...|      15| 20200531|
|4C:91:7A:12:DF:95|  192157|    |2014|  7.4|       0| 8.953E-5|[[135108, , 1...|      16| 20200531|
|4C:91:7A:12:DF:95|  967638|     |2009|  7.1|       0| 8.875E-5|[[135239, ,...|      17| 20200531|
|4C:91:7A:12:DF:95| 1117057|      |2013|  0.0|       0| 8.495E-5|[[135108, , 1...|      18| 20200531|
|4C:91:7A:12:DF:95|  193356|    |2012|  7.3|       0| 8.381E-5|[[135108, , 1...|      19| 20200531|
|4C:91:7A:12:DF:95|  798152|      |2012|  7.1|       0| 7.642E-5|[[135108, , 1...|      20| 20200531|
+-----------------+--------+---------+----+-----+--------+---------+--------------------+--------+---------+
only showing top 20 rows


update_similar_recall 9.596558606624603 
2020-06-01 02:32:07 WARN  TaskSetManager:66 - Stage 296 contains a task of very large size (7436 KB). The maximum recommended task size is 100 KB.
2020-06-01 02:32:18 WARN  TaskSetManager:66 - Stage 301 contains a task of very large size (3116 KB). The maximum recommended task size is 100 KB.
2020-06-01 02:38:50 WARN  TaskSetManager:66 - Stage 350 contains a task of very large size (5778 KB). The maximum recommended task size is 100 KB.
2020-06-01 02:39:01 WARN  TaskSetManager:66 - Stage 355 contains a task of very large size (11184 KB). The maximum recommended task size is 100 KB.
2020-06-01 02:39:02 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:39:02 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:39:02 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:39:02 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:39:02 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:39:02 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:39:02 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 1641.6074799273106 msec.
==================================================movie_recall.movie_vector_1969==================================================
48475
+--------+-------+--------------------+
|movie_id|cate_id|         movieVector|
+--------+-------+--------------------+
|   95526|   1969|[-0.0336128043302...|
|   95841|   1969|[0.02462345301963...|
|   96165|   1969|[0.03642685649184...|
|   96204|   1969|[0.10642637237871...|
|   96359|   1969|[0.04910043873663...|
|  105657|   1969|[-0.0594352690858...|
|  105784|   1969|[-0.1290140888753...|
|  107108|   1969|[-0.4250065802154...|
|  109192|   1969|[0.04262914159737...|
|  110942|   1969|[-0.0772006379859...|
|  112603|   1969|[-0.4504811377898...|
|  117180|   1969|[0.17078241318604...|
|  117264|   1969|[0.01063971868210...|
|  128784|   1969|[-0.0201483037341...|
|  131797|   1969|[-0.2072349173681...|
|  133142|   1969|[0.08271304391315...|
|  133164|   1969|[0.08545529513503...|
|  133382|   1969|[0.04815989623419...|
|  133904|   1969|[-0.2635574400598...|
|  134532|   1969|[0.05096873270432...|
+--------+-------+--------------------+
only showing top 20 rows

2020-06-01 02:40:43 WARN  TaskSetManager:66 - Stage 370 contains a task of very large size (7397 KB). The maximum recommended task size is 100 KB.
2020-06-01 02:40:48 WARN  TaskSetManager:66 - Stage 371 contains a task of very large size (7397 KB). The maximum recommended task size is 100 KB.
==================================================factor.movie_hot_sort==================================================
341869
+-------+--------------------+---+------+--------+
|    aid|               title|cid|weight|sort_num|
+-------+--------------------+---+------+--------+
|1220433|ROBLIX...|  0|    98|       1|
|1162502|  |  0|    33|       2|
|1231801|txf...|  0|    25|       3|
|1161506|02...|  0|    15|       4|
|1218177|...|  0|    13|       5|
|1161617|...|  0|    12|       6|
|1224441|...|  0|    11|       7|
|1164208|...|  0|     7|       8|
|1156459|...|  0|     7|       9|
|1305026|        |  0|     7|      10|
|1163896|...|  0|     6|      11|
|1149860|817...|  0|     6|      12|
|1222562|...|  0|     5|      13|
|1132343|...|  0|     5|      14|
|1232968|              |  0|     5|      15|
|1151678|...|  0|     5|      16|
|1303750|               |  0|     4|      17|
|1150528|...|  0|     4|      18|
|1161634|...|  0|     4|      19|
|1166197|...|  0|     4|      20|
+-------+--------------------+---+------+--------+
only showing top 20 rows

19026
2020-06-01 02:41:50 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:41:50 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:41:50 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:41:50 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:41:50 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:41:50 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:41:50 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 1039.1494398871398 msec.
==================================================factor.movie_hot_factor==================================================
341869
+-------+--------------------+---+--------+----------+--------+
|    aid|               title|cid|play_num|    factor|sort_num|
+-------+--------------------+---+--------+----------+--------+
|1220433|ROBLIX...|  0|      98|0.09217906|       1|
|1162502|  |  0|      33|0.09214995|       2|
|1231801|txf...|  0|      25|0.09214637|       3|
|1161506|02...|  0|      15| 0.0921419|       4|
|1218177|...|  0|      13|0.09214101|       5|
|1161617|...|  0|      12|0.09214056|       6|
|1224441|...|  0|      11|0.09214012|       7|
|1164208|...|  0|       7|0.09213833|       8|
|1156459|...|  0|       7|0.09213833|       9|
|1305026|        |  0|       7|0.09213833|      10|
|1163896|...|  0|       6|0.09213788|      11|
|1149860|817...|  0|       6|0.09213788|      12|
|1222562|...|  0|       5|0.09213744|      13|
|1132343|...|  0|       5|0.09213744|      14|
|1232968|              |  0|       5|0.09213744|      15|
|1151678|...|  0|       5|0.09213744|      16|
|1303750|               |  0|       4|0.09213699|      17|
|1150528|...|  0|       4|0.09213699|      18|
|1161634|...|  0|       4|0.09213699|      19|
|1166197|...|  0|       4|0.09213699|      20|
+-------+--------------------+---+--------+----------+--------+
only showing top 20 rows

7.314743127404829
2020-06-01 02:42:19 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:19 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:19 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:19 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:19 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:19 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:19 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 1222.4640402392436 msec.
==================================================factor.movie_score_factor==================================================
341869
+-------+--------+---+-----+------+--------+
|    aid|   title|cid|score|factor|sort_num|
+-------+--------+---+-----+------+--------+
|1271614||  0| 10.0|   1.0|       1|
|1271615||  0| 10.0|   1.0|       2|
|1271616||  0| 10.0|   1.0|       3|
|1271617||  0| 10.0|   1.0|       4|
|1271618||  0| 10.0|   1.0|       5|
|1271619||  0| 10.0|   1.0|       6|
|1271620||  0| 10.0|   1.0|       7|
|1271621| |  0| 10.0|   1.0|       8|
|1271622||  0| 10.0|   1.0|       9|
|1305067|  |  0|  9.5|0.7115|      10|
|1302743|    |  0|  9.3|0.6533|      11|
|1305098|   |  0|  9.2|0.6298|      12|
|1305107|     |  0|  9.2|0.6298|      13|
|1331679|  |  0|  9.2|0.6298|      14|
|1302865|    |  0|  9.1|0.6091|      15|
|1302810|     |  0|  9.0|0.5906|      16|
|1302888|    |  0|  9.0|0.5906|      17|
|1303629|  ALLOUT|  0|  9.0|0.5906|      18|
|1304987|   |  0|  9.0|0.5906|      19|
|1304988|    |  0|  9.0|0.5906|      20|
+-------+--------+---+-----+------+--------+
only showing top 20 rows

2020-06-01 02:42:47 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:47 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:47 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:47 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:47 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:47 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:47 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 1259.354707703418 msec.
==================================================factor.movie_year_factor==================================================
341869
+--------+----+------+
|movie_id|year|factor|
+--------+----+------+
|       8|2020|   1.0|
|      12|2015|0.7071|
|      57|2014|0.6598|
|     105|2016|0.7579|
|     157|2014|0.6598|
|     228|2014|0.6598|
|     517|2012|0.5743|
|     548|2009|0.4665|
|     571|2012|0.5743|
|     790|1997|0.2031|
|     818|2012|0.5743|
|     940|2015|0.7071|
|    1221|2015|0.7071|
|    1244|2014|0.6598|
|    1257|2014|0.6598|
|    1297|2017|0.8123|
|    1361|2015|0.7071|
|    1377|2017|0.8123|
|    1379|2016|0.7579|
|    1391|1999|0.2333|
+--------+----+------+
only showing top 20 rows

2020-06-01 02:43:12 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:43:12 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:43:12 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:43:12 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:43:12 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:43:12 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:43:12 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 2120.9840600580037 msec.
==================================================factor.movie_time==================================================
266701
+--------+---------+
|movie_id|movie_len|
+--------+---------+
|    2214|        0|
|    8484|       68|
|   95841|       90|
|   96165|       87|
|   96204|        0|
|   96359|      125|
|   98089|       45|
|   98426|        0|
|  101738|        1|
|  103809|       83|
|  103854|       19|
|  104529|       86|
|  104919|        0|
|  105657|        0|
|  105784|      153|
|  109192|        0|
|  112603|       86|
|  117180|      134|
|  117264|        0|
|  119319|       54|
+--------+---------+
only showing top 20 rows


update_movie_recall 27.781594383716584 
2020-06-01 02:57:47 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:57:47 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:57:47 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:57:47 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:57:47 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:57:47 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:57:47 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 1514.0510622786119 msec.
==================================================pre_user.user_click_35==================================================
750264
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|  datetime|
+-----------------+--------+-------+----------+
|4C:91:7A:C3:7D:26|  371302|   1970|1589990400|
|4C:91:7A:39:98:51|  938160|   1970|1589990400|
|4C:91:7A:71:5B:66| 1298428|   1970|1589990400|
|4C:91:7A:6E:AB:07| 1325674|   1969|1589990400|
|4C:91:7A:8E:B1:61|  932677|   1969|1589990400|
|4C:91:7A:C9:39:9A|  761429|   1969|1589990400|
|4C:91:7A:4E:27:9C| 1324430|   1970|1589990400|
|4C:91:7A:AA:6F:7A| 1139145|   1970|1589990400|
|4C:91:7A:25:9F:53| 1324818|   1971|1589990400|
|4C:91:7A:53:09:D2| 1267581|   1971|1589990400|
|4C:91:7A:53:09:D2| 1289934|   1971|1589990400|
|4C:91:7A:53:09:D2| 1332657|   1971|1589990400|
|4C:91:7A:73:90:F9|  105358|   1972|1589990400|
|4C:91:7A:A8:66:3C|  939589|   1969|1589990400|
|4C:91:7A:7D:38:9B| 1331284|   1973|1589990400|
|4C:91:7A:7D:38:9B| 1331333|   1973|1589990400|
|4C:91:7A:7D:38:9B|  884662|   1972|1589990400|
|4C:91:7A:37:89:4B|  172145|   1969|1589990400|
|4C:91:7A:37:89:4B|  590212|   1969|1589990400|
|4C:91:7A:1D:4B:E8| 1260511|   1972|1589990400|
+-----------------+--------+-------+----------+
only showing top 20 rows

==================================================pre_user.merge_click==================================================
15798777
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|  datetime|
+-----------------+--------+-------+----------+
|4C:91:7A:BB:C5:6E|  511000|      0|1559318400|
|4C:91:7A:BB:C5:6E| 1077842|      0|1559318400|
|4C:91:7A:77:6D:3B|  736033|   1969|1559318400|
|4C:91:7A:77:6D:3B|  754332|   1973|1559318400|
|4C:91:7A:77:6D:3B| 1077842|   1970|1559318400|
|80:0B:52:06:1F:95|  319578|   1969|1559318400|
|80:0B:52:10:AE:DF|  765403|   1969|1559318400|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400|
|80:0B:52:10:AE:DF|  997259|   1969|1559318400|
|80:0B:52:10:AE:DF| 1084636|   1969|1559318400|
|80:0B:52:10:AE:DF|  973986|   1969|1559318400|
|80:0B:52:10:AE:DF| 1088005|   1970|1559318400|
|80:0B:52:10:AE:DF| 1088005|   1970|1559318400|
|4C:91:7A:2A:61:0C|  997259|   1969|1559318400|
|4C:91:7A:1F:6C:D2|  997259|   1969|1559318400|
|80:0B:52:10:AE:4D| 1100215|   1970|1559318400|
|4C:91:7A:24:B0:27|      49|   2271|1559318400|
|4C:91:7A:24:B0:27|      26|   2271|1559318400|
+-----------------+--------+-------+----------+
only showing top 20 rows

2020-06-01 03:00:36 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:00:36 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:00:36 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:00:36 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:00:36 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:00:36 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:00:36 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 859.5405032310587 msec.
==================================================pre_user.user_top_35==================================================
9191
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|  datetime|
+-----------------+--------+-------+----------+
|4C:91:7A:FC:E5:0F|  981351|   1973|1589990400|
|4C:91:7A:C3:7B:E2| 1081950|   1970|1589990400|
|4C:91:7A:76:A5:B3| 1329495|   1970|1589990400|
|4C:91:7A:76:A5:B3| 1286974|   1970|1589990400|
|4C:91:7A:E9:6F:D3|  970100|   1969|1589990400|
|4C:91:7A:FC:E5:0F|  938012|   1973|1589990400|
|4C:91:7A:C3:7B:E2| 1284424|   1970|1589990400|
|4C:91:7A:C3:7B:E2|   95646|   1969|1589990400|
|4C:91:7A:C3:7B:E2| 1241542|   1969|1589990400|
|4C:91:7A:C3:7B:E2| 1306481|   1970|1589990400|
|4C:91:7A:C3:7B:E2| 1186911|   1970|1589990400|
|4C:91:7A:C3:7B:E2| 1081928|   1970|1589990400|
|4C:91:7A:C3:7B:E2| 1323403|   1970|1589990400|
|4C:91:7A:7A:2D:C6|  245532|   1973|1589990400|
|4C:91:7A:2C:09:D1| 1324818|   1971|1589990400|
|4C:91:7A:C3:7B:E2| 1189693|   1969|1589990400|
|4C:91:7A:3F:18:E0| 1221984|   1969|1589990400|
|4C:91:7A:D1:C5:7B|  320789|   1969|1589990400|
|4C:91:7A:69:C0:F5| 1267454|   1971|1589990400|
|4C:91:7A:9F:16:BC|  928968|   1970|1589990400|
+-----------------+--------+-------+----------+
only showing top 20 rows

==================================================pre_user.merge_top==================================================
206636
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|  datetime|
+-----------------+--------+-------+----------+
|4C:91:7A:0B:27:D9|  997259|   1969|1559318400|
|4C:91:7A:36:9C:23|      19|   2271|1559318400|
|4C:91:7A:36:9C:23| 1089783|      0|1559318400|
|4C:91:7A:24:B0:27| 1103390|      0|1559318400|
|4C:91:7A:24:B0:27| 1103390|      0|1559318400|
|4C:91:7A:24:B0:27| 1103390|      0|1559318400|
|4C:91:7A:24:B0:27| 1103390|      0|1559318400|
|4C:91:7A:56:63:55| 1082910|   1973|1559404800|
|4C:91:7A:56:63:55|  574779|   1973|1559404800|
|4C:91:7A:F2:44:34|  997259|   1969|1559491200|
|4C:91:7A:F2:44:34|  509420|   1969|1559491200|
|4C:91:7A:F2:44:34|  188413|   1969|1559491200|
|4C:91:7A:36:9C:23| 1091820|      0|1559491200|
|4C:91:7A:51:6E:D9|  987081|   1969|1559491200|
|4C:91:7A:51:6E:D9|  987081|   1969|1559491200|
|4C:91:7A:F2:44:34|  983559|   1969|1559491200|
|4C:91:7A:29:7D:88| 1085389|      0|1559577600|
|4C:91:7A:7E:4A:ED| 1104223|   1970|1559664000|
|4C:91:7A:7E:4A:ED|  966570|   1970|1559664000|
|4C:91:7A:7E:4A:ED| 1088358|   1969|1559664000|
+-----------------+--------+-------+----------+
only showing top 20 rows

2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.client.capability.check does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.false.positive.probability does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.broker.address.default does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.orc.time.counters does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve-fraction.min does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.ms.footer.cache.ppd.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.event.message.factory does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.metrics.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.hs2.user.access does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.storage.storageDirectory does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.liveness.connection.timeout.ms does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.dynamic.semijoin.reduction.threshold does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.connect.retry.limit does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.xmx.headroom does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.dynamic.semijoin.reduction does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.direct does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.stats does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.client.consistent.splits does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.session.lifetime does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.timedout.txn.reaper.start does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.ttl does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.management.acl does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.delegation.token.lifetime does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.guidKey does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.ats.hook.queue.capacity does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.large.query does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.bigtable.minsize.semijoin.reduction does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.alloc.min does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.user does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.alloc.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.wait.queue.comparator.class.name does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.port does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.orc.cache.use.soft.references does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve.fraction.max does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.listener.thread-count does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.container.max.java.heap.fraction does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.stats.column.autogather does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.am.liveness.heartbeat.interval.ms does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.decoding.metrics.percentiles.intervals does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.groupby.position.alias does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.txn.store.impl does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.groupby.shuffle does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.object.cache.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.parallel.ops.in.session does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.groupby.limit.extrastep does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.use.ssl does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.local does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.file.location does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.retry.delay.seconds does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.fileformat does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.num.file.cleaner.threads does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.test.fail.compaction does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.use.blobstore.as.scratchdir does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.class does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.mmap.path does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.download.permanent.fns does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.max.historic.queries does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.execution.reducesink.new.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.compactor.max.num.delta does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.attempted does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.port does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.compactor.initiator.failed.compacts.threshold does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.reporter does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.max.pending.writes does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.execution.mode does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.enable.grace.join.in.llap does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.memory.mode does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.threadpool.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.select.threshold does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.scratchdir.lock does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.use.spnego does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.file.frequency does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.hs2.coordinator.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.timeout.seconds does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.filter.stats.reduction does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.exec.orc.base.delta.ratio does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.fastpath does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.clear.dangling.scratchdir does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.test.fail.heartbeater does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.file.cleanup.delay.seconds does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.management.rpc.port does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.mapjoin.hybridgrace.bloomfilter does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.tree does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.stats.ndv.tuner does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.query.length does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.failed does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.close.session.on.disconnect does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.ppd.windowing does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.initial.metadata.count.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.host does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.ms.footer.cache.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.point.lookup.min does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.file.metadata.threads does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.service.refresh.interval.sec does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.max.output.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.driver.parallel.compilation does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.remote.token.requires.signing does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.bucket.pruning does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.cache.allow.synthetic.fileid does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.hash.table.inflation.factor does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.hbase.ttl does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.vectorized does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.writeset.reaper.interval does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.vector.serde.deserialize does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.order.columnalignment does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.send.buffer.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.exec.schema.evolution does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.elements.values.clause does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.llap.concurrent.queries does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.allow.uber does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.partition.size.max does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.auth does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.include.fileid does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.communicator.num.threads does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.orderby.position.alias does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.connection.sleep.between.retries.ms does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.max.partitions does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.hadoop2.component does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.yarn.shuffle.port does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.elements.in.clause does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.passiveWaitTimeMs does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.load.dynamic.partitions.thread does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.segments.granularity does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.http.response.header.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.conf.internal.variable.list does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose.reductionpercentage does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.retry.limit does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.serialize.in.tasks does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.query.timeout.seconds does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.hadoop2.frequency does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.directory.batch.ms does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.reader.wait does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.reenable.max.timeout.ms does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.max.open.txns does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.auto.convert.sortmerge.join.reduce.side does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.zookeeper.publish.configs does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.auto.convert.join.hashtable.max.entries does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.sessions.init.threads does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.authorization.storage.check.externaltable.drop does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.execution.mode does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.cbo.cnf.maxnodes does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.adaptor.usage.mode does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.rewriting does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.groupMembershipKey does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.catalog.cache.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.cbo.show.warnings does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.fshandler.threads does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.max.bloom.filter.entries does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.metadata.fraction does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.serde does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.scheduler.wait.queue.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.cache.entries does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.txn.operational.properties does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.memory.ttl does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.rpc.port does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.nonvector.wrapper.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.cache.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.vectorized.input.format does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.cte.materialize.threshold does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.clean.until does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.semijoin.conversion does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.port does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.spark.dynamic.partition.pruning does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.metrics.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.repl.rootdir does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.limit.partition.request does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.async.log.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.logger does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.allow.udf.load.on.demand does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.cli.tez.session.async does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.bloom.filter.factor does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.am-reporter.max.threads does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.file.size.for.mapjoin does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.bucketing does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.bucket.pruning.compat does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.spnego.principal does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.preemption.metrics.intervals does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.shuffle.dir.watcher.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.arena.count does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.use.SSL does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.connection.timeout.ms does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.transpose.aggr.join does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.maxTries does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.spark.dynamic.partition.pruning.max.data.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.metadata.base does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.invalidator.frequency does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.use.lrfu does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.mmap does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.coordinator.address.default does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.max.fetch.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.conf.hidden.list does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.io.sarg.cache.max.weight.mb does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.clear.dangling.scratchdir.interval does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.sleep.time does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.row.serde.deserialize does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.compile.lock.timeout does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.timedout.txn.reaper.interval does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.max.variance does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.lrfu.lambda does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.metadata.db.type does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.stream.timeout does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.transactional.events.mem does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.default.fetch.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.retain does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.merge.cardinality.check does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.groupClassKey does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.point.lookup does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.allow.permanent.fns does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.web.ssl does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.txn.manager.dump.lock.state.on.acquire.timeout does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.succeeded does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.use.fileid.path does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.slice.row.count does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.mapjoin.optimized.hashtable.probe.percent does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.select.distribute does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.use.fqdn does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.reenable.min.timeout.ms does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.validate.acls does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.support.special.characters.tablename does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.mv.files.thread does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.skip.compile.udf.check does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.vector.serde.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.interval does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.sleep.interval.between.start.attempts does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.yarn.container.mb does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.http.read.timeout does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.optimizations.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.orc.gap.cache does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.dynamic.partition.hashjoin does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.exec.copyfile.maxnumfiles does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.formats does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.http.numConnection does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.scheduler.enable.preemption does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.num.executors does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.full does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.connection.class does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.sessions.custom.queue.allowed does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.slice.lrr does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.password does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.writer.wait does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.http.request.header.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.max.threads does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose.reductiontuples does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.test.rollbacktxn does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.num.schedulable.tasks.per.node does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.acl does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.memory.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.type.safety does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.async.exec.async.compile does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.max.input.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.enable.memory.manager does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.msck.repair.batch.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.supported.schemes does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.allow.synthetic.fileid does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.stats.filter.in.factor does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.op.stats does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.exec.input.listing.max.threads does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.session.lifetime.jitter does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.web.port does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.cartesian.product does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.rpc.num.handlers does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.vcpus.per.instance does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.count.open.txns.interval does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.min.bloom.filter.entries does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.partition.columns.separate does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.orc.cache.stripe.details.mem.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.txn.heartbeat.threadpool.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.locality.delay does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.repl.cmrootdir does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.disable.backoff.factor does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.liveness.connection.sleep.between.retries.ms does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.spark.exec.inplace.progress does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.working.directory does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.memory.per.instance.mb does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.msck.path.validation does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve.fraction does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.merge.nway.joins does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.reaper.interval does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.txn.strict.locking.mode does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.vector.serde.async.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.input.generate.consistent.splits does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.in.place.progress does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.memory.rownum.max does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.xsrf.filter.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.alloc.max does not exist
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.3.4
      /_/

Using Python version 3.7.7 (default, Mar 26 2020 15:48:22)
SparkSession available as 'spark'.
2020-06-01 03:01:13 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:01:13 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:01:13 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:01:13 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:01:13 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:01:13 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:01:13 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 1898.520029949901 msec.
==================================================pre_user.user_play_35==================================================
3119264
+-----------------+--------+-------+---------+---------+----------+
|          user_id|movie_id|cate_id|play_time|movie_num|  datetime|
+-----------------+--------+-------+---------+---------+----------+
|4C:91:7A:C3:7D:26|  371302|   1970|  2305386|       35|1589990400|
|4C:91:7A:C3:7D:26|  371302|   1970|  4773890|       35|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  4338638|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  5068065|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  5797567|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  6526846|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  7256091|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  7985334|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  8714614|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  9443856|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973| 10173401|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973| 10902644|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973| 11631973|        1|1589990400|
|4C:91:7A:39:98:51| 1233848|   1972|  1216727|      720|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973| 14364906|       17|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973|   344060|       18|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973|   473599|       18|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973|  1063331|       18|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973|  1713674|       18|1589990400|
|4C:91:7A:39:98:51|  938160|   1970|     7883|        1|1589990400|
+-----------------+--------+-------+---------+---------+----------+
only showing top 20 rows

==================================================pre_user.merge_play==================================================
36644989
+-----------------+--------+-------+---------+---------+----------+
|          user_id|movie_id|cate_id|play_time|movie_num|  datetime|
+-----------------+--------+-------+---------+---------+----------+
|00:0C:18:EF:EE:E7|  226509|   1969|        2|      -10|1565712000|
|00:0C:18:EF:EE:E7|  226509|   1969|        2|      -10|1565712000|
|00:0C:18:EF:EE:E7|  143088|   1969|        6|      -10|1565712000|
|00:0C:18:EF:EE:E7|  992955|   1969|        6|      -10|1565712000|
|00:0C:18:EF:EE:E7| 1132226|   1970|       16|       13|1565712000|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       11|       31|1565798400|
|4C:91:7A:34:CA:3A|  987187|   1969|       53|      -10|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       28|       36|1565798400|
|4C:91:7A:29:7D:88|  927671|   1969|        6|      -10|1565884800|
|4C:91:7A:29:7D:88|  984018|   1969|        3|      -10|1565884800|
|4C:91:7A:29:7D:88|  984018|   1969|        3|      -10|1565884800|
|4C:91:7A:E3:F0:B6| 1156978|   1969|        2|      -10|1565884800|
|4C:91:7A:29:7D:88| 1138814|   1970|       38|        1|1565884800|
|4C:91:7A:35:60:C1|  978191|   1970|       20|        1|1565884800|
|4C:91:7A:35:60:C1|  978191|   1970|       20|        1|1565884800|
+-----------------+--------+-------+---------+---------+----------+
only showing top 20 rows

==================================================pre_user.merge_action==================================================
52650402
+-----------------+--------+-------+----------+-----+-----+---------+---------+
|          user_id|movie_id|cate_id|  datetime|click|  top|play_time|movie_num|
+-----------------+--------+-------+----------+-----+-----+---------+---------+
|4C:91:7A:BB:C5:6E|  511000|      0|1559318400| true|false|      -10|      -10|
|4C:91:7A:BB:C5:6E| 1077842|      0|1559318400| true|false|      -10|      -10|
|4C:91:7A:77:6D:3B|  736033|   1969|1559318400| true|false|      -10|      -10|
|4C:91:7A:77:6D:3B|  754332|   1973|1559318400| true|false|      -10|      -10|
|4C:91:7A:77:6D:3B| 1077842|   1970|1559318400| true|false|      -10|      -10|
|80:0B:52:06:1F:95|  319578|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF|  765403|   1969|1559318400| true|false|      -10|      -10|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400| true|false|      -10|      -10|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400| true|false|      -10|      -10|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF|  997259|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF| 1084636|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF|  973986|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF| 1088005|   1970|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF| 1088005|   1970|1559318400| true|false|      -10|      -10|
|4C:91:7A:2A:61:0C|  997259|   1969|1559318400| true|false|      -10|      -10|
|4C:91:7A:1F:6C:D2|  997259|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:4D| 1100215|   1970|1559318400| true|false|      -10|      -10|
|4C:91:7A:24:B0:27|      49|   2271|1559318400| true|false|      -10|      -10|
|4C:91:7A:24:B0:27|      26|   2271|1559318400| true|false|      -10|      -10|
+-----------------+--------+-------+----------+-----+-----+---------+---------+
only showing top 20 rows

==================================================factor.user_history_click==================================================
4766243
+--------+-----------------+-------+----------+--------------------+----+---------+
|movie_id|          user_id|cate_id|  datetime|               title|year|sort_time|
+--------+-----------------+-------+----------+--------------------+----+---------+
| 1286974|00:11:22:33:44:55|   1970|1588867200|                |2020|        1|
| 1311945|00:11:22:33:44:55|   1970|1588694400|                  |2020|        2|
|   98176|00:11:22:33:44:55|   1970|1587916800|          |2012|        3|
| 1286089|00:11:22:33:44:55|   1970|1585670400|             |2020|        4|
|  994596|00:24:68:D5:A7:72|   1971|1561737600|            3|2019|        1|
| 1228118|00:66:CE:0C:31:37|      0|1579104000|1-1.Dubbo+zookeee...|2019|        1|
| 1266797|00:66:CE:0C:31:37|      0|1579017600|               5|2020|        2|
|  105540|04:95:73:35:B5:7D|      0|1579104000|          |1993|        1|
|  142595|04:95:73:35:B5:7D|      0|1579104000|                |2015|        2|
| 1266684|04:95:73:35:B5:7D|      0|1579104000|               |2020|        3|
|  972618|04:95:73:35:B5:7D|      0|1579104000|            |2018|        4|
| 1109895|04:95:73:F8:D5:2E|   1970|1561651200|               |2019|        1|
|  997259|08:A5:C8:59:E2:C2|   1969|1561651200|               4|2019|        1|
| 1085389|0C:C6:55:AD:04:74|   1971|1561651200|             5|2019|        1|
|  971906|0C:C6:55:BF:BB:45|   1973|1561737600|              X|2018|        1|
| 1085389|0E:18:E2:37:97:82|   1971|1561564800|             5|2019|        1|
| 1089711|18:82:19:19:17:39|   2272|1561564800|      |2019|        1|
|  897593|18:82:19:1B:05:39|   1973|1561737600|                |2017|        1|
|  327574|18:82:19:1B:05:39|   1973|1561737600|          |2013|        2|
|  927941|18:82:19:1B:05:39|   1973|1561651200|               |2018|        3|
+--------+-----------------+-------+----------+--------------------+----+---------+
only showing top 20 rows

==================================================factor.user_history_top==================================================
126124
+--------+-----------------+-------+----------+-----------------+----+---------+
|movie_id|          user_id|cate_id|  datetime|            title|year|sort_time|
+--------+-----------------+-------+----------+-----------------+----+---------+
| 1087476|4C:91:7A:00:3B:82|      0|1563465600|             |2019|        1|
| 1112139|4C:91:7A:00:C7:D6|   1969|1572278400|             |2019|        1|
|  988774|4C:91:7A:01:1C:8D|   1969|1590768000|         |2018|        1|
|  804690|4C:91:7A:01:82:68|   1972|1590508800|    1|2016|        1|
|  987081|4C:91:7A:02:31:C7|   1969|1577548800|          |2018|        1|
| 1102963|4C:91:7A:02:31:C7|   1969|1575734400|             |2019|        2|
|  938839|4C:91:7A:02:31:C7|   1969|1575734400|           2|2019|        3|
| 1033987|4C:91:7A:03:4B:10|   1971|1583942400|           3|2019|        1|
| 1200865|4C:91:7A:03:4B:10|   1971|1583683200|      |2020|        2|
| 1129956|4C:91:7A:03:4B:10|   1971|1582819200|     3()|   0|        3|
| 1144115|4C:91:7A:03:4B:10|   1971|1582819200|        4|2016|        4|
| 1221969|4C:91:7A:03:4B:10|   1971|1582819200|              |2019|        5|
|  108278|4C:91:7A:03:4B:10|   1971|1580486400|         2|2015|        6|
| 1192084|4C:91:7A:04:31:F2|   1970|1575216000|           |2019|        1|
|  188773|4C:91:7A:04:DC:87|   1970|1582819200|            3|2009|        1|
|  917354|4C:91:7A:05:E3:2A|   1972|1590768000|       3|2017|        1|
|  991179|4C:91:7A:05:E3:2A|   1972|1587139200|StarTwinkle|2019|        2|
|  492089|4C:91:7A:05:E3:2A|   1972|1585238400|      Suite|2011|        3|
|  270722|4C:91:7A:05:E3:2A|   1972|1585238400|  Crystal3|2016|        4|
|  153105|4C:91:7A:05:E3:2A|   1972|1585238400|     Crystal|2014|        5|
+--------+-----------------+-------+----------+-----------------+----+---------+
only showing top 20 rows

==================================================factor.user_history_play==================================================
1721651
+--------+-----------------+-------+----------+---------------+----+---------+
|movie_id|          user_id|cate_id|  datetime|          title|year|sort_time|
+--------+-----------------+-------+----------+---------------+----+---------+
| 1286974|00:11:22:33:44:55|   1970|1588694400|           |2020|        1|
| 1311945|00:11:22:33:44:55|   1970|1588694400|             |2020|        2|
|   98176|00:11:22:33:44:55|   1970|1587830400|     |2012|        3|
|  113798|4C:91:7A:00:74:25|   1969|1581350400|           |2007|        1|
| 1122414|4C:91:7A:00:74:25|   1969|1581350400|            |2019|        2|
| 1235405|4C:91:7A:00:74:25|   1969|1581264000|         |2019|        3|
| 1192086|4C:91:7A:00:74:25|   1969|1580659200|           |2019|        4|
|  717396|4C:91:7A:00:74:25|   1969|1580659200|           |2014|        5|
|   95844|4C:91:7A:00:74:25|   1969|1580659200|           |2014|        6|
| 1081744|4C:91:7A:00:74:25|   1969|1580659200|           |2010|        7|
| 1236354|4C:91:7A:00:74:25|   1969|1580400000|           |2019|        8|
|  971886|4C:91:7A:00:74:25|   1969|1580054400|      1|2001|        9|
| 1130996|4C:91:7A:00:74:25|   1969|1580054400|           |2019|       10|
|  103287|4C:91:7A:00:AF:28|   1972|1588089600|           |1993|        1|
|  990193|4C:91:7A:00:D8:3F|   1970|1580313600|             |2019|        1|
| 1214970|4C:91:7A:01:51:D2|   1973|1589644800|        7|2019|        1|
| 1082077|4C:91:7A:01:51:D2|   1973|1589558400|2|2019|        2|
| 1300676|4C:91:7A:01:51:D2|   1973|1588435200|     3|2020|        3|
|  244404|4C:91:7A:01:51:D2|   1973|1587657600|         |2015|        4|
|  216817|4C:91:7A:01:51:D2|   1973|1586448000|      |2016|        5|
+--------+-----------------+-------+----------+---------------+----+---------+
only showing top 20 rows


update_user_history 44.22408875226974 
==================================================update_user.merge_action==================================================
5839264
+-----------------+--------+-------+----------+-----+-----+---------+---------+
|          user_id|movie_id|cate_id|  datetime|click|  top|play_time|movie_num|
+-----------------+--------+-------+----------+-----+-----+---------+---------+
|4C:91:7A:00:55:6E|  254913|   1973|1590422400| true|false|      -10|      -10|
|4C:91:7A:51:7A:4E|  789289|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:33:CF:3B|  190723|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:2F:EF:10|  244404|   1973|1590422400| true|false|      -10|      -10|
|4C:91:7A:0D:7A:E7|  142697|   1969|1590422400| true|false|      -10|      -10|
|4C:91:7A:83:D1:20| 1311945|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:73:94:23| 1337106|   1969|1590422400| true|false|      -10|      -10|
|4C:91:7A:52:10:0A|  149716|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:D5:7E:B8|  766804|   1969|1590422400| true|false|      -10|      -10|
|4C:91:7A:24:F4:4D|  789289|      0|1590422400| true|false|      -10|      -10|
|4C:91:7A:25:BA:5A|   96232|   1969|1590422400| true|false|      -10|      -10|
|4C:91:7A:8C:72:08| 1307048|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:8C:72:08| 1121378|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:8C:72:08|  784541|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:BC:DE:DB| 1329495|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:8C:72:08| 1307048|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:8C:72:08| 1323939|   1971|1590422400| true|false|      -10|      -10|
|4C:91:7A:8C:72:08| 1307164|   1971|1590422400| true|false|      -10|      -10|
|4C:91:7A:2A:B5:77| 1286977|   1971|1590422400| true|false|      -10|      -10|
|4C:91:7A:2A:B5:77| 1303173|   1973|1590422400| true|false|      -10|      -10|
+-----------------+--------+-------+----------+-----+-----+---------+---------+
only showing top 20 rows

2020-06-01 03:47:11 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:47:11 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:47:11 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:47:11 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:47:11 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:47:11 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:47:11 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 2166.558847762075 msec.
==================================================update_user.action_weight==================================================
222350
+-----------------+--------+------------------+-------+
|          user_id|movie_id|            weight|cate_id|
+-----------------+--------+------------------+-------+
|4C:91:7A:BB:DE:6E|  169663|33.952299999999994|   1970|
|4C:91:7A:74:1A:CE|  148993| 37.55709999999998|   1970|
|4C:91:7A:71:D3:B0|  917528|           26.6615|   1973|
|4C:91:7A:D8:F1:3E|      44| 4.752299999999999|   2271|
|4C:91:7A:38:8F:19| 1289125| 9.504599999999998|   1969|
|4C:91:7A:97:5B:D7|  115668|15.878799999999998|   1970|
|4C:91:7A:DA:17:8B| 1334823|           46.6042|   1970|
|4C:91:7A:D1:DB:8E|  990138|           14.2823|   1971|
|4C:91:7A:AD:CF:86|  107930| 4.752299999999999|   1972|
|4C:91:7A:FB:B6:9E| 1334823| 4.752299999999999|   1970|
|4C:91:7A:1C:B7:42|   97483| 4.752299999999999|   1970|
|4C:91:7A:17:86:30| 1140011| 4.752299999999999|   1972|
|4C:91:7A:82:6F:F9| 1336923| 4.752299999999999|   1969|
|4C:91:7A:61:63:38| 1138825| 7.175699999999996|   1972|
|4C:91:7A:88:98:53| 1147367|26.158300000000008|   1973|
|4C:91:7A:67:C2:6F|  793001|23.761499999999995|   1973|
|4C:91:7A:1D:0A:44| 1217713|20.685699999999994|   1972|
|4C:91:7A:D8:33:AA|  880843|11.881099999999998|   1969|
|4C:91:7A:DE:AC:AF|  582136|           28.6389|   1973|
|4C:91:7A:17:75:74|  586693|           19.0099|   1973|
+-----------------+--------+------------------+-------+
only showing top 20 rows

0.0011
2612.286999999998
==================================================update_user.action_weight_normal==================================================
222350
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|    weight|
+-----------------+--------+-------+----------+
|4C:91:7A:BB:DE:6E|  169663|   1970|0.01299674|
|4C:91:7A:74:1A:CE|  148993|   1970|0.01437668|
|4C:91:7A:71:D3:B0|  917528|   1973|0.01020577|
|4C:91:7A:D8:F1:3E|      44|   2271|0.00181879|
|4C:91:7A:38:8F:19| 1289125|   1969|  0.003638|
|4C:91:7A:97:5B:D7|  115668|   1970|0.00607809|
|4C:91:7A:DA:17:8B| 1334823|   1970|0.01783997|
|4C:91:7A:D1:DB:8E|  990138|   1971|0.00546694|
|4C:91:7A:AD:CF:86|  107930|   1972|0.00181879|
|4C:91:7A:FB:B6:9E| 1334823|   1970|0.00181879|
|4C:91:7A:1C:B7:42|   97483|   1970|0.00181879|
|4C:91:7A:17:86:30| 1140011|   1972|0.00181879|
|4C:91:7A:82:6F:F9| 1336923|   1969|0.00181879|
|4C:91:7A:61:63:38| 1138825|   1972|0.00274648|
|4C:91:7A:88:98:53| 1147367|   1973|0.01001315|
|4C:91:7A:67:C2:6F|  793001|   1973|0.00909564|
|4C:91:7A:1D:0A:44| 1217713|   1972| 0.0079182|
|4C:91:7A:D8:33:AA|  880843|   1969|0.00454774|
|4C:91:7A:DE:AC:AF|  582136|   1973|0.01096274|
|4C:91:7A:17:75:74|  586693|   1973|0.00727669|
+-----------------+--------+-------+----------+
only showing top 20 rows


update_user_profile 6.564022163550059 
==================================================update_user.user_similar_recall_1969==================================================
4708951
+-----------------+--------+-------------+----+---------+----------+-----+-----+--------+--------+----------+
|          user_id|movie_id|        title|year|movie_id2|    title2|year2|score|play_num|  weight|total_sort|
+-----------------+--------+-------------+----+---------+----------+-----+-----+--------+--------+----------+
|4C:91:7A:02:1F:50| 1008988||2003|   994213|   ()| 2018|  8.8|       3|6.111E-5|         1|
|4C:91:7A:02:1F:50| 1008988||2003|   612310|| 2017|  8.6|      38|5.828E-5|         2|
|4C:91:7A:02:1F:50| 1008988||2003|   964565|      | 2017|  8.6|       0|5.777E-5|         3|
|4C:91:7A:02:1F:50| 1008988||2003|   191171|    | 2017|  8.5|      18|5.574E-5|         4|
|4C:91:7A:02:1F:50| 1008988||2003|  1144039|      | 2019|  7.5|       1|5.504E-5|         5|
|4C:91:7A:02:1F:50| 1008988||2003|   890203|   | 2017|  8.4|       6|5.426E-5|         6|
|4C:91:7A:02:1F:50| 1008988||2003|   191298|      X| 2016|  8.7|       0|5.292E-5|         7|
|4C:91:7A:02:1F:50| 1008988||2003|   156939|        | 2016|  8.5|       0|5.203E-5|         8|
|4C:91:7A:02:1F:50| 1008988||2003|   933834|      | 2018|  7.6|       0|5.139E-5|         9|
|4C:91:7A:02:1F:50| 1008988||2003|   901713|       | 2018|  8.2|       6|5.069E-5|        10|
|4C:91:7A:02:1F:50| 1008988||2003|   927764|      | 2016|  8.8|      16|5.053E-5|        11|
|4C:91:7A:02:1F:50| 1008988||2003|   221178|    6| 2013|  9.2|       3|4.986E-5|        12|
|4C:91:7A:02:1F:50| 1008988||2003|   939252|        | 2017|  7.5|       0|4.779E-5|        13|
|4C:91:7A:02:1F:50| 1008988||2003|   766255|   | 2015|  8.4|       2|4.635E-5|        14|
|4C:91:7A:02:1F:50| 1008988||2003|  1337564|  (4K)| 2014|  8.5|       0|4.606E-5|        15|
|4C:91:7A:02:1F:50| 1008988||2003|   747777|       | 2015|  8.3|       0|4.563E-5|        16|
|4C:91:7A:02:1F:50| 1008988||2003|   559546|      | 2017|  7.5|      19|4.434E-5|        17|
|4C:91:7A:02:1F:50| 1008988||2003|   171066|  | 2015|  7.8|       2|4.269E-5|        18|
|4C:91:7A:02:1F:50| 1008988||2003|   511457|      | 2015|  7.6|       1|4.197E-5|        19|
|4C:91:7A:02:1F:50| 1008988||2003|    96288|     | 2013|  8.7|       0|3.991E-5|        20|
+-----------------+--------+-------------+----+---------+----------+-----+-----+--------+--------+----------+
only showing top 20 rows

==================================================update_user.user_similar_filter_same_recall_1969==================================================
4375999
+-----------------+--------+-------------+----+-----+--------+---------+--------------------+
|          user_id|movie_id|        title|year|score|play_num|   weight|          base_movie|
+-----------------+--------+-------------+----+-----+--------+---------+--------------------+
|00:22:33:44:55:66|  762602|    |1963|  7.8|       0|  3.72E-6|[[1269531, , ...|
|00:30:1B:BA:02:DB|    6954|           |1996|  8.2|       0| 7.492E-5|[[965305, , 20...|
|00:30:1B:BA:02:DB|  112919|      |2008|  7.0|       1| 8.505E-5|[[196680, ,...|
|00:30:1B:BA:02:DB|  114773|         |2006|  6.8|       0|  1.92E-5|[[965005, , 2...|
|00:30:1B:BA:02:DB|  133679|          |1983|  7.9|       3| 2.778E-5|[[134075, ...|
|00:30:1B:BA:02:DB|  397549|         |2014|  0.0|       0|7.0459E-4|[[1121176, , 2...|
|00:30:1B:BA:02:DB|  733250|        |2014|  7.0|       0|1.2043E-4|[[981932, , 2...|
|00:30:1B:BA:02:DB|  829993||2017|  7.4|       0| 3.727E-5|[[987140, , 20...|
|00:30:1B:BA:02:DB|  874933|         |2000|  7.5|       0| 9.017E-5|[[965305, , 20...|
|00:30:1B:BA:02:DB|  927242|       |1989|  7.6|       1|1.2051E-4|[[1338119, , ...|
|00:30:1B:BA:02:DB|  974075|         |2002|  7.9|       0|  8.79E-6|[[1340904, ...|
|00:30:1B:BA:02:DB|  982333|         |2017|  7.0|       0|1.6723E-4|[[196680, ,...|
|00:30:1B:BA:02:DB|  985668|          |2019|  7.5|       0|  5.06E-6|[[105897, , 2...|
|00:30:1B:BA:02:DB|  987558|          |1992|  5.8|       0|   2.1E-7|[[105897, , 2...|
|00:30:1B:BA:02:DB| 1119983|        |1970|  7.5|       0|  2.64E-6|[[993066, ...|
|00:30:1B:BA:02:DB| 1337461|         |1989|  7.2|       0|  3.04E-6|[[985095, ...|
|00:38:58:00:11:01|  140837|         |2013|  7.4|       0| 2.818E-5|[[800353, , ...|
|00:38:58:00:11:01|  895627|          |2005|  8.2|       0| 8.929E-5|[[157996, 2, ...|
|08:00:27:CD:BC:0C|  191370|           |2015|  8.4|       0| 5.197E-5|[[1269531, , ...|
|08:00:27:CD:BC:0C|  195402|       1949|2010|  6.7|       0| 1.839E-5|[[778689, , 2...|
+-----------------+--------+-------------+----+-----+--------+---------+--------------------+
only showing top 20 rows

==================================================update_user.user_similar_filter_history_recall_1969==================================================
4343553
+-----------------+--------+----------+----+-----+--------+--------+--------------------+--------+---------+
|          user_id|movie_id|     title|year|score|play_num|  weight|          base_movie|sort_num|timestamp|
+-----------------+--------+----------+----+-----+--------+--------+--------------------+--------+---------+
|4C:91:7A:02:1F:50|  994213|   ()|2018|  8.8|       3|6.111E-5|[[1008988, ...|       1| 20200601|
|4C:91:7A:02:1F:50|  612310||2017|  8.6|      38|5.828E-5|[[1008988, ...|       2| 20200601|
|4C:91:7A:02:1F:50|  964565|      |2017|  8.6|       0|5.777E-5|[[1008988, ...|       3| 20200601|
|4C:91:7A:02:1F:50|  191171|    |2017|  8.5|      18|5.574E-5|[[1008988, ...|       4| 20200601|
|4C:91:7A:02:1F:50| 1144039|      |2019|  7.5|       1|5.504E-5|[[1008988, ...|       5| 20200601|
|4C:91:7A:02:1F:50|  890203|   |2017|  8.4|       6|5.426E-5|[[1008988, ...|       6| 20200601|
|4C:91:7A:02:1F:50|  191298|      X|2016|  8.7|       0|5.292E-5|[[1008988, ...|       7| 20200601|
|4C:91:7A:02:1F:50|  156939|        |2016|  8.5|       0|5.203E-5|[[1008988, ...|       8| 20200601|
|4C:91:7A:02:1F:50|  933834|      |2018|  7.6|       0|5.139E-5|[[1008988, ...|       9| 20200601|
|4C:91:7A:02:1F:50|  901713|       |2018|  8.2|       6|5.069E-5|[[1008988, ...|      10| 20200601|
|4C:91:7A:02:1F:50|  927764|      |2016|  8.8|      16|5.053E-5|[[1008988, ...|      11| 20200601|
|4C:91:7A:02:1F:50|  221178|    6|2013|  9.2|       3|4.986E-5|[[1008988, ...|      12| 20200601|
|4C:91:7A:02:1F:50|  939252|        |2017|  7.5|       0|4.779E-5|[[1008988, ...|      13| 20200601|
|4C:91:7A:02:1F:50|  766255|   |2015|  8.4|       2|4.635E-5|[[1008988, ...|      14| 20200601|
|4C:91:7A:02:1F:50| 1337564|  (4K)|2014|  8.5|       0|4.606E-5|[[1008988, ...|      15| 20200601|
|4C:91:7A:02:1F:50|  747777|       |2015|  8.3|       0|4.563E-5|[[1008988, ...|      16| 20200601|
|4C:91:7A:02:1F:50|  559546|      |2017|  7.5|      19|4.434E-5|[[1008988, ...|      17| 20200601|
|4C:91:7A:02:1F:50|  171066|  |2015|  7.8|       2|4.269E-5|[[1008988, ...|      18| 20200601|
|4C:91:7A:02:1F:50|  511457|      |2015|  7.6|       1|4.197E-5|[[1008988, ...|      19| 20200601|
|4C:91:7A:02:1F:50|   96288|     |2013|  8.7|       0|3.991E-5|[[1008988, ...|      20| 20200601|
+-----------------+--------+----------+----+-----+--------+--------+--------------------+--------+---------+
only showing top 20 rows

==================================================update_user.user_similar_recall_1969_100==================================================
1413631
+-----------------+--------+----------+----+-----+--------+--------+--------------------+--------+---------+
|          user_id|movie_id|     title|year|score|play_num|  weight|          base_movie|sort_num|timestamp|
+-----------------+--------+----------+----+-----+--------+--------+--------------------+--------+---------+
|4C:91:7A:02:1F:50|  994213|   ()|2018|  8.8|       3|6.111E-5|[[1008988, ...|       1| 20200601|
|4C:91:7A:02:1F:50|  612310||2017|  8.6|      38|5.828E-5|[[1008988, ...|       2| 20200601|
|4C:91:7A:02:1F:50|  964565|      |2017|  8.6|       0|5.777E-5|[[1008988, ...|       3| 20200601|
|4C:91:7A:02:1F:50|  191171|    |2017|  8.5|      18|5.574E-5|[[1008988, ...|       4| 20200601|
|4C:91:7A:02:1F:50| 1144039|      |2019|  7.5|       1|5.504E-5|[[1008988, ...|       5| 20200601|
|4C:91:7A:02:1F:50|  890203|   |2017|  8.4|       6|5.426E-5|[[1008988, ...|       6| 20200601|
|4C:91:7A:02:1F:50|  191298|      X|2016|  8.7|       0|5.292E-5|[[1008988, ...|       7| 20200601|
|4C:91:7A:02:1F:50|  156939|        |2016|  8.5|       0|5.203E-5|[[1008988, ...|       8| 20200601|
|4C:91:7A:02:1F:50|  933834|      |2018|  7.6|       0|5.139E-5|[[1008988, ...|       9| 20200601|
|4C:91:7A:02:1F:50|  901713|       |2018|  8.2|       6|5.069E-5|[[1008988, ...|      10| 20200601|
|4C:91:7A:02:1F:50|  927764|      |2016|  8.8|      16|5.053E-5|[[1008988, ...|      11| 20200601|
|4C:91:7A:02:1F:50|  221178|    6|2013|  9.2|       3|4.986E-5|[[1008988, ...|      12| 20200601|
|4C:91:7A:02:1F:50|  939252|        |2017|  7.5|       0|4.779E-5|[[1008988, ...|      13| 20200601|
|4C:91:7A:02:1F:50|  766255|   |2015|  8.4|       2|4.635E-5|[[1008988, ...|      14| 20200601|
|4C:91:7A:02:1F:50| 1337564|  (4K)|2014|  8.5|       0|4.606E-5|[[1008988, ...|      15| 20200601|
|4C:91:7A:02:1F:50|  747777|       |2015|  8.3|       0|4.563E-5|[[1008988, ...|      16| 20200601|
|4C:91:7A:02:1F:50|  559546|      |2017|  7.5|      19|4.434E-5|[[1008988, ...|      17| 20200601|
|4C:91:7A:02:1F:50|  171066|  |2015|  7.8|       2|4.269E-5|[[1008988, ...|      18| 20200601|
|4C:91:7A:02:1F:50|  511457|      |2015|  7.6|       1|4.197E-5|[[1008988, ...|      19| 20200601|
|4C:91:7A:02:1F:50|   96288|     |2013|  8.7|       0|3.991E-5|[[1008988, ...|      20| 20200601|
+-----------------+--------+----------+----+-----+--------+--------+--------------------+--------+---------+
only showing top 20 rows


update_similar_recall 9.743529458840689 
