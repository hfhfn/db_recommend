2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.client.capability.check does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.false.positive.probability does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.broker.address.default does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.orc.time.counters does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve-fraction.min does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.ms.footer.cache.ppd.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.event.message.factory does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.metrics.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.hs2.user.access does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.storage.storageDirectory does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.liveness.connection.timeout.ms does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.dynamic.semijoin.reduction.threshold does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.connect.retry.limit does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.xmx.headroom does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.dynamic.semijoin.reduction does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.direct does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.stats does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.client.consistent.splits does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.session.lifetime does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.timedout.txn.reaper.start does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.ttl does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.management.acl does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.delegation.token.lifetime does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.guidKey does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.ats.hook.queue.capacity does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.large.query does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.bigtable.minsize.semijoin.reduction does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.alloc.min does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.user does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.alloc.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.wait.queue.comparator.class.name does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.port does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.cache.use.soft.references does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve.fraction.max does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.listener.thread-count does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.container.max.java.heap.fraction does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.stats.column.autogather does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.am.liveness.heartbeat.interval.ms does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.decoding.metrics.percentiles.intervals does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.groupby.position.alias does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.txn.store.impl does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.groupby.shuffle does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.object.cache.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.parallel.ops.in.session does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.groupby.limit.extrastep does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.use.ssl does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.local does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.file.location does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.retry.delay.seconds does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.fileformat does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.num.file.cleaner.threads does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.test.fail.compaction does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.use.blobstore.as.scratchdir does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.class does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.mmap.path does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.download.permanent.fns does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.max.historic.queries does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.execution.reducesink.new.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.max.num.delta does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.attempted does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.port does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.initiator.failed.compacts.threshold does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.reporter does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.max.pending.writes does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.execution.mode does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.enable.grace.join.in.llap does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.memory.mode does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.threadpool.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.select.threshold does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.scratchdir.lock does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.use.spnego does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.file.frequency does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.hs2.coordinator.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.timeout.seconds does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.filter.stats.reduction does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.exec.orc.base.delta.ratio does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.fastpath does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.clear.dangling.scratchdir does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.test.fail.heartbeater does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.file.cleanup.delay.seconds does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.management.rpc.port does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.mapjoin.hybridgrace.bloomfilter does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.tree does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.stats.ndv.tuner does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.query.length does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.failed does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.close.session.on.disconnect does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.ppd.windowing does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.initial.metadata.count.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.host does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.ms.footer.cache.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.point.lookup.min does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.file.metadata.threads does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.service.refresh.interval.sec does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.max.output.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.driver.parallel.compilation does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.remote.token.requires.signing does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.bucket.pruning does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.cache.allow.synthetic.fileid does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.hash.table.inflation.factor does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.hbase.ttl does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.vectorized does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.writeset.reaper.interval does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.vector.serde.deserialize does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.order.columnalignment does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.send.buffer.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.exec.schema.evolution does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.elements.values.clause does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.llap.concurrent.queries does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.allow.uber does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.partition.size.max does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.auth does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.include.fileid does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.communicator.num.threads does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.orderby.position.alias does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.connection.sleep.between.retries.ms does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.max.partitions does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.hadoop2.component does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.yarn.shuffle.port does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.elements.in.clause does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.passiveWaitTimeMs does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.load.dynamic.partitions.thread does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.segments.granularity does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.http.response.header.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.conf.internal.variable.list does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose.reductionpercentage does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.retry.limit does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.serialize.in.tasks does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.query.timeout.seconds does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.hadoop2.frequency does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.directory.batch.ms does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.reader.wait does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.reenable.max.timeout.ms does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.max.open.txns does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.auto.convert.sortmerge.join.reduce.side does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.zookeeper.publish.configs does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.auto.convert.join.hashtable.max.entries does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.sessions.init.threads does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.authorization.storage.check.externaltable.drop does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.execution.mode does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.cbo.cnf.maxnodes does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.adaptor.usage.mode does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.rewriting does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.groupMembershipKey does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.catalog.cache.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.cbo.show.warnings does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.fshandler.threads does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.max.bloom.filter.entries does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.metadata.fraction does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.serde does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.scheduler.wait.queue.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.cache.entries does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.txn.operational.properties does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.memory.ttl does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.rpc.port does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.nonvector.wrapper.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.cache.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.vectorized.input.format does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.cte.materialize.threshold does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.clean.until does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.semijoin.conversion does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.port does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.dynamic.partition.pruning does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.metrics.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.rootdir does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.limit.partition.request does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.async.log.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.logger does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.allow.udf.load.on.demand does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.cli.tez.session.async does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.bloom.filter.factor does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.am-reporter.max.threads does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.file.size.for.mapjoin does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.bucketing does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.bucket.pruning.compat does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.spnego.principal does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.preemption.metrics.intervals does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.shuffle.dir.watcher.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.arena.count does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.use.SSL does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.connection.timeout.ms does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.transpose.aggr.join does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.maxTries does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.dynamic.partition.pruning.max.data.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.metadata.base does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.invalidator.frequency does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.use.lrfu does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.mmap does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.coordinator.address.default does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.max.fetch.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.conf.hidden.list does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.io.sarg.cache.max.weight.mb does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.clear.dangling.scratchdir.interval does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.sleep.time does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.row.serde.deserialize does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.compile.lock.timeout does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.timedout.txn.reaper.interval does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.max.variance does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.lrfu.lambda does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.metadata.db.type does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.stream.timeout does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.transactional.events.mem does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.default.fetch.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.retain does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.merge.cardinality.check does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.groupClassKey does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.point.lookup does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.allow.permanent.fns does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.web.ssl does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.txn.manager.dump.lock.state.on.acquire.timeout does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.succeeded does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.use.fileid.path does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.slice.row.count does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.mapjoin.optimized.hashtable.probe.percent does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.select.distribute does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.use.fqdn does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.reenable.min.timeout.ms does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.validate.acls does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.support.special.characters.tablename does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.mv.files.thread does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.skip.compile.udf.check does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.vector.serde.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.interval does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.sleep.interval.between.start.attempts does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.yarn.container.mb does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.http.read.timeout does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.optimizations.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.orc.gap.cache does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.dynamic.partition.hashjoin does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.exec.copyfile.maxnumfiles does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.formats does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.http.numConnection does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.scheduler.enable.preemption does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.num.executors does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.full does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.connection.class does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.sessions.custom.queue.allowed does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.slice.lrr does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.password does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.writer.wait does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.http.request.header.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.max.threads does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose.reductiontuples does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.test.rollbacktxn does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.num.schedulable.tasks.per.node does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.acl does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.memory.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.type.safety does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.async.exec.async.compile does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.max.input.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.enable.memory.manager does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.msck.repair.batch.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.supported.schemes does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.allow.synthetic.fileid does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.stats.filter.in.factor does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.op.stats does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.exec.input.listing.max.threads does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.session.lifetime.jitter does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.web.port does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.cartesian.product does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.rpc.num.handlers does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.vcpus.per.instance does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.count.open.txns.interval does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.min.bloom.filter.entries does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.partition.columns.separate does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.cache.stripe.details.mem.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.txn.heartbeat.threadpool.size does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.locality.delay does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.cmrootdir does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.disable.backoff.factor does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.liveness.connection.sleep.between.retries.ms does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.exec.inplace.progress does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.working.directory does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.memory.per.instance.mb does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.msck.path.validation does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve.fraction does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.merge.nway.joins does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.reaper.interval does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.txn.strict.locking.mode does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.vector.serde.async.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.input.generate.consistent.splits does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.in.place.progress does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.memory.rownum.max does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.xsrf.filter.enabled does not exist
2020-05-31 02:30:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.alloc.max does not exist
2020-05-31 02:30:27 ERROR KeyProviderCache:87 - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
2020-05-31 02:46:57 WARN  TaskSetManager:66 - Stage 9 contains a task of very large size (7429 KB). The maximum recommended task size is 100 KB.
2020-05-31 02:47:09 WARN  TaskSetManager:66 - Stage 14 contains a task of very large size (3112 KB). The maximum recommended task size is 100 KB.
2020-05-31 02:47:22 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 02:47:22 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 02:47:22 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 02:47:22 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 02:47:22 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 02:47:22 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 02:47:22 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 469.508450623002 msec.
2020-05-31 03:23:10 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
2020-05-31 03:23:10 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
2020-05-31 03:24:06 WARN  TaskSetManager:66 - Stage 58 contains a task of very large size (5777 KB). The maximum recommended task size is 100 KB.
2020-05-31 03:24:18 WARN  TaskSetManager:66 - Stage 63 contains a task of very large size (11182 KB). The maximum recommended task size is 100 KB.
==================================================movie_recall.movie_vector_1969==================================================
48470
+--------+-------+--------------------+
|movie_id|cate_id|         movieVector|
+--------+-------+--------------------+
|   95526|   1969|[0.23771860052645...|
|   95841|   1969|[0.17754080359252...|
|   96165|   1969|[0.17630079222572...|
|   96204|   1969|[0.18295143597674...|
|   96359|   1969|[0.18900632141778...|
|  105657|   1969|[0.39322548967709...|
|  105784|   1969|[0.23620111830271...|
|  107108|   1969|[0.16201304576183...|
|  109192|   1969|[0.44579496613561...|
|  110942|   1969|[0.26769197148234...|
|  112603|   1969|[0.01049936843707...|
|  117180|   1969|[0.09482653237247...|
|  117264|   1969|[0.13695887875695...|
|  128784|   1969|[0.20858274922611...|
|  131797|   1969|[0.31830866695862...|
|  133142|   1969|[0.24694094740720...|
|  133164|   1969|[0.68205416575771...|
|  133382|   1969|[0.38621537197859...|
|  133904|   1969|[0.19140644934334...|
|  134532|   1969|[0.25569060864241...|
+--------+-------+--------------------+
only showing top 20 rows

2020-05-31 03:25:56 WARN  TaskSetManager:66 - Stage 78 contains a task of very large size (7397 KB). The maximum recommended task size is 100 KB.
2020-05-31 03:26:02 WARN  TaskSetManager:66 - Stage 79 contains a task of very large size (7397 KB). The maximum recommended task size is 100 KB.
==================================================factor.movie_hot_sort==================================================
341441
+-------+--------------------+---+------+--------+
|    aid|               title|cid|weight|sort_num|
+-------+--------------------+---+------+--------+
|1220433|蓝少ROBLIX：狼与猎人模拟器，...|  0|    98|       1|
|1162502|  乐乐偷喝可乐被妈妈打屁股宝宝巴士游戏|  0|    34|       2|
|1231801|入梦txf：《窗户》墙上画着一幅画...|  0|    26|       3|
|1161506|乐高无限02：我找到沙漠宝藏，里面...|  0|    16|       4|
|1218177|看见那些笔墨了吗？那是我在天上为你...|  0|    14|       5|
|1224441|和平精英：龙脊山有个“监狱地下室”...|  0|    11|       6|
|1161617|《迷你特工队动画》塞米宝宝和弗特宝...|  0|    10|       7|
|1164208|猫和老鼠：升级版莱特宁，捉鼠捉到飞...|  0|     8|       8|
|1305026|        海绵宝宝大电影：海绵出水|  0|     8|       9|
|1156459|猫和老鼠：遇上一只佛系莱特宁，捉鼠...|  0|     7|      10|
|1163896|人民的名义：得知是沙瑞金的电话，李...|  0|     6|      11|
|1232968|              会吃人的凉亭|  0|     6|      12|
|1149860|817小朋友色彩早教，机械手臂把白...|  0|     6|      13|
|1222562|西游记后传，悟空真实来历，悟空自己...|  0|     5|      14|
|1132343|王者别闹：诸葛亮设计，埋伏司马懿，...|  0|     5|      15|
|1151678|《迷你特工队动画》麦克斯抢到了许多...|  0|     5|      16|
|1303750|               超少年密码|  0|     4|      17|
|1150528|海绵宝宝趣味惊喜糖果盒子，派大星、...|  0|     4|      18|
|1161634|正阳门下：候大爷顿悟，感谢春明，并...|  0|     4|      19|
|1166197|传奇特工钱壮飞：不仅才华出众还曾挽...|  0|     4|      20|
+-------+--------------------+---+------+--------+
only showing top 20 rows

18210
2020-05-31 03:27:03 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:03 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:03 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:03 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:03 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:03 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:03 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 25.096518254027366 msec.
==================================================factor.movie_hot_factor==================================================
341441
+-------+--------------------+---+--------+----------+--------+
|    aid|               title|cid|play_num|    factor|sort_num|
+-------+--------------------+---+--------+----------+--------+
|1220433|蓝少ROBLIX：狼与猎人模拟器，...|  0|      98|0.09255501|       1|
|1162502|  乐乐偷喝可乐被妈妈打屁股宝宝巴士游戏|  0|      34|0.09252481|       2|
|1231801|入梦txf：《窗户》墙上画着一幅画...|  0|      26|0.09252104|       3|
|1161506|乐高无限02：我找到沙漠宝藏，里面...|  0|      16|0.09251633|       4|
|1218177|看见那些笔墨了吗？那是我在天上为你...|  0|      14|0.09251539|       5|
|1224441|和平精英：龙脊山有个“监狱地下室”...|  0|      11|0.09251398|       6|
|1161617|《迷你特工队动画》塞米宝宝和弗特宝...|  0|      10|0.09251351|       7|
|1164208|猫和老鼠：升级版莱特宁，捉鼠捉到飞...|  0|       8|0.09251257|       8|
|1305026|        海绵宝宝大电影：海绵出水|  0|       8|0.09251257|       9|
|1156459|猫和老鼠：遇上一只佛系莱特宁，捉鼠...|  0|       7| 0.0925121|      10|
|1163896|人民的名义：得知是沙瑞金的电话，李...|  0|       6|0.09251163|      11|
|1232968|              会吃人的凉亭|  0|       6|0.09251163|      12|
|1149860|817小朋友色彩早教，机械手臂把白...|  0|       6|0.09251163|      13|
|1222562|西游记后传，悟空真实来历，悟空自己...|  0|       5|0.09251116|      14|
|1132343|王者别闹：诸葛亮设计，埋伏司马懿，...|  0|       5|0.09251116|      15|
|1151678|《迷你特工队动画》麦克斯抢到了许多...|  0|       5|0.09251116|      16|
|1303750|               超少年密码|  0|       4|0.09251069|      17|
|1150528|海绵宝宝趣味惊喜糖果盒子，派大星、...|  0|       4|0.09251069|      18|
|1161634|正阳门下：候大爷顿悟，感谢春明，并...|  0|       4|0.09251069|      19|
|1166197|传奇特工钱壮飞：不仅才华出众还曾挽...|  0|       4|0.09251069|      20|
+-------+--------------------+---+--------+----------+--------+
only showing top 20 rows

7.314787520059517
2020-05-31 03:27:31 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:31 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:31 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:31 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:31 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:31 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:27:31 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 686.4449325040218 msec.
==================================================factor.movie_score_factor==================================================
341441
+-------+--------+---+-----+------+--------+
|    aid|   title|cid|score|factor|sort_num|
+-------+--------+---+-----+------+--------+
|1271614|安徽卫视春节联欢|  0| 10.0|   1.0|       1|
|1271615|天津卫视相声晚会|  0| 10.0|   1.0|       2|
|1271616|央视春节联欢晚会|  0| 10.0|   1.0|       3|
|1271617|湖南卫视春节联欢|  0| 10.0|   1.0|       4|
|1271618|吉林卫视春节联欢|  0| 10.0|   1.0|       5|
|1271619|山东卫视春节联欢|  0| 10.0|   1.0|       6|
|1271620|深圳卫视知识春晚|  0| 10.0|   1.0|       7|
|1271621| 北京台春节联欢|  0| 10.0|   1.0|       8|
|1271622|东方卫视春节联欢|  0| 10.0|   1.0|       9|
|1305067|  当幸福来敲门|  0|  9.5|0.7115|      10|
|1302743|    机器管家|  0|  9.3|0.6533|      11|
|1305098|   幸福终点站|  0|  9.2|0.6298|      12|
|1305107|     范海辛|  0|  9.2|0.6298|      13|
|1331679|  辛德勒的名单|  0|  9.2|0.6298|      14|
|1302865|    精灵旅社|  0|  9.1|0.6091|      15|
|1302810|     黑磨坊|  0|  9.0|0.5906|      16|
|1302888|    战地情人|  0|  9.0|0.5906|      17|
|1303629|  ALLOUT|  0|  9.0|0.5906|      18|
|1304987|   傲慢与偏见|  0|  9.0|0.5906|      19|
|1304988|    冥界警局|  0|  9.0|0.5906|      20|
+-------+--------+---+-----+------+--------+
only showing top 20 rows

2020-05-31 03:28:00 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:00 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:00 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:00 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:00 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:00 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:00 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 2016.5263020264213 msec.
==================================================factor.movie_year_factor==================================================
341441
+--------+----+------+
|movie_id|year|factor|
+--------+----+------+
|       8|2020|   1.0|
|      12|2015|0.7071|
|      57|2014|0.6598|
|     105|2016|0.7579|
|     157|2014|0.6598|
|     228|2014|0.6598|
|     517|2012|0.5743|
|     548|2009|0.4665|
|     571|2012|0.5743|
|     790|1997|0.2031|
|     818|2012|0.5743|
|     940|2015|0.7071|
|    1221|2015|0.7071|
|    1244|2014|0.6598|
|    1257|2014|0.6598|
|    1297|2017|0.8123|
|    1361|2015|0.7071|
|    1377|2017|0.8123|
|    1379|2016|0.7579|
|    1391|1999|0.2333|
+--------+----+------+
only showing top 20 rows

2020-05-31 03:28:24 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:24 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:24 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:24 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:24 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:24 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:28:24 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 2418.5998715120495 msec.
==================================================factor.movie_time==================================================
266411
+--------+---------+
|movie_id|movie_len|
+--------+---------+
|    2214|        0|
|    8484|       68|
|   95841|       90|
|   96165|       87|
|   96204|        0|
|   96359|      125|
|   98089|       45|
|   98426|        0|
|  101738|        1|
|  103809|       83|
|  103854|       19|
|  104529|       86|
|  104919|        0|
|  105657|        0|
|  105784|      153|
|  109192|        0|
|  112603|       86|
|  117180|      134|
|  117264|        0|
|  119319|       54|
+--------+---------+
only showing top 20 rows

电影召回更新完成
update_movie_recall 运行时长：72.83600540161133 分钟
2020-05-31 03:42:50 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:42:50 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:42:50 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:42:50 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:42:50 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:42:50 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:42:50 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 1104.358359198705 msec.
==================================================pre_user.user_click_35==================================================
675909
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|  datetime|
+-----------------+--------+-------+----------+
|4C:91:7A:C3:7D:26|  371302|   1970|1589990400|
|4C:91:7A:39:98:51|  938160|   1970|1589990400|
|4C:91:7A:71:5B:66| 1298428|   1970|1589990400|
|4C:91:7A:6E:AB:07| 1325674|   1969|1589990400|
|4C:91:7A:8E:B1:61|  932677|   1969|1589990400|
|4C:91:7A:C9:39:9A|  761429|   1969|1589990400|
|4C:91:7A:4E:27:9C| 1324430|   1970|1589990400|
|4C:91:7A:AA:6F:7A| 1139145|   1970|1589990400|
|4C:91:7A:25:9F:53| 1324818|   1971|1589990400|
|4C:91:7A:53:09:D2| 1267581|   1971|1589990400|
|4C:91:7A:53:09:D2| 1289934|   1971|1589990400|
|4C:91:7A:53:09:D2| 1332657|   1971|1589990400|
|4C:91:7A:73:90:F9|  105358|   1972|1589990400|
|4C:91:7A:A8:66:3C|  939589|   1969|1589990400|
|4C:91:7A:7D:38:9B| 1331284|   1973|1589990400|
|4C:91:7A:7D:38:9B| 1331333|   1973|1589990400|
|4C:91:7A:7D:38:9B|  884662|   1972|1589990400|
|4C:91:7A:37:89:4B|  172145|   1969|1589990400|
|4C:91:7A:37:89:4B|  590212|   1969|1589990400|
|4C:91:7A:1D:4B:E8| 1260511|   1972|1589990400|
+-----------------+--------+-------+----------+
only showing top 20 rows

==================================================pre_user.merge_click==================================================
15048513
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|  datetime|
+-----------------+--------+-------+----------+
|4C:91:7A:BB:C5:6E|  511000|      0|1559318400|
|4C:91:7A:BB:C5:6E| 1077842|      0|1559318400|
|4C:91:7A:77:6D:3B|  736033|   1969|1559318400|
|4C:91:7A:77:6D:3B|  754332|   1973|1559318400|
|4C:91:7A:77:6D:3B| 1077842|   1970|1559318400|
|80:0B:52:06:1F:95|  319578|   1969|1559318400|
|80:0B:52:10:AE:DF|  765403|   1969|1559318400|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400|
|80:0B:52:10:AE:DF|  997259|   1969|1559318400|
|80:0B:52:10:AE:DF| 1084636|   1969|1559318400|
|80:0B:52:10:AE:DF|  973986|   1969|1559318400|
|80:0B:52:10:AE:DF| 1088005|   1970|1559318400|
|80:0B:52:10:AE:DF| 1088005|   1970|1559318400|
|4C:91:7A:2A:61:0C|  997259|   1969|1559318400|
|4C:91:7A:1F:6C:D2|  997259|   1969|1559318400|
|80:0B:52:10:AE:4D| 1100215|   1970|1559318400|
|4C:91:7A:24:B0:27|      49|   2271|1559318400|
|4C:91:7A:24:B0:27|      26|   2271|1559318400|
+-----------------+--------+-------+----------+
only showing top 20 rows

2020-05-31 03:45:27 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:45:27 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:45:27 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:45:27 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:45:27 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:45:27 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:45:27 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 2164.982515704551 msec.
==================================================pre_user.user_top_35==================================================
8191
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|  datetime|
+-----------------+--------+-------+----------+
|4C:91:7A:FC:E5:0F|  981351|   1973|1589990400|
|4C:91:7A:C3:7B:E2| 1081950|   1970|1589990400|
|4C:91:7A:76:A5:B3| 1329495|   1970|1589990400|
|4C:91:7A:76:A5:B3| 1286974|   1970|1589990400|
|4C:91:7A:E9:6F:D3|  970100|   1969|1589990400|
|4C:91:7A:FC:E5:0F|  938012|   1973|1589990400|
|4C:91:7A:C3:7B:E2| 1284424|   1970|1589990400|
|4C:91:7A:C3:7B:E2|   95646|   1969|1589990400|
|4C:91:7A:C3:7B:E2| 1241542|   1969|1589990400|
|4C:91:7A:C3:7B:E2| 1306481|   1970|1589990400|
|4C:91:7A:C3:7B:E2| 1186911|   1970|1589990400|
|4C:91:7A:C3:7B:E2| 1081928|   1970|1589990400|
|4C:91:7A:C3:7B:E2| 1323403|   1970|1589990400|
|4C:91:7A:7A:2D:C6|  245532|   1973|1589990400|
|4C:91:7A:2C:09:D1| 1324818|   1971|1589990400|
|4C:91:7A:C3:7B:E2| 1189693|   1969|1589990400|
|4C:91:7A:3F:18:E0| 1221984|   1969|1589990400|
|4C:91:7A:D1:C5:7B|  320789|   1969|1589990400|
|4C:91:7A:69:C0:F5| 1267454|   1971|1589990400|
|4C:91:7A:9F:16:BC|  928968|   1970|1589990400|
+-----------------+--------+-------+----------+
only showing top 20 rows

==================================================pre_user.merge_top==================================================
197445
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|  datetime|
+-----------------+--------+-------+----------+
|4C:91:7A:0B:27:D9|  997259|   1969|1559318400|
|4C:91:7A:36:9C:23|      19|   2271|1559318400|
|4C:91:7A:36:9C:23| 1089783|      0|1559318400|
|4C:91:7A:24:B0:27| 1103390|      0|1559318400|
|4C:91:7A:24:B0:27| 1103390|      0|1559318400|
|4C:91:7A:24:B0:27| 1103390|      0|1559318400|
|4C:91:7A:24:B0:27| 1103390|      0|1559318400|
|4C:91:7A:56:63:55| 1082910|   1973|1559404800|
|4C:91:7A:56:63:55|  574779|   1973|1559404800|
|4C:91:7A:F2:44:34|  997259|   1969|1559491200|
|4C:91:7A:F2:44:34|  509420|   1969|1559491200|
|4C:91:7A:F2:44:34|  188413|   1969|1559491200|
|4C:91:7A:36:9C:23| 1091820|      0|1559491200|
|4C:91:7A:51:6E:D9|  987081|   1969|1559491200|
|4C:91:7A:51:6E:D9|  987081|   1969|1559491200|
|4C:91:7A:F2:44:34|  983559|   1969|1559491200|
|4C:91:7A:29:7D:88| 1085389|      0|1559577600|
|4C:91:7A:7E:4A:ED| 1104223|   1970|1559664000|
|4C:91:7A:7E:4A:ED|  966570|   1970|1559664000|
|4C:91:7A:7E:4A:ED| 1088358|   1969|1559664000|
+-----------------+--------+-------+----------+
only showing top 20 rows

2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.client.capability.check does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.false.positive.probability does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.broker.address.default does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.orc.time.counters does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve-fraction.min does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.ms.footer.cache.ppd.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.event.message.factory does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.metrics.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.hs2.user.access does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.storage.storageDirectory does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.liveness.connection.timeout.ms does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.dynamic.semijoin.reduction.threshold does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.connect.retry.limit does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.xmx.headroom does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.dynamic.semijoin.reduction does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.direct does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.stats does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.client.consistent.splits does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.session.lifetime does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.timedout.txn.reaper.start does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.ttl does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.management.acl does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.delegation.token.lifetime does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.guidKey does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.ats.hook.queue.capacity does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.large.query does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.bigtable.minsize.semijoin.reduction does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.alloc.min does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.user does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.alloc.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.wait.queue.comparator.class.name does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.port does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.cache.use.soft.references does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve.fraction.max does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.listener.thread-count does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.container.max.java.heap.fraction does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.stats.column.autogather does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.am.liveness.heartbeat.interval.ms does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.decoding.metrics.percentiles.intervals does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.groupby.position.alias does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.txn.store.impl does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.groupby.shuffle does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.object.cache.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.parallel.ops.in.session does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.groupby.limit.extrastep does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.use.ssl does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.local does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.file.location does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.retry.delay.seconds does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.fileformat does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.num.file.cleaner.threads does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.test.fail.compaction does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.use.blobstore.as.scratchdir does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.class does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.mmap.path does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.download.permanent.fns does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.max.historic.queries does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.execution.reducesink.new.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.max.num.delta does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.attempted does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.port does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.initiator.failed.compacts.threshold does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.reporter does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.max.pending.writes does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.execution.mode does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.enable.grace.join.in.llap does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.memory.mode does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.threadpool.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.select.threshold does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.scratchdir.lock does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.use.spnego does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.file.frequency does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.hs2.coordinator.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.timeout.seconds does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.filter.stats.reduction does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.exec.orc.base.delta.ratio does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.fastpath does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.clear.dangling.scratchdir does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.test.fail.heartbeater does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.file.cleanup.delay.seconds does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.management.rpc.port does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.mapjoin.hybridgrace.bloomfilter does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.tree does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.stats.ndv.tuner does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.query.length does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.failed does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.close.session.on.disconnect does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.ppd.windowing does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.initial.metadata.count.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.host does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.ms.footer.cache.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.point.lookup.min does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.file.metadata.threads does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.service.refresh.interval.sec does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.max.output.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.driver.parallel.compilation does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.remote.token.requires.signing does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.bucket.pruning does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.cache.allow.synthetic.fileid does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.hash.table.inflation.factor does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.hbase.ttl does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.vectorized does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.writeset.reaper.interval does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.vector.serde.deserialize does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.order.columnalignment does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.send.buffer.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.exec.schema.evolution does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.elements.values.clause does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.llap.concurrent.queries does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.allow.uber does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.partition.size.max does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.auth does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.include.fileid does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.communicator.num.threads does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.orderby.position.alias does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.connection.sleep.between.retries.ms does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.max.partitions does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.hadoop2.component does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.yarn.shuffle.port does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.elements.in.clause does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.passiveWaitTimeMs does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.load.dynamic.partitions.thread does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.segments.granularity does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.http.response.header.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.conf.internal.variable.list does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose.reductionpercentage does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.retry.limit does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.serialize.in.tasks does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.query.timeout.seconds does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.hadoop2.frequency does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.directory.batch.ms does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.reader.wait does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.reenable.max.timeout.ms does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.max.open.txns does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.auto.convert.sortmerge.join.reduce.side does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.zookeeper.publish.configs does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.auto.convert.join.hashtable.max.entries does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.sessions.init.threads does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.authorization.storage.check.externaltable.drop does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.execution.mode does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.cbo.cnf.maxnodes does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.adaptor.usage.mode does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.rewriting does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.groupMembershipKey does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.catalog.cache.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.cbo.show.warnings does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.fshandler.threads does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.max.bloom.filter.entries does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.metadata.fraction does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.serde does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.scheduler.wait.queue.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.cache.entries does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.txn.operational.properties does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.memory.ttl does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.rpc.port does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.nonvector.wrapper.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.cache.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.vectorized.input.format does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.cte.materialize.threshold does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.clean.until does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.semijoin.conversion does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.port does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.dynamic.partition.pruning does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.metrics.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.rootdir does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.limit.partition.request does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.async.log.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.logger does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.allow.udf.load.on.demand does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.cli.tez.session.async does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.bloom.filter.factor does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.am-reporter.max.threads does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.file.size.for.mapjoin does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.bucketing does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.bucket.pruning.compat does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.spnego.principal does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.preemption.metrics.intervals does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.shuffle.dir.watcher.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.arena.count does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.use.SSL does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.connection.timeout.ms does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.transpose.aggr.join does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.maxTries does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.dynamic.partition.pruning.max.data.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.metadata.base does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.invalidator.frequency does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.use.lrfu does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.mmap does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.coordinator.address.default does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.max.fetch.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.conf.hidden.list does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.io.sarg.cache.max.weight.mb does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.clear.dangling.scratchdir.interval does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.sleep.time does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.row.serde.deserialize does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.compile.lock.timeout does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.timedout.txn.reaper.interval does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.max.variance does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.lrfu.lambda does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.metadata.db.type does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.stream.timeout does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.transactional.events.mem does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.default.fetch.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.retain does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.merge.cardinality.check does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.groupClassKey does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.point.lookup does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.allow.permanent.fns does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.web.ssl does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.txn.manager.dump.lock.state.on.acquire.timeout does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.succeeded does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.use.fileid.path does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.slice.row.count does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.mapjoin.optimized.hashtable.probe.percent does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.select.distribute does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.use.fqdn does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.reenable.min.timeout.ms does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.validate.acls does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.support.special.characters.tablename does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.mv.files.thread does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.skip.compile.udf.check does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.vector.serde.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.interval does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.sleep.interval.between.start.attempts does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.yarn.container.mb does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.http.read.timeout does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.optimizations.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.orc.gap.cache does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.dynamic.partition.hashjoin does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.exec.copyfile.maxnumfiles does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.formats does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.http.numConnection does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.scheduler.enable.preemption does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.num.executors does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.full does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.connection.class does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.sessions.custom.queue.allowed does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.slice.lrr does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.password does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.writer.wait does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.http.request.header.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.max.threads does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose.reductiontuples does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.test.rollbacktxn does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.num.schedulable.tasks.per.node does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.acl does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.memory.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.type.safety does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.async.exec.async.compile does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.max.input.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.enable.memory.manager does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.msck.repair.batch.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.supported.schemes does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.allow.synthetic.fileid does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.stats.filter.in.factor does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.op.stats does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.exec.input.listing.max.threads does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.session.lifetime.jitter does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.web.port does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.cartesian.product does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.rpc.num.handlers does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.vcpus.per.instance does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.count.open.txns.interval does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.min.bloom.filter.entries does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.optimize.partition.columns.separate does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.orc.cache.stripe.details.mem.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.txn.heartbeat.threadpool.size does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.locality.delay does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.repl.cmrootdir does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.disable.backoff.factor does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.liveness.connection.sleep.between.retries.ms does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.spark.exec.inplace.progress does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.working.directory does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.memory.per.instance.mb does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.msck.path.validation does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve.fraction does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.merge.nway.joins does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.reaper.interval does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.txn.strict.locking.mode does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.vector.serde.async.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.tez.input.generate.consistent.splits does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.in.place.progress does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.memory.rownum.max does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.server2.xsrf.filter.enabled does not exist
2020-05-31 03:46:00 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.alloc.max does not exist
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.3.4
      /_/

Using Python version 3.7.7 (default, Mar 26 2020 15:48:22)
SparkSession available as 'spark'.
2020-05-31 03:46:00 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:46:00 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:46:00 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:46:00 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:46:00 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:46:00 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 03:46:00 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 2633.8572796639946 msec.
==================================================pre_user.user_play_35==================================================
2834016
+-----------------+--------+-------+---------+---------+----------+
|          user_id|movie_id|cate_id|play_time|movie_num|  datetime|
+-----------------+--------+-------+---------+---------+----------+
|4C:91:7A:C3:7D:26|  371302|   1970|  2305386|       35|1589990400|
|4C:91:7A:C3:7D:26|  371302|   1970|  4773890|       35|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  4338638|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  5068065|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  5797567|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  6526846|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  7256091|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  7985334|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  8714614|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  9443856|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973| 10173401|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973| 10902644|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973| 11631973|        1|1589990400|
|4C:91:7A:39:98:51| 1233848|   1972|  1216727|      720|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973| 14364906|       17|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973|   344060|       18|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973|   473599|       18|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973|  1063331|       18|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973|  1713674|       18|1589990400|
|4C:91:7A:39:98:51|  938160|   1970|     7883|        1|1589990400|
+-----------------+--------+-------+---------+---------+----------+
only showing top 20 rows

==================================================pre_user.merge_play==================================================
34639987
+-----------------+--------+-------+---------+---------+----------+
|          user_id|movie_id|cate_id|play_time|movie_num|  datetime|
+-----------------+--------+-------+---------+---------+----------+
|00:0C:18:EF:EE:E7|  226509|   1969|        2|      -10|1565712000|
|00:0C:18:EF:EE:E7|  226509|   1969|        2|      -10|1565712000|
|00:0C:18:EF:EE:E7|  143088|   1969|        6|      -10|1565712000|
|00:0C:18:EF:EE:E7|  992955|   1969|        6|      -10|1565712000|
|00:0C:18:EF:EE:E7| 1132226|   1970|       16|       13|1565712000|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       11|       31|1565798400|
|4C:91:7A:34:CA:3A|  987187|   1969|       53|      -10|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       28|       36|1565798400|
|4C:91:7A:29:7D:88|  927671|   1969|        6|      -10|1565884800|
|4C:91:7A:29:7D:88|  984018|   1969|        3|      -10|1565884800|
|4C:91:7A:29:7D:88|  984018|   1969|        3|      -10|1565884800|
|4C:91:7A:E3:F0:B6| 1156978|   1969|        2|      -10|1565884800|
|4C:91:7A:29:7D:88| 1138814|   1970|       38|        1|1565884800|
|4C:91:7A:35:60:C1|  978191|   1970|       20|        1|1565884800|
|4C:91:7A:35:60:C1|  978191|   1970|       20|        1|1565884800|
+-----------------+--------+-------+---------+---------+----------+
only showing top 20 rows

==================================================pre_user.merge_action==================================================
49885945
+-----------------+--------+-------+----------+-----+-----+---------+---------+
|          user_id|movie_id|cate_id|  datetime|click|  top|play_time|movie_num|
+-----------------+--------+-------+----------+-----+-----+---------+---------+
|4C:91:7A:BB:C5:6E|  511000|      0|1559318400| true|false|      -10|      -10|
|4C:91:7A:BB:C5:6E| 1077842|      0|1559318400| true|false|      -10|      -10|
|4C:91:7A:77:6D:3B|  736033|   1969|1559318400| true|false|      -10|      -10|
|4C:91:7A:77:6D:3B|  754332|   1973|1559318400| true|false|      -10|      -10|
|4C:91:7A:77:6D:3B| 1077842|   1970|1559318400| true|false|      -10|      -10|
|80:0B:52:06:1F:95|  319578|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF|  765403|   1969|1559318400| true|false|      -10|      -10|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400| true|false|      -10|      -10|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400| true|false|      -10|      -10|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF|  997259|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF| 1084636|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF|  973986|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF| 1088005|   1970|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF| 1088005|   1970|1559318400| true|false|      -10|      -10|
|4C:91:7A:2A:61:0C|  997259|   1969|1559318400| true|false|      -10|      -10|
|4C:91:7A:1F:6C:D2|  997259|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:4D| 1100215|   1970|1559318400| true|false|      -10|      -10|
|4C:91:7A:24:B0:27|      49|   2271|1559318400| true|false|      -10|      -10|
|4C:91:7A:24:B0:27|      26|   2271|1559318400| true|false|      -10|      -10|
+-----------------+--------+-------+----------+-----+-----+---------+---------+
only showing top 20 rows

==================================================factor.user_history_click==================================================
4739571
+--------+-----------------+-------+----------+--------------------+----+---------+
|movie_id|          user_id|cate_id|  datetime|               title|year|sort_time|
+--------+-----------------+-------+----------+--------------------+----+---------+
| 1286974|00:11:22:33:44:55|   1970|1588867200|                天醒之路|2020|        1|
| 1311945|00:11:22:33:44:55|   1970|1588694400|                  猎狐|2020|        2|
|   98176|00:11:22:33:44:55|   1970|1587916800|          我是特种兵之利刃出鞘|2012|        3|
| 1286089|00:11:22:33:44:55|   1970|1585670400|             如果岁月可回头|2020|        4|
|  994596|00:24:68:D5:A7:72|   1971|1561737600|            向往的生活第3季|2019|        1|
| 1228118|00:66:CE:0C:31:37|      0|1579104000|1-1.Dubbo+zookeee...|2019|        1|
| 1266797|00:66:CE:0C:31:37|      0|1579017600|               爱情公寓5|2020|        2|
|  105540|04:95:73:35:B5:7D|      0|1579104000|          倚天屠龙记之魔教教主|1993|        1|
|  142595|04:95:73:35:B5:7D|      0|1579104000|                百团大战|2015|        2|
| 1266684|04:95:73:35:B5:7D|      0|1579104000|               狄仁杰探案|2020|        3|
|  972618|04:95:73:35:B5:7D|      0|1579104000|            狄仁杰之蚩尤血藤|2018|        4|
| 1109895|04:95:73:F8:D5:2E|   1970|1561651200|               动物管理局|2019|        1|
|  997259|08:A5:C8:59:E2:C2|   1969|1561651200|               反贪风暴4|2019|        1|
| 1085389|0C:C6:55:AD:04:74|   1971|1561651200|             极限挑战第5季|2019|        1|
|  971906|0C:C6:55:BF:BB:45|   1973|1561737600|              迷你特工队X|2018|        1|
| 1085389|0E:18:E2:37:97:82|   1971|1561564800|             极限挑战第5季|2019|        1|
| 1089711|18:82:19:19:17:39|   2272|1561564800|      张雪峰手把手教你填报高考志愿|2019|        1|
|  897593|18:82:19:1B:05:39|   1973|1561737600|                萌鸡小队|2017|        1|
|  327574|18:82:19:1B:05:39|   1973|1561737600|          新大头儿子和小头爸爸|2013|        2|
|  927941|18:82:19:1B:05:39|   1973|1561651200|               睡衣小英雄|2018|        3|
+--------+-----------------+-------+----------+--------------------+----+---------+
only showing top 20 rows

==================================================factor.user_history_top==================================================
125337
+--------+-----------------+-------+----------+-----------------+----+---------+
|movie_id|          user_id|cate_id|  datetime|            title|year|sort_time|
+--------+-----------------+-------+----------+-----------------+----+---------+
| 1087476|4C:91:7A:00:3B:82|      0|1563465600|             筑梦情缘|2019|        1|
| 1112139|4C:91:7A:00:C7:D6|   1969|1572278400|             烈火英雄|2019|        1|
|  988774|4C:91:7A:01:1C:8D|   1969|1590768000|         我想吃掉你的胰脏|2018|        1|
|  804690|4C:91:7A:01:82:68|   1972|1590508800|    舞法天女朵法拉第1季（上）|2016|        1|
|  987081|4C:91:7A:02:31:C7|   1969|1577548800|          波西米亚狂想曲|2018|        1|
| 1102963|4C:91:7A:02:31:C7|   1969|1575734400|             雪人奇缘|2019|        2|
|  938839|4C:91:7A:02:31:C7|   1969|1575734400|           乐高大电影2|2019|        3|
| 1033987|4C:91:7A:03:4B:10|   1971|1583942400|           奔跑吧第3季|2019|        1|
| 1200865|4C:91:7A:03:4B:10|   1971|1583683200|      《奔跑吧兄弟》精彩合集|2020|        2|
| 1129956|4C:91:7A:03:4B:10|   1971|1582819200|     奔跑吧第3季(爆笑时刻)|   0|        3|
| 1144115|4C:91:7A:03:4B:10|   1971|1582819200|        奔跑吧兄弟第4季度|2016|        4|
| 1221969|4C:91:7A:03:4B:10|   1971|1582819200|              追我吧|2019|        5|
|  108278|4C:91:7A:03:4B:10|   1971|1580486400|         奔跑吧兄弟第2季|2015|        6|
| 1192084|4C:91:7A:04:31:F2|   1970|1575216000|           初恋那件小事|2019|        1|
|  188773|4C:91:7A:04:DC:87|   1970|1582819200|            重案六组3|2009|        1|
|  917354|4C:91:7A:05:E3:2A|   1972|1590768000|       星学院3之潘朵拉秘境|2017|        1|
|  991179|4C:91:7A:05:E3:2A|   1972|1587139200|Star☆Twinkle光之美少女|2019|        2|
|  492089|4C:91:7A:05:E3:2A|   1972|1585238400|      Suite光之美少女♪|2011|        3|
|  270722|4C:91:7A:05:E3:2A|   1972|1585238400|  美少女战士Crystal第3季|2016|        4|
|  153105|4C:91:7A:05:E3:2A|   1972|1585238400|     美少女战士Crystal|2014|        5|
+--------+-----------------+-------+----------+-----------------+----+---------+
only showing top 20 rows

==================================================factor.user_history_play==================================================
1709391
+--------+-----------------+-------+----------+---------------+----+---------+
|movie_id|          user_id|cate_id|  datetime|          title|year|sort_time|
+--------+-----------------+-------+----------+---------------+----+---------+
| 1286974|00:11:22:33:44:55|   1970|1588694400|           天醒之路|2020|        1|
| 1311945|00:11:22:33:44:55|   1970|1588694400|             猎狐|2020|        2|
|   98176|00:11:22:33:44:55|   1970|1587830400|     我是特种兵之利刃出鞘|2012|        3|
|  113798|4C:91:7A:00:74:25|   1969|1581350400|           我是传奇|2007|        1|
| 1122414|4C:91:7A:00:74:25|   1969|1581350400|            冰峰暴|2019|        2|
| 1235405|4C:91:7A:00:74:25|   1969|1581264000|         亲爱的新年好|2019|        3|
| 1192086|4C:91:7A:00:74:25|   1969|1580659200|           犯罪现场|2019|        4|
|  717396|4C:91:7A:00:74:25|   1969|1580659200|           星际穿越|2014|        5|
|   95844|4C:91:7A:00:74:25|   1969|1580659200|           一号目标|2014|        6|
| 1081744|4C:91:7A:00:74:25|   1969|1580659200|           孤胆特工|2010|        7|
| 1236354|4C:91:7A:00:74:25|   1969|1580400000|           半个喜剧|2019|        8|
|  971886|4C:91:7A:00:74:25|   1969|1580054400|      指环王1：魔戒再现|2001|        9|
| 1130996|4C:91:7A:00:74:25|   1969|1580054400|           中国机长|2019|       10|
|  103287|4C:91:7A:00:AF:28|   1972|1588089600|           灌篮高手|1993|        1|
|  990193|4C:91:7A:00:D8:3F|   1970|1580313600|             招摇|2019|        1|
| 1214970|4C:91:7A:01:51:D2|   1973|1589644800|        超级飞侠第7季|2019|        1|
| 1082077|4C:91:7A:01:51:D2|   1973|1589558400|伊恩庄园启蒙英文儿歌精选第2季|2019|        2|
| 1300676|4C:91:7A:01:51:D2|   1973|1588435200|     瑞奇宝宝第3季中文版|2020|        3|
|  244404|4C:91:7A:01:51:D2|   1973|1587657600|         宝宝巴士儿歌|2015|        4|
|  216817|4C:91:7A:01:51:D2|   1973|1586448000|      芭比之梦想豪宅全集|2016|        5|
+--------+-----------------+-------+----------+---------------+----+---------+
only showing top 20 rows

用户历史更新完成
update_user_history 运行时长：41.57600565751394 分钟
==================================================update_user.merge_action==================================================
5838420
+-----------------+--------+-------+----------+-----+-----+---------+---------+
|          user_id|movie_id|cate_id|  datetime|click|  top|play_time|movie_num|
+-----------------+--------+-------+----------+-----+-----+---------+---------+
|4C:91:7A:F3:EA:5F| 1302343|   1971|1590336000| true|false|      -10|      -10|
|4C:91:7A:16:C5:F1| 1286974|   1970|1590336000| true|false|      -10|      -10|
|4C:91:7A:E6:AF:9B|  203962|   1973|1590336000| true|false|      -10|      -10|
|4C:91:7A:16:C5:F1|  789289|   1970|1590336000| true|false|      -10|      -10|
|4C:91:7A:16:C5:F1| 1324430|   1970|1590336000| true|false|      -10|      -10|
|4C:91:7A:C9:B5:50| 1324818|   1971|1590336000| true|false|      -10|      -10|
|4C:91:7A:CE:2D:49|  191686|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:CE:2D:49|  929092|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:AB:10:23|  765286|   1973|1590336000| true|false|      -10|      -10|
|4C:91:7A:54:BB:6C|  717166|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:54:BB:6C|  732487|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:6C:65:D9|   96360|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:6C:65:D9|  789289|   1970|1590336000| true|false|      -10|      -10|
|4C:91:7A:38:5C:72| 1328252|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:6C:65:D9|  789289|   1970|1590336000| true|false|      -10|      -10|
|4C:91:7A:6C:65:D9|  789289|   1970|1590336000| true|false|      -10|      -10|
|4C:91:7A:24:86:71|  108164|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:24:86:71|  747492|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:24:86:71|  113983|   1969|1590336000| true|false|      -10|      -10|
|4C:91:7A:A2:27:06|  154385|      0|1590336000| true|false|      -10|      -10|
+-----------------+--------+-------+----------+-----+-----+---------+---------+
only showing top 20 rows

2020-05-31 04:29:20 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 04:29:20 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 04:29:20 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 04:29:20 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 04:29:20 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 04:29:20 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-05-31 04:29:20 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 2626.0906198378857 msec.
==================================================update_user.action_weight==================================================
215175
+-----------------+--------+------------------+-------+
|          user_id|movie_id|            weight|cate_id|
+-----------------+--------+------------------+-------+
|4C:91:7A:FB:7F:EC|  142940|10.183799999999998|   1969|
|4C:91:7A:C7:F9:DE|  575643| 67.15499999999996|   1970|
|4C:91:7A:29:42:F8| 1083262|12.220199999999997|   1969|
|4C:91:7A:08:7B:48|  997501|4.0733999999999995|   1973|
|4C:91:7A:70:47:CE| 1337100| 8.147400000000001|   1971|
|4C:91:7A:3C:6E:8F|  991362|4.0733999999999995|   1973|
|4C:91:7A:E6:A1:01|  244404|13.802799999999992|   1973|
|4C:91:7A:E0:76:AE|  949244| 51.93660000000001|   1973|
|4C:91:7A:BF:C4:72|  153232|21.323000000000004|   1972|
|4C:91:7A:C2:A5:41|  935968|4.0733999999999995|   1969|
|4C:91:7A:76:A6:3E| 1129516|4.0733999999999995|   1971|
|4C:91:7A:80:1B:6E| 1311945|           79.1526|   1970|
|4C:91:7A:D4:E6:56|  204212| 9.827300000000003|   1973|
|4C:91:7A:1C:B7:42|  108978|4.0733999999999995|   1969|
|4C:91:7A:66:3C:05| 1229066|18.212199999999985|   1973|
|4C:91:7A:06:76:67|  376781|12.428399999999996|   1970|
|4C:91:7A:4E:87:0F|  203286|           46.8456|   1973|
|4C:91:7A:44:E7:8A|  878564|30.551399999999994|   1972|
|4C:91:7A:24:67:D8| 1181218|4.0733999999999995|   1970|
|4C:91:7A:82:68:37|  935895| 45.23819999999999|   1973|
+-----------------+--------+------------------+-------+
only showing top 20 rows

0.0016
2435.8332
==================================================update_user.action_weight_normal==================================================
215175
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|    weight|
+-----------------+--------+-------+----------+
|4C:91:7A:FB:7F:EC|  142940|   1969|0.00418017|
|4C:91:7A:C7:F9:DE|  575643|   1970|0.02756898|
|4C:91:7A:29:42:F8| 1083262|   1969|0.00501619|
|4C:91:7A:08:7B:48|  997501|   1973|0.00167163|
|4C:91:7A:70:47:CE| 1337100|   1971|0.00334416|
|4C:91:7A:3C:6E:8F|  991362|   1973|0.00167163|
|4C:91:7A:E6:A1:01|  244404|   1973|0.00566591|
|4C:91:7A:E0:76:AE|  949244|   1973|0.02132126|
|4C:91:7A:BF:C4:72|  153232|   1972|0.00875323|
|4C:91:7A:C2:A5:41|  935968|   1969|0.00167163|
|4C:91:7A:76:A6:3E| 1129516|   1971|0.00167163|
|4C:91:7A:80:1B:6E| 1311945|   1970|0.03249445|
|4C:91:7A:D4:E6:56|  204212|   1973|0.00403382|
|4C:91:7A:1C:B7:42|  108978|   1969|0.00167163|
|4C:91:7A:66:3C:05| 1229066|   1973|0.00747613|
|4C:91:7A:06:76:67|  376781|   1970|0.00510167|
|4C:91:7A:4E:87:0F|  203286|   1973|0.01923121|
|4C:91:7A:44:E7:8A|  878564|   1972|0.01254184|
|4C:91:7A:24:67:D8| 1181218|   1970|0.00167163|
|4C:91:7A:82:68:37|  935895|   1973|0.01857132|
+-----------------+--------+-------+----------+
only showing top 20 rows

用户画像更新完成
update_user_profile 运行时长：6.3613568862279255 分钟
==================================================update_user.user_similar_recall_1969==================================================
4491399
+-----------------+--------+------+----+---------+---------+-----+-----+--------+---------+----------+
|          user_id|movie_id| title|year|movie_id2|   title2|year2|score|play_num|   weight|total_sort|
+-----------------+--------+------+----+---------+---------+-----+-----+--------+---------+----------+
|4C:91:7A:12:DF:95|  135239|战火中的青春|1959|  1112139|     烈火英雄| 2019|  8.7|      41|2.1777E-4|         1|
|4C:91:7A:12:DF:95|  135239|战火中的青春|1959|   929382|    我不是药神| 2018|  9.1|      18|2.1269E-4|         2|
|4C:91:7A:12:DF:95|  135239|战火中的青春|1959|  1130996|     中国机长| 2019|  8.8|      83|2.1233E-4|         3|
|4C:91:7A:12:DF:95|  135239|战火中的青春|1959|   846996|      空天猎| 2017|  8.3|       9|  1.77E-4|         4|
|4C:91:7A:12:DF:95|  135239|战火中的青春|1959|   226812|     铁道飞虎| 2016|  8.7|      11|1.7348E-4|         5|
|4C:91:7A:12:DF:95|  135239|战火中的青春|1959|   715777|  人再囧途之泰囧| 2012|  9.1|       3|1.5238E-4|         6|
|4C:91:7A:12:DF:95|  135239|战火中的青春|1959|   968610|     刀背藏身| 2017|  7.3|       0|1.2465E-4|         7|
|4C:91:7A:12:DF:95|  135239|战火中的青春|1959|  1062210|    唐山大地震| 2010|  9.1|      19|1.2133E-4|         8|
|4C:91:7A:12:DF:95|  135108|  红鹰展翅|1960|   972249|痞子熊兵之杀狼祭天| 2018|  0.0|       2|1.1859E-4|         9|
|4C:91:7A:12:DF:95|  135239|战火中的青春|1959|  1106506|一八九四甲午大海战| 2012|  0.0|       0|1.1261E-4|        10|
|4C:91:7A:12:DF:95|  135108|  红鹰展翅|1960|   507701|       勇士| 2016|  7.4|       0|1.0293E-4|        11|
|4C:91:7A:12:DF:95|  135108|  红鹰展翅|1960|   192117|    黄克功案件| 2014|  8.3|       0| 1.024E-4|        12|
|4C:91:7A:12:DF:95|  135108|  红鹰展翅|1960|   109773|    金陵十三钗| 2011|  8.9|      55|1.0175E-4|        13|
|4C:91:7A:12:DF:95|  135239|战火中的青春|1959|   109285|      我11| 2012|  8.2|       0| 9.947E-5|        14|
|4C:91:7A:12:DF:95|  135108|  红鹰展翅|1960|  1131852|       大虎| 2015|  0.0|       0| 9.816E-5|        15|
|4C:91:7A:12:DF:95|  135108|  红鹰展翅|1960|   192157|    镇海保卫战| 2014|  7.4|       0| 8.953E-5|        16|
|4C:91:7A:12:DF:95|  135239|战火中的青春|1959|   967638|     京武春秋| 2009|  7.1|       0| 8.875E-5|        17|
|4C:91:7A:12:DF:95|  135108|  红鹰展翅|1960|  1117057|      南泥湾| 2013|  0.0|       0| 8.495E-5|        18|
|4C:91:7A:12:DF:95|  135108|  红鹰展翅|1960|   193356|    一号化妆间| 2012|  7.3|       0| 8.381E-5|        19|
|4C:91:7A:12:DF:95|  135108|  红鹰展翅|1960|   798152|      邰忠利| 2012|  7.1|       0| 7.642E-5|        20|
+-----------------+--------+------+----+---------+---------+-----+-----+--------+---------+----------+
only showing top 20 rows

==================================================update_user.user_similar_filter_same_recall_1969==================================================
4169817
+-----------------+--------+---------+----+-----+--------+--------------------+--------------------+
|          user_id|movie_id|    title|year|score|play_num|              weight|          base_movie|
+-----------------+--------+---------+----+-----+--------+--------------------+--------------------+
|00:22:33:44:55:66|  129967|     褓姆惊魂|1999|  7.1|       0|           1.4943E-4|[[1121176, 攀登者, 2...|
|00:22:33:44:55:66|  968449|      桃花劫|2018|  7.4|       0|             5.35E-6|[[1341341, 斗破乱世情,...|
|00:22:33:44:55:66| 1322357|     成人仪式|2003|  6.9|       0|             7.63E-6|[[1339141, 大饿, 20...|
|00:30:1B:BA:02:DB|    6954|       断箭|1996|  8.2|       0|            7.607E-5|[[965305, 极盗者, 20...|
|00:30:1B:BA:02:DB|  114773|     重振球风|2006|  6.8|       0|            6.469E-5|[[196680, 小黄人大眼萌,...|
|00:30:1B:BA:02:DB|  128747|     游园惊梦|1960|  9.0|       0|            2.056E-5|[[1121176, 攀登者, 2...|
|00:30:1B:BA:02:DB|  133740|     卒仔抽车|1982|  7.8|       0|1.579999999999999...|[[132723, 黑心鬼, 19...|
|00:30:1B:BA:02:DB|  156511|       鬼夫|2013|  8.2|       0|            7.405E-5|[[151623, 坏蛋必须死, ...|
|00:30:1B:BA:02:DB|  211034|     红门虎子|2011|  7.2|       0|           1.4888E-4|[[1269531, 王者天下, ...|
|00:30:1B:BA:02:DB|  397549|     分手达人|2014|  0.0|       0|           5.7346E-4|[[1121176, 攀登者, 2...|
|00:30:1B:BA:02:DB|  509382|     上学路上|2013|  8.6|       0|            2.195E-5|[[984161, 龙猫, 1988]]|
|00:30:1B:BA:02:DB|  762707|     六月新娘|1960|  7.2|       1|             3.61E-6|[[901794, 祖宗十九代, ...|
|00:30:1B:BA:02:DB|  901801|   哺乳期的女人|2015|  6.8|      12|             7.62E-6|[[129710, 菊次郎的夏天,...|
|00:30:1B:BA:02:DB|  902140|     美好人间|2015|  7.1|       0|           1.4804E-4|[[1339141, 大饿, 20...|
|00:30:1B:BA:02:DB|  927242|   女模特的风波|1989|  7.6|       1|           1.4554E-4|[[1338119, 无影之镜, ...|
|00:30:1B:BA:02:DB| 1074939|      英雄泪|2011|  7.5|       0|           3.3888E-4|[[778689, 红海行动, 2...|
|00:30:1B:BA:02:DB| 1080214|考死：血之期中考试|2008|  0.0|       0|            4.616E-5|[[151623, 坏蛋必须死, ...|
|00:30:1B:BA:02:DB| 1119983|    路客与刀客|1970|  7.5|       0|3.420000000000000...|[[993066, 黄飞鸿之狮魂觉...|
|00:30:1B:BA:02:DB| 1124350|  碟仙(毕业照)|2017|  4.0|       0|             6.89E-6|[[879132, 西游之决战大佛...|
|00:30:1B:BA:02:DB| 1337461|     找你算账|1989|  7.2|       0|              8.3E-6|[[1326854, 无双花木兰,...|
+-----------------+--------+---------+----+-----+--------+--------------------+--------------------+
only showing top 20 rows

==================================================update_user.user_similar_filter_history_recall_1969==================================================
4139726
+-----------------+--------+---------+----+-----+--------+---------+--------------------+--------+---------+
|          user_id|movie_id|    title|year|score|play_num|   weight|          base_movie|sort_num|timestamp|
+-----------------+--------+---------+----+-----+--------+---------+--------------------+--------+---------+
|4C:91:7A:12:DF:95| 1112139|     烈火英雄|2019|  8.7|      41|2.1777E-4|[[135239, 战火中的青春,...|       1| 20200531|
|4C:91:7A:12:DF:95|  929382|    我不是药神|2018|  9.1|      18|2.1269E-4|[[135239, 战火中的青春,...|       2| 20200531|
|4C:91:7A:12:DF:95| 1130996|     中国机长|2019|  8.8|      83|2.1233E-4|[[135239, 战火中的青春,...|       3| 20200531|
|4C:91:7A:12:DF:95|  846996|      空天猎|2017|  8.3|       9|  1.77E-4|[[135239, 战火中的青春,...|       4| 20200531|
|4C:91:7A:12:DF:95|  226812|     铁道飞虎|2016|  8.7|      11|1.7348E-4|[[135239, 战火中的青春,...|       5| 20200531|
|4C:91:7A:12:DF:95|  715777|  人再囧途之泰囧|2012|  9.1|       3|1.5238E-4|[[135239, 战火中的青春,...|       6| 20200531|
|4C:91:7A:12:DF:95|  968610|     刀背藏身|2017|  7.3|       0|1.2465E-4|[[135239, 战火中的青春,...|       7| 20200531|
|4C:91:7A:12:DF:95| 1062210|    唐山大地震|2010|  9.1|      19|1.2133E-4|[[135239, 战火中的青春,...|       8| 20200531|
|4C:91:7A:12:DF:95|  972249|痞子熊兵之杀狼祭天|2018|  0.0|       2|1.1859E-4|[[135108, 红鹰展翅, 1...|       9| 20200531|
|4C:91:7A:12:DF:95| 1106506|一八九四甲午大海战|2012|  0.0|       0|1.1261E-4|[[135239, 战火中的青春,...|      10| 20200531|
|4C:91:7A:12:DF:95|  507701|       勇士|2016|  7.4|       0|1.0293E-4|[[135108, 红鹰展翅, 1...|      11| 20200531|
|4C:91:7A:12:DF:95|  192117|    黄克功案件|2014|  8.3|       0| 1.024E-4|[[135108, 红鹰展翅, 1...|      12| 20200531|
|4C:91:7A:12:DF:95|  109773|    金陵十三钗|2011|  8.9|      55|1.0175E-4|[[135108, 红鹰展翅, 1...|      13| 20200531|
|4C:91:7A:12:DF:95|  109285|      我11|2012|  8.2|       0| 9.947E-5|[[135239, 战火中的青春,...|      14| 20200531|
|4C:91:7A:12:DF:95| 1131852|       大虎|2015|  0.0|       0| 9.816E-5|[[135108, 红鹰展翅, 1...|      15| 20200531|
|4C:91:7A:12:DF:95|  192157|    镇海保卫战|2014|  7.4|       0| 8.953E-5|[[135108, 红鹰展翅, 1...|      16| 20200531|
|4C:91:7A:12:DF:95|  967638|     京武春秋|2009|  7.1|       0| 8.875E-5|[[135239, 战火中的青春,...|      17| 20200531|
|4C:91:7A:12:DF:95| 1117057|      南泥湾|2013|  0.0|       0| 8.495E-5|[[135108, 红鹰展翅, 1...|      18| 20200531|
|4C:91:7A:12:DF:95|  193356|    一号化妆间|2012|  7.3|       0| 8.381E-5|[[135108, 红鹰展翅, 1...|      19| 20200531|
|4C:91:7A:12:DF:95|  798152|      邰忠利|2012|  7.1|       0| 7.642E-5|[[135108, 红鹰展翅, 1...|      20| 20200531|
+-----------------+--------+---------+----+-----+--------+---------+--------------------+--------+---------+
only showing top 20 rows

==================================================update_user.user_similar_recall_1969_100==================================================
1369302
+-----------------+--------+---------+----+-----+--------+---------+--------------------+--------+---------+
|          user_id|movie_id|    title|year|score|play_num|   weight|          base_movie|sort_num|timestamp|
+-----------------+--------+---------+----+-----+--------+---------+--------------------+--------+---------+
|4C:91:7A:12:DF:95| 1112139|     烈火英雄|2019|  8.7|      41|2.1777E-4|[[135239, 战火中的青春,...|       1| 20200531|
|4C:91:7A:12:DF:95|  929382|    我不是药神|2018|  9.1|      18|2.1269E-4|[[135239, 战火中的青春,...|       2| 20200531|
|4C:91:7A:12:DF:95| 1130996|     中国机长|2019|  8.8|      83|2.1233E-4|[[135239, 战火中的青春,...|       3| 20200531|
|4C:91:7A:12:DF:95|  846996|      空天猎|2017|  8.3|       9|  1.77E-4|[[135239, 战火中的青春,...|       4| 20200531|
|4C:91:7A:12:DF:95|  226812|     铁道飞虎|2016|  8.7|      11|1.7348E-4|[[135239, 战火中的青春,...|       5| 20200531|
|4C:91:7A:12:DF:95|  715777|  人再囧途之泰囧|2012|  9.1|       3|1.5238E-4|[[135239, 战火中的青春,...|       6| 20200531|
|4C:91:7A:12:DF:95|  968610|     刀背藏身|2017|  7.3|       0|1.2465E-4|[[135239, 战火中的青春,...|       7| 20200531|
|4C:91:7A:12:DF:95| 1062210|    唐山大地震|2010|  9.1|      19|1.2133E-4|[[135239, 战火中的青春,...|       8| 20200531|
|4C:91:7A:12:DF:95|  972249|痞子熊兵之杀狼祭天|2018|  0.0|       2|1.1859E-4|[[135108, 红鹰展翅, 1...|       9| 20200531|
|4C:91:7A:12:DF:95| 1106506|一八九四甲午大海战|2012|  0.0|       0|1.1261E-4|[[135239, 战火中的青春,...|      10| 20200531|
|4C:91:7A:12:DF:95|  507701|       勇士|2016|  7.4|       0|1.0293E-4|[[135108, 红鹰展翅, 1...|      11| 20200531|
|4C:91:7A:12:DF:95|  192117|    黄克功案件|2014|  8.3|       0| 1.024E-4|[[135108, 红鹰展翅, 1...|      12| 20200531|
|4C:91:7A:12:DF:95|  109773|    金陵十三钗|2011|  8.9|      55|1.0175E-4|[[135108, 红鹰展翅, 1...|      13| 20200531|
|4C:91:7A:12:DF:95|  109285|      我11|2012|  8.2|       0| 9.947E-5|[[135239, 战火中的青春,...|      14| 20200531|
|4C:91:7A:12:DF:95| 1131852|       大虎|2015|  0.0|       0| 9.816E-5|[[135108, 红鹰展翅, 1...|      15| 20200531|
|4C:91:7A:12:DF:95|  192157|    镇海保卫战|2014|  7.4|       0| 8.953E-5|[[135108, 红鹰展翅, 1...|      16| 20200531|
|4C:91:7A:12:DF:95|  967638|     京武春秋|2009|  7.1|       0| 8.875E-5|[[135239, 战火中的青春,...|      17| 20200531|
|4C:91:7A:12:DF:95| 1117057|      南泥湾|2013|  0.0|       0| 8.495E-5|[[135108, 红鹰展翅, 1...|      18| 20200531|
|4C:91:7A:12:DF:95|  193356|    一号化妆间|2012|  7.3|       0| 8.381E-5|[[135108, 红鹰展翅, 1...|      19| 20200531|
|4C:91:7A:12:DF:95|  798152|      邰忠利|2012|  7.1|       0| 7.642E-5|[[135108, 红鹰展翅, 1...|      20| 20200531|
+-----------------+--------+---------+----+-----+--------+---------+--------------------+--------+---------+
only showing top 20 rows

用户历史相似度召回更新完成
update_similar_recall 运行时长：9.596558606624603 分钟
2020-06-01 02:32:07 WARN  TaskSetManager:66 - Stage 296 contains a task of very large size (7436 KB). The maximum recommended task size is 100 KB.
2020-06-01 02:32:18 WARN  TaskSetManager:66 - Stage 301 contains a task of very large size (3116 KB). The maximum recommended task size is 100 KB.
2020-06-01 02:38:50 WARN  TaskSetManager:66 - Stage 350 contains a task of very large size (5778 KB). The maximum recommended task size is 100 KB.
2020-06-01 02:39:01 WARN  TaskSetManager:66 - Stage 355 contains a task of very large size (11184 KB). The maximum recommended task size is 100 KB.
2020-06-01 02:39:02 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:39:02 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:39:02 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:39:02 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:39:02 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:39:02 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:39:02 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 1641.6074799273106 msec.
==================================================movie_recall.movie_vector_1969==================================================
48475
+--------+-------+--------------------+
|movie_id|cate_id|         movieVector|
+--------+-------+--------------------+
|   95526|   1969|[-0.0336128043302...|
|   95841|   1969|[0.02462345301963...|
|   96165|   1969|[0.03642685649184...|
|   96204|   1969|[0.10642637237871...|
|   96359|   1969|[0.04910043873663...|
|  105657|   1969|[-0.0594352690858...|
|  105784|   1969|[-0.1290140888753...|
|  107108|   1969|[-0.4250065802154...|
|  109192|   1969|[0.04262914159737...|
|  110942|   1969|[-0.0772006379859...|
|  112603|   1969|[-0.4504811377898...|
|  117180|   1969|[0.17078241318604...|
|  117264|   1969|[0.01063971868210...|
|  128784|   1969|[-0.0201483037341...|
|  131797|   1969|[-0.2072349173681...|
|  133142|   1969|[0.08271304391315...|
|  133164|   1969|[0.08545529513503...|
|  133382|   1969|[0.04815989623419...|
|  133904|   1969|[-0.2635574400598...|
|  134532|   1969|[0.05096873270432...|
+--------+-------+--------------------+
only showing top 20 rows

2020-06-01 02:40:43 WARN  TaskSetManager:66 - Stage 370 contains a task of very large size (7397 KB). The maximum recommended task size is 100 KB.
2020-06-01 02:40:48 WARN  TaskSetManager:66 - Stage 371 contains a task of very large size (7397 KB). The maximum recommended task size is 100 KB.
==================================================factor.movie_hot_sort==================================================
341869
+-------+--------------------+---+------+--------+
|    aid|               title|cid|weight|sort_num|
+-------+--------------------+---+------+--------+
|1220433|蓝少ROBLIX：狼与猎人模拟器，...|  0|    98|       1|
|1162502|  乐乐偷喝可乐被妈妈打屁股宝宝巴士游戏|  0|    33|       2|
|1231801|入梦txf：《窗户》墙上画着一幅画...|  0|    25|       3|
|1161506|乐高无限02：我找到沙漠宝藏，里面...|  0|    15|       4|
|1218177|看见那些笔墨了吗？那是我在天上为你...|  0|    13|       5|
|1161617|《迷你特工队动画》塞米宝宝和弗特宝...|  0|    12|       6|
|1224441|和平精英：龙脊山有个“监狱地下室”...|  0|    11|       7|
|1164208|猫和老鼠：升级版莱特宁，捉鼠捉到飞...|  0|     7|       8|
|1156459|猫和老鼠：遇上一只佛系莱特宁，捉鼠...|  0|     7|       9|
|1305026|        海绵宝宝大电影：海绵出水|  0|     7|      10|
|1163896|人民的名义：得知是沙瑞金的电话，李...|  0|     6|      11|
|1149860|817小朋友色彩早教，机械手臂把白...|  0|     6|      12|
|1222562|西游记后传，悟空真实来历，悟空自己...|  0|     5|      13|
|1132343|王者别闹：诸葛亮设计，埋伏司马懿，...|  0|     5|      14|
|1232968|              会吃人的凉亭|  0|     5|      15|
|1151678|《迷你特工队动画》麦克斯抢到了许多...|  0|     5|      16|
|1303750|               超少年密码|  0|     4|      17|
|1150528|海绵宝宝趣味惊喜糖果盒子，派大星、...|  0|     4|      18|
|1161634|正阳门下：候大爷顿悟，感谢春明，并...|  0|     4|      19|
|1166197|传奇特工钱壮飞：不仅才华出众还曾挽...|  0|     4|      20|
+-------+--------------------+---+------+--------+
only showing top 20 rows

19026
2020-06-01 02:41:50 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:41:50 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:41:50 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:41:50 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:41:50 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:41:50 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:41:50 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 1039.1494398871398 msec.
==================================================factor.movie_hot_factor==================================================
341869
+-------+--------------------+---+--------+----------+--------+
|    aid|               title|cid|play_num|    factor|sort_num|
+-------+--------------------+---+--------+----------+--------+
|1220433|蓝少ROBLIX：狼与猎人模拟器，...|  0|      98|0.09217906|       1|
|1162502|  乐乐偷喝可乐被妈妈打屁股宝宝巴士游戏|  0|      33|0.09214995|       2|
|1231801|入梦txf：《窗户》墙上画着一幅画...|  0|      25|0.09214637|       3|
|1161506|乐高无限02：我找到沙漠宝藏，里面...|  0|      15| 0.0921419|       4|
|1218177|看见那些笔墨了吗？那是我在天上为你...|  0|      13|0.09214101|       5|
|1161617|《迷你特工队动画》塞米宝宝和弗特宝...|  0|      12|0.09214056|       6|
|1224441|和平精英：龙脊山有个“监狱地下室”...|  0|      11|0.09214012|       7|
|1164208|猫和老鼠：升级版莱特宁，捉鼠捉到飞...|  0|       7|0.09213833|       8|
|1156459|猫和老鼠：遇上一只佛系莱特宁，捉鼠...|  0|       7|0.09213833|       9|
|1305026|        海绵宝宝大电影：海绵出水|  0|       7|0.09213833|      10|
|1163896|人民的名义：得知是沙瑞金的电话，李...|  0|       6|0.09213788|      11|
|1149860|817小朋友色彩早教，机械手臂把白...|  0|       6|0.09213788|      12|
|1222562|西游记后传，悟空真实来历，悟空自己...|  0|       5|0.09213744|      13|
|1132343|王者别闹：诸葛亮设计，埋伏司马懿，...|  0|       5|0.09213744|      14|
|1232968|              会吃人的凉亭|  0|       5|0.09213744|      15|
|1151678|《迷你特工队动画》麦克斯抢到了许多...|  0|       5|0.09213744|      16|
|1303750|               超少年密码|  0|       4|0.09213699|      17|
|1150528|海绵宝宝趣味惊喜糖果盒子，派大星、...|  0|       4|0.09213699|      18|
|1161634|正阳门下：候大爷顿悟，感谢春明，并...|  0|       4|0.09213699|      19|
|1166197|传奇特工钱壮飞：不仅才华出众还曾挽...|  0|       4|0.09213699|      20|
+-------+--------------------+---+--------+----------+--------+
only showing top 20 rows

7.314743127404829
2020-06-01 02:42:19 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:19 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:19 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:19 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:19 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:19 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:19 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 1222.4640402392436 msec.
==================================================factor.movie_score_factor==================================================
341869
+-------+--------+---+-----+------+--------+
|    aid|   title|cid|score|factor|sort_num|
+-------+--------+---+-----+------+--------+
|1271614|安徽卫视春节联欢|  0| 10.0|   1.0|       1|
|1271615|天津卫视相声晚会|  0| 10.0|   1.0|       2|
|1271616|央视春节联欢晚会|  0| 10.0|   1.0|       3|
|1271617|湖南卫视春节联欢|  0| 10.0|   1.0|       4|
|1271618|吉林卫视春节联欢|  0| 10.0|   1.0|       5|
|1271619|山东卫视春节联欢|  0| 10.0|   1.0|       6|
|1271620|深圳卫视知识春晚|  0| 10.0|   1.0|       7|
|1271621| 北京台春节联欢|  0| 10.0|   1.0|       8|
|1271622|东方卫视春节联欢|  0| 10.0|   1.0|       9|
|1305067|  当幸福来敲门|  0|  9.5|0.7115|      10|
|1302743|    机器管家|  0|  9.3|0.6533|      11|
|1305098|   幸福终点站|  0|  9.2|0.6298|      12|
|1305107|     范海辛|  0|  9.2|0.6298|      13|
|1331679|  辛德勒的名单|  0|  9.2|0.6298|      14|
|1302865|    精灵旅社|  0|  9.1|0.6091|      15|
|1302810|     黑磨坊|  0|  9.0|0.5906|      16|
|1302888|    战地情人|  0|  9.0|0.5906|      17|
|1303629|  ALLOUT|  0|  9.0|0.5906|      18|
|1304987|   傲慢与偏见|  0|  9.0|0.5906|      19|
|1304988|    冥界警局|  0|  9.0|0.5906|      20|
+-------+--------+---+-----+------+--------+
only showing top 20 rows

2020-06-01 02:42:47 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:47 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:47 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:47 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:47 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:47 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:42:47 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 1259.354707703418 msec.
==================================================factor.movie_year_factor==================================================
341869
+--------+----+------+
|movie_id|year|factor|
+--------+----+------+
|       8|2020|   1.0|
|      12|2015|0.7071|
|      57|2014|0.6598|
|     105|2016|0.7579|
|     157|2014|0.6598|
|     228|2014|0.6598|
|     517|2012|0.5743|
|     548|2009|0.4665|
|     571|2012|0.5743|
|     790|1997|0.2031|
|     818|2012|0.5743|
|     940|2015|0.7071|
|    1221|2015|0.7071|
|    1244|2014|0.6598|
|    1257|2014|0.6598|
|    1297|2017|0.8123|
|    1361|2015|0.7071|
|    1377|2017|0.8123|
|    1379|2016|0.7579|
|    1391|1999|0.2333|
+--------+----+------+
only showing top 20 rows

2020-06-01 02:43:12 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:43:12 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:43:12 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:43:12 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:43:12 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:43:12 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:43:12 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 2120.9840600580037 msec.
==================================================factor.movie_time==================================================
266701
+--------+---------+
|movie_id|movie_len|
+--------+---------+
|    2214|        0|
|    8484|       68|
|   95841|       90|
|   96165|       87|
|   96204|        0|
|   96359|      125|
|   98089|       45|
|   98426|        0|
|  101738|        1|
|  103809|       83|
|  103854|       19|
|  104529|       86|
|  104919|        0|
|  105657|        0|
|  105784|      153|
|  109192|        0|
|  112603|       86|
|  117180|      134|
|  117264|        0|
|  119319|       54|
+--------+---------+
only showing top 20 rows

电影召回更新完成
update_movie_recall 运行时长：27.781594383716584 分钟
2020-06-01 02:57:47 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:57:47 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:57:47 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:57:47 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:57:47 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:57:47 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 02:57:47 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 1514.0510622786119 msec.
==================================================pre_user.user_click_35==================================================
750264
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|  datetime|
+-----------------+--------+-------+----------+
|4C:91:7A:C3:7D:26|  371302|   1970|1589990400|
|4C:91:7A:39:98:51|  938160|   1970|1589990400|
|4C:91:7A:71:5B:66| 1298428|   1970|1589990400|
|4C:91:7A:6E:AB:07| 1325674|   1969|1589990400|
|4C:91:7A:8E:B1:61|  932677|   1969|1589990400|
|4C:91:7A:C9:39:9A|  761429|   1969|1589990400|
|4C:91:7A:4E:27:9C| 1324430|   1970|1589990400|
|4C:91:7A:AA:6F:7A| 1139145|   1970|1589990400|
|4C:91:7A:25:9F:53| 1324818|   1971|1589990400|
|4C:91:7A:53:09:D2| 1267581|   1971|1589990400|
|4C:91:7A:53:09:D2| 1289934|   1971|1589990400|
|4C:91:7A:53:09:D2| 1332657|   1971|1589990400|
|4C:91:7A:73:90:F9|  105358|   1972|1589990400|
|4C:91:7A:A8:66:3C|  939589|   1969|1589990400|
|4C:91:7A:7D:38:9B| 1331284|   1973|1589990400|
|4C:91:7A:7D:38:9B| 1331333|   1973|1589990400|
|4C:91:7A:7D:38:9B|  884662|   1972|1589990400|
|4C:91:7A:37:89:4B|  172145|   1969|1589990400|
|4C:91:7A:37:89:4B|  590212|   1969|1589990400|
|4C:91:7A:1D:4B:E8| 1260511|   1972|1589990400|
+-----------------+--------+-------+----------+
only showing top 20 rows

==================================================pre_user.merge_click==================================================
15798777
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|  datetime|
+-----------------+--------+-------+----------+
|4C:91:7A:BB:C5:6E|  511000|      0|1559318400|
|4C:91:7A:BB:C5:6E| 1077842|      0|1559318400|
|4C:91:7A:77:6D:3B|  736033|   1969|1559318400|
|4C:91:7A:77:6D:3B|  754332|   1973|1559318400|
|4C:91:7A:77:6D:3B| 1077842|   1970|1559318400|
|80:0B:52:06:1F:95|  319578|   1969|1559318400|
|80:0B:52:10:AE:DF|  765403|   1969|1559318400|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400|
|80:0B:52:10:AE:DF|  997259|   1969|1559318400|
|80:0B:52:10:AE:DF| 1084636|   1969|1559318400|
|80:0B:52:10:AE:DF|  973986|   1969|1559318400|
|80:0B:52:10:AE:DF| 1088005|   1970|1559318400|
|80:0B:52:10:AE:DF| 1088005|   1970|1559318400|
|4C:91:7A:2A:61:0C|  997259|   1969|1559318400|
|4C:91:7A:1F:6C:D2|  997259|   1969|1559318400|
|80:0B:52:10:AE:4D| 1100215|   1970|1559318400|
|4C:91:7A:24:B0:27|      49|   2271|1559318400|
|4C:91:7A:24:B0:27|      26|   2271|1559318400|
+-----------------+--------+-------+----------+
only showing top 20 rows

2020-06-01 03:00:36 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:00:36 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:00:36 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:00:36 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:00:36 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:00:36 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:00:36 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 859.5405032310587 msec.
==================================================pre_user.user_top_35==================================================
9191
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|  datetime|
+-----------------+--------+-------+----------+
|4C:91:7A:FC:E5:0F|  981351|   1973|1589990400|
|4C:91:7A:C3:7B:E2| 1081950|   1970|1589990400|
|4C:91:7A:76:A5:B3| 1329495|   1970|1589990400|
|4C:91:7A:76:A5:B3| 1286974|   1970|1589990400|
|4C:91:7A:E9:6F:D3|  970100|   1969|1589990400|
|4C:91:7A:FC:E5:0F|  938012|   1973|1589990400|
|4C:91:7A:C3:7B:E2| 1284424|   1970|1589990400|
|4C:91:7A:C3:7B:E2|   95646|   1969|1589990400|
|4C:91:7A:C3:7B:E2| 1241542|   1969|1589990400|
|4C:91:7A:C3:7B:E2| 1306481|   1970|1589990400|
|4C:91:7A:C3:7B:E2| 1186911|   1970|1589990400|
|4C:91:7A:C3:7B:E2| 1081928|   1970|1589990400|
|4C:91:7A:C3:7B:E2| 1323403|   1970|1589990400|
|4C:91:7A:7A:2D:C6|  245532|   1973|1589990400|
|4C:91:7A:2C:09:D1| 1324818|   1971|1589990400|
|4C:91:7A:C3:7B:E2| 1189693|   1969|1589990400|
|4C:91:7A:3F:18:E0| 1221984|   1969|1589990400|
|4C:91:7A:D1:C5:7B|  320789|   1969|1589990400|
|4C:91:7A:69:C0:F5| 1267454|   1971|1589990400|
|4C:91:7A:9F:16:BC|  928968|   1970|1589990400|
+-----------------+--------+-------+----------+
only showing top 20 rows

==================================================pre_user.merge_top==================================================
206636
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|  datetime|
+-----------------+--------+-------+----------+
|4C:91:7A:0B:27:D9|  997259|   1969|1559318400|
|4C:91:7A:36:9C:23|      19|   2271|1559318400|
|4C:91:7A:36:9C:23| 1089783|      0|1559318400|
|4C:91:7A:24:B0:27| 1103390|      0|1559318400|
|4C:91:7A:24:B0:27| 1103390|      0|1559318400|
|4C:91:7A:24:B0:27| 1103390|      0|1559318400|
|4C:91:7A:24:B0:27| 1103390|      0|1559318400|
|4C:91:7A:56:63:55| 1082910|   1973|1559404800|
|4C:91:7A:56:63:55|  574779|   1973|1559404800|
|4C:91:7A:F2:44:34|  997259|   1969|1559491200|
|4C:91:7A:F2:44:34|  509420|   1969|1559491200|
|4C:91:7A:F2:44:34|  188413|   1969|1559491200|
|4C:91:7A:36:9C:23| 1091820|      0|1559491200|
|4C:91:7A:51:6E:D9|  987081|   1969|1559491200|
|4C:91:7A:51:6E:D9|  987081|   1969|1559491200|
|4C:91:7A:F2:44:34|  983559|   1969|1559491200|
|4C:91:7A:29:7D:88| 1085389|      0|1559577600|
|4C:91:7A:7E:4A:ED| 1104223|   1970|1559664000|
|4C:91:7A:7E:4A:ED|  966570|   1970|1559664000|
|4C:91:7A:7E:4A:ED| 1088358|   1969|1559664000|
+-----------------+--------+-------+----------+
only showing top 20 rows

2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.client.capability.check does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.false.positive.probability does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.broker.address.default does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.orc.time.counters does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve-fraction.min does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.ms.footer.cache.ppd.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.event.message.factory does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.metrics.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.hs2.user.access does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.storage.storageDirectory does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.liveness.connection.timeout.ms does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.dynamic.semijoin.reduction.threshold does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.connect.retry.limit does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.xmx.headroom does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.dynamic.semijoin.reduction does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.direct does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.stats does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.client.consistent.splits does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.session.lifetime does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.timedout.txn.reaper.start does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.ttl does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.management.acl does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.delegation.token.lifetime does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.guidKey does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.ats.hook.queue.capacity does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.large.query does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.bigtable.minsize.semijoin.reduction does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.alloc.min does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.user does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.alloc.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.wait.queue.comparator.class.name does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.port does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.orc.cache.use.soft.references does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve.fraction.max does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.listener.thread-count does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.container.max.java.heap.fraction does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.stats.column.autogather does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.am.liveness.heartbeat.interval.ms does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.decoding.metrics.percentiles.intervals does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.groupby.position.alias does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.txn.store.impl does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.groupby.shuffle does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.object.cache.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.parallel.ops.in.session does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.groupby.limit.extrastep does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.use.ssl does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.local does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.file.location does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.retry.delay.seconds does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.fileformat does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.num.file.cleaner.threads does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.test.fail.compaction does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.use.blobstore.as.scratchdir does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.class does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.mmap.path does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.download.permanent.fns does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.max.historic.queries does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.execution.reducesink.new.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.compactor.max.num.delta does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.attempted does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.port does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.compactor.initiator.failed.compacts.threshold does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.reporter does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.max.pending.writes does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.execution.mode does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.enable.grace.join.in.llap does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.memory.mode does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.threadpool.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.select.threshold does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.scratchdir.lock does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.use.spnego does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.file.frequency does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.hs2.coordinator.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.timeout.seconds does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.filter.stats.reduction does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.exec.orc.base.delta.ratio does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.fastpath does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.clear.dangling.scratchdir does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.test.fail.heartbeater does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.file.cleanup.delay.seconds does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.management.rpc.port does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.mapjoin.hybridgrace.bloomfilter does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.tree does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.stats.ndv.tuner does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.query.length does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.failed does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.close.session.on.disconnect does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.ppd.windowing does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.initial.metadata.count.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.host does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.ms.footer.cache.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.point.lookup.min does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.file.metadata.threads does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.service.refresh.interval.sec does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.max.output.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.driver.parallel.compilation does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.remote.token.requires.signing does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.bucket.pruning does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.cache.allow.synthetic.fileid does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.hash.table.inflation.factor does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.hbase.ttl does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.enforce.vectorized does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.writeset.reaper.interval does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.vector.serde.deserialize does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.order.columnalignment does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.service.send.buffer.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.exec.schema.evolution does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.elements.values.clause does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.llap.concurrent.queries does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.allow.uber does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.partition.size.max does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.auth does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.include.fileid does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.communicator.num.threads does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.orderby.position.alias does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.connection.sleep.between.retries.ms does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.max.partitions does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.hadoop2.component does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.yarn.shuffle.port does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.direct.sql.max.elements.in.clause does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.passiveWaitTimeMs does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.load.dynamic.partitions.thread does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.segments.granularity does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.http.response.header.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.conf.internal.variable.list does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose.reductionpercentage does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.retry.limit does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.serialize.in.tasks does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.query.timeout.seconds does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.service.metrics.hadoop2.frequency does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.directory.batch.ms does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.reader.wait does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.reenable.max.timeout.ms does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.max.open.txns does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.auto.convert.sortmerge.join.reduce.side does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.zookeeper.publish.configs does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.auto.convert.join.hashtable.max.entries does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.sessions.init.threads does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.authorization.storage.check.externaltable.drop does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.execution.mode does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.cbo.cnf.maxnodes does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.adaptor.usage.mode does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.rewriting does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.groupMembershipKey does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.catalog.cache.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.cbo.show.warnings does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.fshandler.threads does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.max.bloom.filter.entries does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.metadata.fraction does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.materializedview.serde does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.scheduler.wait.queue.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.cache.entries does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.txn.operational.properties does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.memory.ttl does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.rpc.port does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.nonvector.wrapper.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.cache.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.vectorized.input.format does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.cte.materialize.threshold does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.clean.until does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.semijoin.conversion does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.port does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.spark.dynamic.partition.pruning does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.metrics.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.repl.rootdir does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.limit.partition.request does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.async.log.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.logger does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.allow.udf.load.on.demand does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.cli.tez.session.async does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.bloom.filter.factor does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.am-reporter.max.threads does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.file.size.for.mapjoin does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.bucketing does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.bucket.pruning.compat does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.spnego.principal does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.preemption.metrics.intervals does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.shuffle.dir.watcher.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.arena.count does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.use.SSL does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.communicator.connection.timeout.ms does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.transpose.aggr.join does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.maxTries does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.spark.dynamic.partition.pruning.max.data.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.metadata.base does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggr.stats.invalidator.frequency does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.use.lrfu does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.mmap does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.coordinator.address.default does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.max.fetch.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.conf.hidden.list does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.io.sarg.cache.max.weight.mb does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.clear.dangling.scratchdir.interval does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.sleep.time does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.row.serde.deserialize does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.compile.lock.timeout does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.timedout.txn.reaper.interval does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.aggregate.stats.max.variance does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.lrfu.lambda does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.metadata.db.type does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.output.stream.timeout does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.transactional.events.mem does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.resultset.default.fetch.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.retain does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.merge.cardinality.check does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.authentication.ldap.groupClassKey does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.point.lookup does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.allow.permanent.fns does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.web.ssl does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.txn.manager.dump.lock.state.on.acquire.timeout does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.retention.succeeded does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.use.fileid.path does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.slice.row.count does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.mapjoin.optimized.hashtable.probe.percent does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.select.distribute does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.use.fqdn does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.reenable.min.timeout.ms does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.validate.acls does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.support.special.characters.tablename does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.mv.files.thread does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.skip.compile.udf.check does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.vector.serde.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.repl.cm.interval does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.sleep.interval.between.start.attempts does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.yarn.container.mb does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.http.read.timeout does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.optimizations.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.orc.gap.cache does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.dynamic.partition.hashjoin does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.exec.copyfile.maxnumfiles does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.formats does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.http.numConnection does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.task.scheduler.enable.preemption does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.num.executors does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.full does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.connection.class does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.sessions.custom.queue.allowed does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.slice.lrr does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.client.password does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.metastore.hbase.cache.max.writer.wait does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.thrift.http.request.header.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.webui.max.threads does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.limittranspose.reductiontuples does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.test.rollbacktxn does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.num.schedulable.tasks.per.node does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.acl does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.memory.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.type.safety does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.async.exec.async.compile does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.auto.max.input.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.enable.memory.manager does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.msck.repair.batch.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.blobstore.supported.schemes does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.orc.splits.allow.synthetic.fileid does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.stats.filter.in.factor does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.spark.use.op.stats does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.exec.input.listing.max.threads does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.tez.session.lifetime.jitter does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.web.port does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.cartesian.product does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.rpc.num.handlers does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.vcpus.per.instance does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.count.open.txns.interval does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.min.bloom.filter.entries does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.optimize.partition.columns.separate does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.orc.cache.stripe.details.mem.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.txn.heartbeat.threadpool.size does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.locality.delay does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.repl.cmrootdir does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.task.scheduler.node.disable.backoff.factor does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.am.liveness.connection.sleep.between.retries.ms does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.spark.exec.inplace.progress does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.working.directory does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.daemon.memory.per.instance.mb does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.msck.path.validation does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.task.scale.memory.reserve.fraction does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.merge.nway.joins does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.compactor.history.reaper.interval does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.txn.strict.locking.mode does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.encode.vector.serde.async.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.tez.input.generate.consistent.splits does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.in.place.progress does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.druid.indexer.memory.rownum.max does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.server2.xsrf.filter.enabled does not exist
2020-06-01 03:01:13 WARN  HiveConf:2753 - HiveConf of name hive.llap.io.allocator.alloc.max does not exist
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.3.4
      /_/

Using Python version 3.7.7 (default, Mar 26 2020 15:48:22)
SparkSession available as 'spark'.
2020-06-01 03:01:13 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:01:13 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:01:13 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:01:13 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:01:13 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:01:13 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:01:13 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 1898.520029949901 msec.
==================================================pre_user.user_play_35==================================================
3119264
+-----------------+--------+-------+---------+---------+----------+
|          user_id|movie_id|cate_id|play_time|movie_num|  datetime|
+-----------------+--------+-------+---------+---------+----------+
|4C:91:7A:C3:7D:26|  371302|   1970|  2305386|       35|1589990400|
|4C:91:7A:C3:7D:26|  371302|   1970|  4773890|       35|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  4338638|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  5068065|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  5797567|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  6526846|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  7256091|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  7985334|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  8714614|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973|  9443856|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973| 10173401|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973| 10902644|        1|1589990400|
|4C:91:7A:3F:91:51|  161578|   1973| 11631973|        1|1589990400|
|4C:91:7A:39:98:51| 1233848|   1972|  1216727|      720|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973| 14364906|       17|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973|   344060|       18|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973|   473599|       18|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973|  1063331|       18|1589990400|
|4C:91:7A:E7:CB:A8|  844797|   1973|  1713674|       18|1589990400|
|4C:91:7A:39:98:51|  938160|   1970|     7883|        1|1589990400|
+-----------------+--------+-------+---------+---------+----------+
only showing top 20 rows

==================================================pre_user.merge_play==================================================
36644989
+-----------------+--------+-------+---------+---------+----------+
|          user_id|movie_id|cate_id|play_time|movie_num|  datetime|
+-----------------+--------+-------+---------+---------+----------+
|00:0C:18:EF:EE:E7|  226509|   1969|        2|      -10|1565712000|
|00:0C:18:EF:EE:E7|  226509|   1969|        2|      -10|1565712000|
|00:0C:18:EF:EE:E7|  143088|   1969|        6|      -10|1565712000|
|00:0C:18:EF:EE:E7|  992955|   1969|        6|      -10|1565712000|
|00:0C:18:EF:EE:E7| 1132226|   1970|       16|       13|1565712000|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       40|       31|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       11|       31|1565798400|
|4C:91:7A:34:CA:3A|  987187|   1969|       53|      -10|1565798400|
|4C:91:7A:DD:46:93|  142654|   1970|       28|       36|1565798400|
|4C:91:7A:29:7D:88|  927671|   1969|        6|      -10|1565884800|
|4C:91:7A:29:7D:88|  984018|   1969|        3|      -10|1565884800|
|4C:91:7A:29:7D:88|  984018|   1969|        3|      -10|1565884800|
|4C:91:7A:E3:F0:B6| 1156978|   1969|        2|      -10|1565884800|
|4C:91:7A:29:7D:88| 1138814|   1970|       38|        1|1565884800|
|4C:91:7A:35:60:C1|  978191|   1970|       20|        1|1565884800|
|4C:91:7A:35:60:C1|  978191|   1970|       20|        1|1565884800|
+-----------------+--------+-------+---------+---------+----------+
only showing top 20 rows

==================================================pre_user.merge_action==================================================
52650402
+-----------------+--------+-------+----------+-----+-----+---------+---------+
|          user_id|movie_id|cate_id|  datetime|click|  top|play_time|movie_num|
+-----------------+--------+-------+----------+-----+-----+---------+---------+
|4C:91:7A:BB:C5:6E|  511000|      0|1559318400| true|false|      -10|      -10|
|4C:91:7A:BB:C5:6E| 1077842|      0|1559318400| true|false|      -10|      -10|
|4C:91:7A:77:6D:3B|  736033|   1969|1559318400| true|false|      -10|      -10|
|4C:91:7A:77:6D:3B|  754332|   1973|1559318400| true|false|      -10|      -10|
|4C:91:7A:77:6D:3B| 1077842|   1970|1559318400| true|false|      -10|      -10|
|80:0B:52:06:1F:95|  319578|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF|  765403|   1969|1559318400| true|false|      -10|      -10|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400| true|false|      -10|      -10|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400| true|false|      -10|      -10|
|4C:91:7A:D7:F4:B8| 1088005|   1970|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF|  997259|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF| 1084636|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF|  973986|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF| 1088005|   1970|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:DF| 1088005|   1970|1559318400| true|false|      -10|      -10|
|4C:91:7A:2A:61:0C|  997259|   1969|1559318400| true|false|      -10|      -10|
|4C:91:7A:1F:6C:D2|  997259|   1969|1559318400| true|false|      -10|      -10|
|80:0B:52:10:AE:4D| 1100215|   1970|1559318400| true|false|      -10|      -10|
|4C:91:7A:24:B0:27|      49|   2271|1559318400| true|false|      -10|      -10|
|4C:91:7A:24:B0:27|      26|   2271|1559318400| true|false|      -10|      -10|
+-----------------+--------+-------+----------+-----+-----+---------+---------+
only showing top 20 rows

==================================================factor.user_history_click==================================================
4766243
+--------+-----------------+-------+----------+--------------------+----+---------+
|movie_id|          user_id|cate_id|  datetime|               title|year|sort_time|
+--------+-----------------+-------+----------+--------------------+----+---------+
| 1286974|00:11:22:33:44:55|   1970|1588867200|                天醒之路|2020|        1|
| 1311945|00:11:22:33:44:55|   1970|1588694400|                  猎狐|2020|        2|
|   98176|00:11:22:33:44:55|   1970|1587916800|          我是特种兵之利刃出鞘|2012|        3|
| 1286089|00:11:22:33:44:55|   1970|1585670400|             如果岁月可回头|2020|        4|
|  994596|00:24:68:D5:A7:72|   1971|1561737600|            向往的生活第3季|2019|        1|
| 1228118|00:66:CE:0C:31:37|      0|1579104000|1-1.Dubbo+zookeee...|2019|        1|
| 1266797|00:66:CE:0C:31:37|      0|1579017600|               爱情公寓5|2020|        2|
|  105540|04:95:73:35:B5:7D|      0|1579104000|          倚天屠龙记之魔教教主|1993|        1|
|  142595|04:95:73:35:B5:7D|      0|1579104000|                百团大战|2015|        2|
| 1266684|04:95:73:35:B5:7D|      0|1579104000|               狄仁杰探案|2020|        3|
|  972618|04:95:73:35:B5:7D|      0|1579104000|            狄仁杰之蚩尤血藤|2018|        4|
| 1109895|04:95:73:F8:D5:2E|   1970|1561651200|               动物管理局|2019|        1|
|  997259|08:A5:C8:59:E2:C2|   1969|1561651200|               反贪风暴4|2019|        1|
| 1085389|0C:C6:55:AD:04:74|   1971|1561651200|             极限挑战第5季|2019|        1|
|  971906|0C:C6:55:BF:BB:45|   1973|1561737600|              迷你特工队X|2018|        1|
| 1085389|0E:18:E2:37:97:82|   1971|1561564800|             极限挑战第5季|2019|        1|
| 1089711|18:82:19:19:17:39|   2272|1561564800|      张雪峰手把手教你填报高考志愿|2019|        1|
|  897593|18:82:19:1B:05:39|   1973|1561737600|                萌鸡小队|2017|        1|
|  327574|18:82:19:1B:05:39|   1973|1561737600|          新大头儿子和小头爸爸|2013|        2|
|  927941|18:82:19:1B:05:39|   1973|1561651200|               睡衣小英雄|2018|        3|
+--------+-----------------+-------+----------+--------------------+----+---------+
only showing top 20 rows

==================================================factor.user_history_top==================================================
126124
+--------+-----------------+-------+----------+-----------------+----+---------+
|movie_id|          user_id|cate_id|  datetime|            title|year|sort_time|
+--------+-----------------+-------+----------+-----------------+----+---------+
| 1087476|4C:91:7A:00:3B:82|      0|1563465600|             筑梦情缘|2019|        1|
| 1112139|4C:91:7A:00:C7:D6|   1969|1572278400|             烈火英雄|2019|        1|
|  988774|4C:91:7A:01:1C:8D|   1969|1590768000|         我想吃掉你的胰脏|2018|        1|
|  804690|4C:91:7A:01:82:68|   1972|1590508800|    舞法天女朵法拉第1季（上）|2016|        1|
|  987081|4C:91:7A:02:31:C7|   1969|1577548800|          波西米亚狂想曲|2018|        1|
| 1102963|4C:91:7A:02:31:C7|   1969|1575734400|             雪人奇缘|2019|        2|
|  938839|4C:91:7A:02:31:C7|   1969|1575734400|           乐高大电影2|2019|        3|
| 1033987|4C:91:7A:03:4B:10|   1971|1583942400|           奔跑吧第3季|2019|        1|
| 1200865|4C:91:7A:03:4B:10|   1971|1583683200|      《奔跑吧兄弟》精彩合集|2020|        2|
| 1129956|4C:91:7A:03:4B:10|   1971|1582819200|     奔跑吧第3季(爆笑时刻)|   0|        3|
| 1144115|4C:91:7A:03:4B:10|   1971|1582819200|        奔跑吧兄弟第4季度|2016|        4|
| 1221969|4C:91:7A:03:4B:10|   1971|1582819200|              追我吧|2019|        5|
|  108278|4C:91:7A:03:4B:10|   1971|1580486400|         奔跑吧兄弟第2季|2015|        6|
| 1192084|4C:91:7A:04:31:F2|   1970|1575216000|           初恋那件小事|2019|        1|
|  188773|4C:91:7A:04:DC:87|   1970|1582819200|            重案六组3|2009|        1|
|  917354|4C:91:7A:05:E3:2A|   1972|1590768000|       星学院3之潘朵拉秘境|2017|        1|
|  991179|4C:91:7A:05:E3:2A|   1972|1587139200|Star☆Twinkle光之美少女|2019|        2|
|  492089|4C:91:7A:05:E3:2A|   1972|1585238400|      Suite光之美少女♪|2011|        3|
|  270722|4C:91:7A:05:E3:2A|   1972|1585238400|  美少女战士Crystal第3季|2016|        4|
|  153105|4C:91:7A:05:E3:2A|   1972|1585238400|     美少女战士Crystal|2014|        5|
+--------+-----------------+-------+----------+-----------------+----+---------+
only showing top 20 rows

==================================================factor.user_history_play==================================================
1721651
+--------+-----------------+-------+----------+---------------+----+---------+
|movie_id|          user_id|cate_id|  datetime|          title|year|sort_time|
+--------+-----------------+-------+----------+---------------+----+---------+
| 1286974|00:11:22:33:44:55|   1970|1588694400|           天醒之路|2020|        1|
| 1311945|00:11:22:33:44:55|   1970|1588694400|             猎狐|2020|        2|
|   98176|00:11:22:33:44:55|   1970|1587830400|     我是特种兵之利刃出鞘|2012|        3|
|  113798|4C:91:7A:00:74:25|   1969|1581350400|           我是传奇|2007|        1|
| 1122414|4C:91:7A:00:74:25|   1969|1581350400|            冰峰暴|2019|        2|
| 1235405|4C:91:7A:00:74:25|   1969|1581264000|         亲爱的新年好|2019|        3|
| 1192086|4C:91:7A:00:74:25|   1969|1580659200|           犯罪现场|2019|        4|
|  717396|4C:91:7A:00:74:25|   1969|1580659200|           星际穿越|2014|        5|
|   95844|4C:91:7A:00:74:25|   1969|1580659200|           一号目标|2014|        6|
| 1081744|4C:91:7A:00:74:25|   1969|1580659200|           孤胆特工|2010|        7|
| 1236354|4C:91:7A:00:74:25|   1969|1580400000|           半个喜剧|2019|        8|
|  971886|4C:91:7A:00:74:25|   1969|1580054400|      指环王1：魔戒再现|2001|        9|
| 1130996|4C:91:7A:00:74:25|   1969|1580054400|           中国机长|2019|       10|
|  103287|4C:91:7A:00:AF:28|   1972|1588089600|           灌篮高手|1993|        1|
|  990193|4C:91:7A:00:D8:3F|   1970|1580313600|             招摇|2019|        1|
| 1214970|4C:91:7A:01:51:D2|   1973|1589644800|        超级飞侠第7季|2019|        1|
| 1082077|4C:91:7A:01:51:D2|   1973|1589558400|伊恩庄园启蒙英文儿歌精选第2季|2019|        2|
| 1300676|4C:91:7A:01:51:D2|   1973|1588435200|     瑞奇宝宝第3季中文版|2020|        3|
|  244404|4C:91:7A:01:51:D2|   1973|1587657600|         宝宝巴士儿歌|2015|        4|
|  216817|4C:91:7A:01:51:D2|   1973|1586448000|      芭比之梦想豪宅全集|2016|        5|
+--------+-----------------+-------+----------+---------------+----+---------+
only showing top 20 rows

用户历史更新完成
update_user_history 运行时长：44.22408875226974 分钟
==================================================update_user.merge_action==================================================
5839264
+-----------------+--------+-------+----------+-----+-----+---------+---------+
|          user_id|movie_id|cate_id|  datetime|click|  top|play_time|movie_num|
+-----------------+--------+-------+----------+-----+-----+---------+---------+
|4C:91:7A:00:55:6E|  254913|   1973|1590422400| true|false|      -10|      -10|
|4C:91:7A:51:7A:4E|  789289|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:33:CF:3B|  190723|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:2F:EF:10|  244404|   1973|1590422400| true|false|      -10|      -10|
|4C:91:7A:0D:7A:E7|  142697|   1969|1590422400| true|false|      -10|      -10|
|4C:91:7A:83:D1:20| 1311945|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:73:94:23| 1337106|   1969|1590422400| true|false|      -10|      -10|
|4C:91:7A:52:10:0A|  149716|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:D5:7E:B8|  766804|   1969|1590422400| true|false|      -10|      -10|
|4C:91:7A:24:F4:4D|  789289|      0|1590422400| true|false|      -10|      -10|
|4C:91:7A:25:BA:5A|   96232|   1969|1590422400| true|false|      -10|      -10|
|4C:91:7A:8C:72:08| 1307048|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:8C:72:08| 1121378|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:8C:72:08|  784541|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:BC:DE:DB| 1329495|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:8C:72:08| 1307048|   1970|1590422400| true|false|      -10|      -10|
|4C:91:7A:8C:72:08| 1323939|   1971|1590422400| true|false|      -10|      -10|
|4C:91:7A:8C:72:08| 1307164|   1971|1590422400| true|false|      -10|      -10|
|4C:91:7A:2A:B5:77| 1286977|   1971|1590422400| true|false|      -10|      -10|
|4C:91:7A:2A:B5:77| 1303173|   1973|1590422400| true|false|      -10|      -10|
+-----------------+--------+-------+----------+-----+-----+---------+---------+
only showing top 20 rows

2020-06-01 03:47:11 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:47:11 WARN  DFSClient:692 - Failed to connect to /192.168.18.147:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:47:11 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:47:11 WARN  DFSClient:692 - Failed to connect to /192.168.18.162:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:47:11 WARN  BlockReaderFactory:716 - I/O error constructing remote block reader.
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:47:11 WARN  DFSClient:692 - Failed to connect to /192.168.18.131:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:673)
	at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1575)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:855)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:891)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:631)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:444)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:250)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:173)
2020-06-01 03:47:11 WARN  DFSClient:1007 - DFS chooseDataNode: got # 1 IOException, will wait for 2166.558847762075 msec.
==================================================update_user.action_weight==================================================
222350
+-----------------+--------+------------------+-------+
|          user_id|movie_id|            weight|cate_id|
+-----------------+--------+------------------+-------+
|4C:91:7A:BB:DE:6E|  169663|33.952299999999994|   1970|
|4C:91:7A:74:1A:CE|  148993| 37.55709999999998|   1970|
|4C:91:7A:71:D3:B0|  917528|           26.6615|   1973|
|4C:91:7A:D8:F1:3E|      44| 4.752299999999999|   2271|
|4C:91:7A:38:8F:19| 1289125| 9.504599999999998|   1969|
|4C:91:7A:97:5B:D7|  115668|15.878799999999998|   1970|
|4C:91:7A:DA:17:8B| 1334823|           46.6042|   1970|
|4C:91:7A:D1:DB:8E|  990138|           14.2823|   1971|
|4C:91:7A:AD:CF:86|  107930| 4.752299999999999|   1972|
|4C:91:7A:FB:B6:9E| 1334823| 4.752299999999999|   1970|
|4C:91:7A:1C:B7:42|   97483| 4.752299999999999|   1970|
|4C:91:7A:17:86:30| 1140011| 4.752299999999999|   1972|
|4C:91:7A:82:6F:F9| 1336923| 4.752299999999999|   1969|
|4C:91:7A:61:63:38| 1138825| 7.175699999999996|   1972|
|4C:91:7A:88:98:53| 1147367|26.158300000000008|   1973|
|4C:91:7A:67:C2:6F|  793001|23.761499999999995|   1973|
|4C:91:7A:1D:0A:44| 1217713|20.685699999999994|   1972|
|4C:91:7A:D8:33:AA|  880843|11.881099999999998|   1969|
|4C:91:7A:DE:AC:AF|  582136|           28.6389|   1973|
|4C:91:7A:17:75:74|  586693|           19.0099|   1973|
+-----------------+--------+------------------+-------+
only showing top 20 rows

0.0011
2612.286999999998
==================================================update_user.action_weight_normal==================================================
222350
+-----------------+--------+-------+----------+
|          user_id|movie_id|cate_id|    weight|
+-----------------+--------+-------+----------+
|4C:91:7A:BB:DE:6E|  169663|   1970|0.01299674|
|4C:91:7A:74:1A:CE|  148993|   1970|0.01437668|
|4C:91:7A:71:D3:B0|  917528|   1973|0.01020577|
|4C:91:7A:D8:F1:3E|      44|   2271|0.00181879|
|4C:91:7A:38:8F:19| 1289125|   1969|  0.003638|
|4C:91:7A:97:5B:D7|  115668|   1970|0.00607809|
|4C:91:7A:DA:17:8B| 1334823|   1970|0.01783997|
|4C:91:7A:D1:DB:8E|  990138|   1971|0.00546694|
|4C:91:7A:AD:CF:86|  107930|   1972|0.00181879|
|4C:91:7A:FB:B6:9E| 1334823|   1970|0.00181879|
|4C:91:7A:1C:B7:42|   97483|   1970|0.00181879|
|4C:91:7A:17:86:30| 1140011|   1972|0.00181879|
|4C:91:7A:82:6F:F9| 1336923|   1969|0.00181879|
|4C:91:7A:61:63:38| 1138825|   1972|0.00274648|
|4C:91:7A:88:98:53| 1147367|   1973|0.01001315|
|4C:91:7A:67:C2:6F|  793001|   1973|0.00909564|
|4C:91:7A:1D:0A:44| 1217713|   1972| 0.0079182|
|4C:91:7A:D8:33:AA|  880843|   1969|0.00454774|
|4C:91:7A:DE:AC:AF|  582136|   1973|0.01096274|
|4C:91:7A:17:75:74|  586693|   1973|0.00727669|
+-----------------+--------+-------+----------+
only showing top 20 rows

用户画像更新完成
update_user_profile 运行时长：6.564022163550059 分钟
==================================================update_user.user_similar_recall_1969==================================================
4708951
+-----------------+--------+-------------+----+---------+----------+-----+-----+--------+--------+----------+
|          user_id|movie_id|        title|year|movie_id2|    title2|year2|score|play_num|  weight|total_sort|
+-----------------+--------+-------------+----+---------+----------+-----+-----+--------+--------+----------+
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|   994213|   海王(普通话)| 2018|  8.8|       3|6.111E-5|         1|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|   612310|大卫贝肯之倒霉特工熊| 2017|  8.6|      38|5.828E-5|         2|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|   964565|      太空旅客| 2017|  8.6|       0|5.777E-5|         3|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|   191171|    金刚：骷髅岛| 2017|  8.5|      18|5.574E-5|         4|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|  1144039|      龙牌之谜| 2019|  7.5|       1|5.504E-5|         5|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|   890203|   生化危机：终章| 2017|  8.4|       6|5.426E-5|         6|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|   191298|      X特遣队| 2016|  8.7|       0|5.292E-5|         7|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|   156939|        死侍| 2016|  8.5|       0|5.203E-5|         8|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|   933834|      深海越狱| 2018|  7.6|       0|5.139E-5|         9|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|   901713|       犬之岛| 2018|  8.2|       6|5.069E-5|        10|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|   927764|      奇幻森林| 2016|  8.8|      16|5.053E-5|        11|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|   221178|    速度与激情6| 2013|  9.2|       3|4.986E-5|        12|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|   939252|        危墙| 2017|  7.5|       0|4.779E-5|        13|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|   766255|   最后的巫师猎人| 2015|  8.4|       2|4.635E-5|        14|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|  1337564|  明日边缘(4K)| 2014|  8.5|       0|4.606E-5|        15|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|   747777|       女间谍| 2015|  8.3|       0|4.563E-5|        16|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|   559546|      超凡战队| 2017|  7.5|      19|4.434E-5|        17|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|   171066|  小飞侠：幻梦启航| 2015|  7.8|       2|4.269E-5|        18|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|   511457|      屠魔战士| 2015|  7.6|       1|4.197E-5|        19|
|4C:91:7A:02:1F:50| 1008988|加勒比海盗：黑珍珠号的诅咒|2003|    96288|     菲利普船长| 2013|  8.7|       0|3.991E-5|        20|
+-----------------+--------+-------------+----+---------+----------+-----+-----+--------+--------+----------+
only showing top 20 rows

==================================================update_user.user_similar_filter_same_recall_1969==================================================
4375999
+-----------------+--------+-------------+----+-----+--------+---------+--------------------+
|          user_id|movie_id|        title|year|score|play_num|   weight|          base_movie|
+-----------------+--------+-------------+----+-----+--------+---------+--------------------+
|00:22:33:44:55:66|  762602|    宫本武藏二刀流开眼|1963|  7.8|       0|  3.72E-6|[[1269531, 王者天下, ...|
|00:30:1B:BA:02:DB|    6954|           断箭|1996|  8.2|       0| 7.492E-5|[[965305, 极盗者, 20...|
|00:30:1B:BA:02:DB|  112919|      灯红酒绿杀人夜|2008|  7.0|       1| 8.505E-5|[[196680, 小黄人大眼萌,...|
|00:30:1B:BA:02:DB|  114773|         重振球风|2006|  6.8|       0|  1.92E-5|[[965005, 料理鼠王, 2...|
|00:30:1B:BA:02:DB|  133679|          掌门人|1983|  7.9|       3| 2.778E-5|[[134075, 神偷妙探手多多...|
|00:30:1B:BA:02:DB|  397549|         分手达人|2014|  0.0|       0|7.0459E-4|[[1121176, 攀登者, 2...|
|00:30:1B:BA:02:DB|  733250|        爸爸去哪儿|2014|  7.0|       0|1.2043E-4|[[981932, 天气预爆, 2...|
|00:30:1B:BA:02:DB|  829993|校花的超级保镖之消失的村落|2017|  7.4|       0| 3.727E-5|[[987140, 恐龙王, 20...|
|00:30:1B:BA:02:DB|  874933|         大开杀戒|2000|  7.5|       0| 9.017E-5|[[965305, 极盗者, 20...|
|00:30:1B:BA:02:DB|  927242|       女模特的风波|1989|  7.6|       1|1.2051E-4|[[1338119, 无影之镜, ...|
|00:30:1B:BA:02:DB|  974075|         五月八月|2002|  7.9|       0|  8.79E-6|[[1340904, 刻在书桌上的...|
|00:30:1B:BA:02:DB|  982333|         露水风波|2017|  7.0|       0|1.6723E-4|[[196680, 小黄人大眼萌,...|
|00:30:1B:BA:02:DB|  985668|          等·等|2019|  7.5|       0|  5.06E-6|[[105897, 一九四二, 2...|
|00:30:1B:BA:02:DB|  987558|          大冲撞|1992|  5.8|       0|   2.1E-7|[[105897, 一九四二, 2...|
|00:30:1B:BA:02:DB| 1119983|        路客与刀客|1970|  7.5|       0|  2.64E-6|[[993066, 黄飞鸿之狮魂觉...|
|00:30:1B:BA:02:DB| 1337461|         找你算账|1989|  7.2|       0|  3.04E-6|[[985095, 黄飞鸿之怒海雄...|
|00:38:58:00:11:01|  140837|         爱，很美|2013|  7.4|       0| 2.818E-5|[[800353, 荒野加油站, ...|
|00:38:58:00:11:01|  895627|          三岔口|2005|  8.2|       0| 8.929E-5|[[157996, 监狱风云2, ...|
|08:00:27:CD:BC:0C|  191370|           思悼|2015|  8.4|       0| 5.197E-5|[[1269531, 王者天下, ...|
|08:00:27:CD:BC:0C|  195402|       澳门1949|2010|  6.7|       0| 1.839E-5|[[778689, 红海行动, 2...|
+-----------------+--------+-------------+----+-----+--------+---------+--------------------+
only showing top 20 rows

==================================================update_user.user_similar_filter_history_recall_1969==================================================
4343553
+-----------------+--------+----------+----+-----+--------+--------+--------------------+--------+---------+
|          user_id|movie_id|     title|year|score|play_num|  weight|          base_movie|sort_num|timestamp|
+-----------------+--------+----------+----+-----+--------+--------+--------------------+--------+---------+
|4C:91:7A:02:1F:50|  994213|   海王(普通话)|2018|  8.8|       3|6.111E-5|[[1008988, 加勒比海盗：...|       1| 20200601|
|4C:91:7A:02:1F:50|  612310|大卫贝肯之倒霉特工熊|2017|  8.6|      38|5.828E-5|[[1008988, 加勒比海盗：...|       2| 20200601|
|4C:91:7A:02:1F:50|  964565|      太空旅客|2017|  8.6|       0|5.777E-5|[[1008988, 加勒比海盗：...|       3| 20200601|
|4C:91:7A:02:1F:50|  191171|    金刚：骷髅岛|2017|  8.5|      18|5.574E-5|[[1008988, 加勒比海盗：...|       4| 20200601|
|4C:91:7A:02:1F:50| 1144039|      龙牌之谜|2019|  7.5|       1|5.504E-5|[[1008988, 加勒比海盗：...|       5| 20200601|
|4C:91:7A:02:1F:50|  890203|   生化危机：终章|2017|  8.4|       6|5.426E-5|[[1008988, 加勒比海盗：...|       6| 20200601|
|4C:91:7A:02:1F:50|  191298|      X特遣队|2016|  8.7|       0|5.292E-5|[[1008988, 加勒比海盗：...|       7| 20200601|
|4C:91:7A:02:1F:50|  156939|        死侍|2016|  8.5|       0|5.203E-5|[[1008988, 加勒比海盗：...|       8| 20200601|
|4C:91:7A:02:1F:50|  933834|      深海越狱|2018|  7.6|       0|5.139E-5|[[1008988, 加勒比海盗：...|       9| 20200601|
|4C:91:7A:02:1F:50|  901713|       犬之岛|2018|  8.2|       6|5.069E-5|[[1008988, 加勒比海盗：...|      10| 20200601|
|4C:91:7A:02:1F:50|  927764|      奇幻森林|2016|  8.8|      16|5.053E-5|[[1008988, 加勒比海盗：...|      11| 20200601|
|4C:91:7A:02:1F:50|  221178|    速度与激情6|2013|  9.2|       3|4.986E-5|[[1008988, 加勒比海盗：...|      12| 20200601|
|4C:91:7A:02:1F:50|  939252|        危墙|2017|  7.5|       0|4.779E-5|[[1008988, 加勒比海盗：...|      13| 20200601|
|4C:91:7A:02:1F:50|  766255|   最后的巫师猎人|2015|  8.4|       2|4.635E-5|[[1008988, 加勒比海盗：...|      14| 20200601|
|4C:91:7A:02:1F:50| 1337564|  明日边缘(4K)|2014|  8.5|       0|4.606E-5|[[1008988, 加勒比海盗：...|      15| 20200601|
|4C:91:7A:02:1F:50|  747777|       女间谍|2015|  8.3|       0|4.563E-5|[[1008988, 加勒比海盗：...|      16| 20200601|
|4C:91:7A:02:1F:50|  559546|      超凡战队|2017|  7.5|      19|4.434E-5|[[1008988, 加勒比海盗：...|      17| 20200601|
|4C:91:7A:02:1F:50|  171066|  小飞侠：幻梦启航|2015|  7.8|       2|4.269E-5|[[1008988, 加勒比海盗：...|      18| 20200601|
|4C:91:7A:02:1F:50|  511457|      屠魔战士|2015|  7.6|       1|4.197E-5|[[1008988, 加勒比海盗：...|      19| 20200601|
|4C:91:7A:02:1F:50|   96288|     菲利普船长|2013|  8.7|       0|3.991E-5|[[1008988, 加勒比海盗：...|      20| 20200601|
+-----------------+--------+----------+----+-----+--------+--------+--------------------+--------+---------+
only showing top 20 rows

==================================================update_user.user_similar_recall_1969_100==================================================
1413631
+-----------------+--------+----------+----+-----+--------+--------+--------------------+--------+---------+
|          user_id|movie_id|     title|year|score|play_num|  weight|          base_movie|sort_num|timestamp|
+-----------------+--------+----------+----+-----+--------+--------+--------------------+--------+---------+
|4C:91:7A:02:1F:50|  994213|   海王(普通话)|2018|  8.8|       3|6.111E-5|[[1008988, 加勒比海盗：...|       1| 20200601|
|4C:91:7A:02:1F:50|  612310|大卫贝肯之倒霉特工熊|2017|  8.6|      38|5.828E-5|[[1008988, 加勒比海盗：...|       2| 20200601|
|4C:91:7A:02:1F:50|  964565|      太空旅客|2017|  8.6|       0|5.777E-5|[[1008988, 加勒比海盗：...|       3| 20200601|
|4C:91:7A:02:1F:50|  191171|    金刚：骷髅岛|2017|  8.5|      18|5.574E-5|[[1008988, 加勒比海盗：...|       4| 20200601|
|4C:91:7A:02:1F:50| 1144039|      龙牌之谜|2019|  7.5|       1|5.504E-5|[[1008988, 加勒比海盗：...|       5| 20200601|
|4C:91:7A:02:1F:50|  890203|   生化危机：终章|2017|  8.4|       6|5.426E-5|[[1008988, 加勒比海盗：...|       6| 20200601|
|4C:91:7A:02:1F:50|  191298|      X特遣队|2016|  8.7|       0|5.292E-5|[[1008988, 加勒比海盗：...|       7| 20200601|
|4C:91:7A:02:1F:50|  156939|        死侍|2016|  8.5|       0|5.203E-5|[[1008988, 加勒比海盗：...|       8| 20200601|
|4C:91:7A:02:1F:50|  933834|      深海越狱|2018|  7.6|       0|5.139E-5|[[1008988, 加勒比海盗：...|       9| 20200601|
|4C:91:7A:02:1F:50|  901713|       犬之岛|2018|  8.2|       6|5.069E-5|[[1008988, 加勒比海盗：...|      10| 20200601|
|4C:91:7A:02:1F:50|  927764|      奇幻森林|2016|  8.8|      16|5.053E-5|[[1008988, 加勒比海盗：...|      11| 20200601|
|4C:91:7A:02:1F:50|  221178|    速度与激情6|2013|  9.2|       3|4.986E-5|[[1008988, 加勒比海盗：...|      12| 20200601|
|4C:91:7A:02:1F:50|  939252|        危墙|2017|  7.5|       0|4.779E-5|[[1008988, 加勒比海盗：...|      13| 20200601|
|4C:91:7A:02:1F:50|  766255|   最后的巫师猎人|2015|  8.4|       2|4.635E-5|[[1008988, 加勒比海盗：...|      14| 20200601|
|4C:91:7A:02:1F:50| 1337564|  明日边缘(4K)|2014|  8.5|       0|4.606E-5|[[1008988, 加勒比海盗：...|      15| 20200601|
|4C:91:7A:02:1F:50|  747777|       女间谍|2015|  8.3|       0|4.563E-5|[[1008988, 加勒比海盗：...|      16| 20200601|
|4C:91:7A:02:1F:50|  559546|      超凡战队|2017|  7.5|      19|4.434E-5|[[1008988, 加勒比海盗：...|      17| 20200601|
|4C:91:7A:02:1F:50|  171066|  小飞侠：幻梦启航|2015|  7.8|       2|4.269E-5|[[1008988, 加勒比海盗：...|      18| 20200601|
|4C:91:7A:02:1F:50|  511457|      屠魔战士|2015|  7.6|       1|4.197E-5|[[1008988, 加勒比海盗：...|      19| 20200601|
|4C:91:7A:02:1F:50|   96288|     菲利普船长|2013|  8.7|       0|3.991E-5|[[1008988, 加勒比海盗：...|      20| 20200601|
+-----------------+--------+----------+----+-----+--------+--------+--------------------+--------+---------+
only showing top 20 rows

用户历史相似度召回更新完成
update_similar_recall 运行时长：9.743529458840689 分钟
